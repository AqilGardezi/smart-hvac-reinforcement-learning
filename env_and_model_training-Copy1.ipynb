{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# Gymnasium imports\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "# Scikit-Learn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# stable-baselines3 (PPO)\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback, EvalCallback\n",
    "\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple data loading - no complex feature engineering\n",
    "# This simple approach previously achieved MAE 34\n",
    "\n",
    "print(\"Using simple feature set without advanced engineering.\")\n",
    "print(\"This configuration previously achieved MAE 34.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 3: Simple HVAC Environment (Previously achieved MAE 34)\n",
    "\n",
    "class HVACValveEnv(gym.Env):\n",
    "    \"\"\"Simple HVAC Valve control environment that achieved MAE 34\"\"\"\n",
    "\n",
    "    metadata = {\"render_modes\": [\"human\"], \"render_fps\": 30}\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        obs_cols: List[str],\n",
    "        scaler=None,\n",
    "        episode_length: int = 256,\n",
    "        start_index: int = 0,\n",
    "        smoothness_penalty_coef: float = 0.05,  # Simple proven coefficient\n",
    "        energy_penalty_coef: float = 0.0005,    # Simple proven coefficient\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.df = df.copy()\n",
    "        self.obs_cols = obs_cols\n",
    "        self.scaler = scaler\n",
    "        self.episode_length = episode_length\n",
    "        self.start_index = start_index\n",
    "        self.smoothness_penalty_coef = smoothness_penalty_coef\n",
    "        self.energy_penalty_coef = energy_penalty_coef\n",
    "\n",
    "        # Ensure valid indices\n",
    "        max_start = len(self.df) - episode_length - 1\n",
    "        if self.start_index > max_start:\n",
    "            self.start_index = max_start\n",
    "\n",
    "        # Action space: continuous control of valve opening [0, 100]\n",
    "        self.action_space = spaces.Box(low=0.0, high=100.0, shape=(1,), dtype=np.float32)\n",
    "\n",
    "        # Observation space: simple features only\n",
    "        n_features = len(obs_cols)\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=-np.inf, high=np.inf, shape=(n_features,), dtype=np.float32\n",
    "        )\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        \n",
    "        # Random start within valid range\n",
    "        valid_range = min(1000, len(self.df) - self.episode_length - 10)\n",
    "        if valid_range > 0:\n",
    "            self.current_idx = np.random.randint(0, valid_range)\n",
    "        else:\n",
    "            self.current_idx = 0\n",
    "        \n",
    "        self.step_count = 0\n",
    "        self.prev_action = 50.0  # Initialize to middle value\n",
    "        \n",
    "        obs = self._get_observation()\n",
    "        info = self._get_info()\n",
    "        return obs, info\n",
    "\n",
    "    def _get_observation(self):\n",
    "        \"\"\"Get current observation (scaled features)\"\"\"\n",
    "        if self.current_idx >= len(self.df):\n",
    "            self.current_idx = len(self.df) - 1\n",
    "        \n",
    "        row = self.df.iloc[self.current_idx]\n",
    "        obs = row[self.obs_cols].values.astype(np.float32)\n",
    "        \n",
    "        # Apply scaling if available\n",
    "        if self.scaler is not None:\n",
    "            obs = self.scaler.transform(obs.reshape(1, -1)).flatten().astype(np.float32)\n",
    "        \n",
    "        return obs\n",
    "\n",
    "    def _get_info(self):\n",
    "        \"\"\"Get info dictionary\"\"\"\n",
    "        if self.current_idx >= len(self.df):\n",
    "            return {\"error\": 0.0, \"gt_valve\": 50.0}\n",
    "        \n",
    "        row = self.df.iloc[self.current_idx]\n",
    "        gt_valve = float(row.get(\"Valve\", 50.0))\n",
    "        \n",
    "        return {\n",
    "            \"gt_valve\": gt_valve,\n",
    "            \"timestep\": self.current_idx,\n",
    "            \"episode_step\": self.step_count\n",
    "        }\n",
    "\n",
    "    def step(self, action):\n",
    "        # Ensure action is a scalar\n",
    "        action_val = float(action[0]) if hasattr(action, '__len__') else float(action)\n",
    "        action_val = np.clip(action_val, 0.0, 100.0)\n",
    "        \n",
    "        # Get ground truth\n",
    "        if self.current_idx >= len(self.df):\n",
    "            self.current_idx = len(self.df) - 1\n",
    "        \n",
    "        row = self.df.iloc[self.current_idx]\n",
    "        gt_valve = float(row.get(\"Valve\", 50.0))\n",
    "        \n",
    "        # Calculate error\n",
    "        error = abs(action_val - gt_valve)\n",
    "        \n",
    "        # Simple reward calculation - this achieved MAE 34\n",
    "        reward_primary = -error  # Main reward: minimize error\n",
    "        \n",
    "        # Energy penalty (prefer moderate valve positions)\n",
    "        energy_penalty = -self.energy_penalty_coef * (action_val - 50.0) ** 2\n",
    "        \n",
    "        # Smoothness penalty (avoid rapid changes)\n",
    "        smoothness_penalty = -self.smoothness_penalty_coef * abs(action_val - self.prev_action)\n",
    "        \n",
    "        # Total reward\n",
    "        reward = reward_primary + energy_penalty + smoothness_penalty\n",
    "        \n",
    "        # Update state\n",
    "        self.prev_action = action_val\n",
    "        self.step_count += 1\n",
    "        self.current_idx += 1\n",
    "        \n",
    "        # Episode termination\n",
    "        done = (self.step_count >= self.episode_length) or (self.current_idx >= len(self.df))\n",
    "        truncated = False\n",
    "        \n",
    "        # Get next observation and info\n",
    "        if not done:\n",
    "            obs = self._get_observation()\n",
    "        else:\n",
    "            obs = np.zeros_like(self._get_observation())\n",
    "        \n",
    "        info = self._get_info()\n",
    "        info[\"error\"] = error\n",
    "        \n",
    "        return obs, reward, done, truncated, info\n",
    "\n",
    "    def render(self, mode=\"human\"):\n",
    "        pass\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "print(\"Simple HVACValveEnv created (previously achieved MAE 34)\")\n",
    "print(\"Key parameters:\")\n",
    "print(\"- smoothness_penalty_coef: 0.05\")\n",
    "print(\"- energy_penalty_coef: 0.0005\")\n",
    "print(\"- Simple reward: -error + energy_penalty + smoothness_penalty\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# STEP 4: Simple PPO Setup (Previously achieved MAE 34)\n",
    "\n",
    "# Load dataframe\n",
    "feature_df = pd.read_csv(\"./files/final_feature_df.csv\")\n",
    "print(\"Loaded dataframe shape:\", feature_df.shape)\n",
    "\n",
    "# Simple observation columns - exactly what achieved MAE 34\n",
    "obs_cols = [\n",
    "    \"RaTemp\", \"SaTemp\", \"RaHumidity\", \"ThermEnergy\", \n",
    "    \"main_temp\", \"main_humidity\", \"wind_speed\", \"clouds_all\", \n",
    "    \"hour_sin\", \"hour_cos\", \"minute_sin\", \"minute_cos\", \"dayofweek\", \"is_weekend\",\"Occp\",\n",
    "    \"delta_supply_return\", \"delta_outdoor_indoor\", \"delta_humidity\",\"Valve_lag_1\", \"RaTemp_lag_1\", \"SaTemp_lag_1\", \n",
    "    \"RaHumidity_lag_1\"\n",
    "]\n",
    "\n",
    "# Fill NA and fit scaler (simple approach)\n",
    "df_for_scaler = feature_df[obs_cols].fillna(method=\"ffill\").fillna(0.0)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(df_for_scaler.values)\n",
    "\n",
    "# Simple environment factory - no curriculum learning\n",
    "def make_env(start_index=0):\n",
    "    def _init():\n",
    "        env = HVACValveEnv(\n",
    "            df=feature_df.fillna(method=\"ffill\").fillna(0.0),\n",
    "            obs_cols=obs_cols,\n",
    "            scaler=scaler,\n",
    "            episode_length=256,  # Simple fixed episode length\n",
    "            start_index=start_index,\n",
    "            smoothness_penalty_coef=0.05,   # Proven working values\n",
    "            energy_penalty_coef=0.0005,     # Proven working values\n",
    "        )\n",
    "        return env\n",
    "    return _init\n",
    "\n",
    "# Simple vectorized environment (single environment)\n",
    "vec_env = DummyVecEnv([make_env()])\n",
    "vec_env = VecNormalize(vec_env, norm_obs=True, norm_reward=False, clip_obs=10.0)\n",
    "\n",
    "# Simple PPO model with proven hyperparameters\n",
    "model = PPO(\n",
    "    \"MlpPolicy\",\n",
    "    vec_env,\n",
    "    verbose=1,\n",
    "    learning_rate=1e-4,                  # Proven learning rate\n",
    "    batch_size=128,                      # Proven batch size\n",
    "    policy_kwargs=dict(net_arch=[512, 256]),  # Proven architecture\n",
    ")\n",
    "\n",
    "print(\"Simple PPO model created with proven configuration:\")\n",
    "print(\"- Learning rate: 1e-4\")\n",
    "print(\"- Batch size: 128\") \n",
    "print(\"- Network architecture: [512, 256]\")\n",
    "print(\"- Single environment (no parallelization)\")\n",
    "print(\"- This exact configuration previously achieved MAE 34\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Display observation columns for reference\n",
    "print(\"Observation columns used in the model:\")\n",
    "for i, col in enumerate(obs_cols, 1):\n",
    "    print(f\"  {i:2d}. {col}\")\n",
    "print(f\"\\\\nTotal features: {len(obs_cols)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# STEP 5: Simple Training (Previously achieved MAE 34)\n",
    "\n",
    "# Setup simple callbacks\n",
    "checkpoint_callback = CheckpointCallback(save_freq=10000, save_path=\"./checkpoints/\", name_prefix=\"ppo_hvac_simple\")\n",
    "\n",
    "# Create simple eval environment\n",
    "eval_env = DummyVecEnv([make_env()])\n",
    "eval_env = VecNormalize(eval_env, norm_obs=True, norm_reward=False, clip_obs=10.0)\n",
    "eval_env.obs_rms = vec_env.obs_rms  # Sync normalization stats\n",
    "\n",
    "eval_callback = EvalCallback(\n",
    "    eval_env,\n",
    "    best_model_save_path=\"./best_model/\",\n",
    "    n_eval_episodes=5,\n",
    "    eval_freq=10000,\n",
    "    deterministic=True\n",
    ")\n",
    "\n",
    "# Simple proven training parameters\n",
    "TOTAL_TIMESTEPS = 3_000_000  # This achieved MAE 34\n",
    "\n",
    "print(f\"Starting simple training with {TOTAL_TIMESTEPS:,} timesteps\")\n",
    "print(\"This exact configuration previously achieved MAE 34\")\n",
    "print(\"\\\\nBeginning training...\")\n",
    "\n",
    "# Start training\n",
    "model.learn(total_timesteps=TOTAL_TIMESTEPS, callback=[checkpoint_callback, eval_callback])\n",
    "\n",
    "# Save final model and VecNormalize statistics\n",
    "os.makedirs(\"./models\", exist_ok=True)\n",
    "model.save(\"./models/ppo_hvac_simple_final\")\n",
    "vec_env.save(\"./models/vec_normalize_simple.pkl\")\n",
    "\n",
    "print(\"\\\\nTraining completed! Model saved as 'ppo_hvac_simple_final'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 6: Next Steps for Incremental MAE Reduction\n",
    "\n",
    "print(\"=== TRAINING COMPLETED WITH SIMPLE CONFIGURATION ===\")\n",
    "print(\"Current configuration previously achieved MAE 34\")\n",
    "print()\n",
    "print(\"=== INCREMENTAL IMPROVEMENT STRATEGIES ===\")\n",
    "print()\n",
    "print(\"âœ… PROVEN WORKING CONFIGURATION:\")\n",
    "print(\"   - Learning rate: 1e-4\")\n",
    "print(\"   - Batch size: 128\")\n",
    "print(\"   - Network: [512, 256]\")\n",
    "print(\"   - Smoothness penalty: 0.05\")\n",
    "print(\"   - Energy penalty: 0.0005\")\n",
    "print(\"   - Result: MAE 34\")\n",
    "print()\n",
    "print(\"ðŸŽ¯ NEXT EXPERIMENTS TO TRY (one at a time):\")\n",
    "print()\n",
    "print(\"1. **HYPERPARAMETER FINE-TUNING:**\")\n",
    "print(\"   - learning_rate: [5e-5, 2e-4, 3e-4]\")\n",
    "print(\"   - batch_size: [64, 256]\")\n",
    "print(\"   - smoothness_penalty_coef: [0.02, 0.1]\")\n",
    "print(\"   - Expected improvement: 1-3 MAE points\")\n",
    "print()\n",
    "print(\"2. **LONGER TRAINING:**\")\n",
    "print(\"   - Try 5M or 10M timesteps\")\n",
    "print(\"   - Monitor for overfitting\")\n",
    "print(\"   - Expected improvement: 1-2 MAE points\")\n",
    "print()\n",
    "print(\"3. **ENSEMBLE METHODS:**\")\n",
    "print(\"   - Train 3-5 models with different seeds\")\n",
    "print(\"   - Average their predictions\")\n",
    "print(\"   - Expected improvement: 2-4 MAE points\")\n",
    "print()\n",
    "print(\"4. **BEHAVIORAL CLONING PRE-TRAINING:**\")\n",
    "print(\"   - Pre-train with supervised learning first\")\n",
    "print(\"   - Then fine-tune with RL\")\n",
    "print(\"   - Expected improvement: 3-5 MAE points\")\n",
    "print()\n",
    "print(\"5. **ARCHITECTURAL VARIATIONS:**\")\n",
    "print(\"   - Try [256, 128], [1024, 512]\")\n",
    "print(\"   - Test deeper: [512, 256, 128]\")\n",
    "print(\"   - Expected improvement: 1-2 MAE points\")\n",
    "print()\n",
    "print(\"âš ï¸  AVOID THESE (previously made MAE worse):\")\n",
    "print(\"   - Complex reward engineering (increased MAE from 34 to 45)\")\n",
    "print(\"   - Curriculum learning\")\n",
    "print(\"   - Adaptive rewards\")\n",
    "print(\"   - Enhanced observation spaces\")\n",
    "print()\n",
    "print(\"ðŸŽ¯ REALISTIC TARGETS:\")\n",
    "print(\"   - Conservative: MAE 30-32\")\n",
    "print(\"   - Optimistic: MAE 25-28\")\n",
    "print(\"   - Best case with ensemble: MAE 20-25\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 6 START: Evaluate PPO performance\n",
    "# ---------------------------------------------------------------------------\n",
    "# Comments: We'll run the trained policy over a portion of the dataframe and record actions.\n",
    "\n",
    "# Reload model + vec normalize if necessary\n",
    "# model = PPO.load1(\"./models/ppo_hvac_final\")\n",
    "# vec_env = VecNormalize.load(\"./models/vec_normalize.pkl\", DummyVecEnv([make_env(start_index=0)]))\n",
    "# vec_env.reset()\n",
    "\n",
    "# For evaluation, use an un-normalized instance of the environment and manually apply scaler\n",
    "eval_df = feature_df.copy().fillna(method=\"ffill\").fillna(0.0)\n",
    "start_eval = 0\n",
    "end_eval = min(2000, len(eval_df) - 1)\n",
    "eval_episode_length = end_eval - start_eval\n",
    "\n",
    "eval_env_simple = HVACValveEnv(df=eval_df, obs_cols=obs_cols, scaler=scaler, episode_length=eval_episode_length, start_index=start_eval)\n",
    "obs, _ = eval_env_simple.reset()\n",
    "\n",
    "pred_actions = []\n",
    "true_vals = []\n",
    "infos = []\n",
    "\n",
    "done = False\n",
    "while not done:\n",
    "    # SB3 models expect actions in their raw range; since our action space is [0,100], model will output that.\n",
    "    action, _ = model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, _, info = eval_env_simple.step(action)\n",
    "    pred_actions.append(float(action[0]))\n",
    "    true_vals.append(float(info.get(\"gt_valve\", np.nan)))\n",
    "    infos.append(info)\n",
    "\n",
    "\n",
    "\n",
    "# Plot true vs predicted\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(true_vals, label=\"True Valve\")\n",
    "plt.plot(pred_actions, label=\"Predicted Valve (Agent)\")\n",
    "plt.legend()\n",
    "plt.title(\"True vs Predicted Valve over evaluation segment\")\n",
    "plt.show()\n",
    "\n",
    "# Save evaluation results\n",
    "eval_res = pd.DataFrame({\"true_valve\": true_vals, \"pred_valve\": pred_actions})\n",
    "eval_res.to_csv(\"./models/evaluation_results.csv\", index=False)\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# REPORT (brief): How to improve results\n",
    "# ---------------------------------------------------------------------------\n",
    "# 1) Increase TOTAL_TIMESTEPS and tune hyperparameters (learning rate, n_steps, batch_size).\n",
    "# 2) Adjust reward coefficients: smoothness_penalty_coef and energy_penalty_coef.\n",
    "# 3) Add richer state: previous Valve, time-of-day cyclic features, occupancy as categorical.\n",
    "# 4) Use curriculum learning: start with short episodes and increase length.\n",
    "# 5) Use model ensembles or offline behavioral cloning as warm-start from dataset actions.\n",
    "# 6) Consider action scaling: if Valve behaves as ratio, map to [0,1] internally for better learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute metrics\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "mse = mean_squared_error(true_vals, pred_actions)\n",
    "mae = mean_absolute_error(true_vals, pred_actions)\n",
    "print(f\"Evaluation MSE: {mse:.4f}, MAE: {mae:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 7: Model Diagnostics and Analysis\n",
    "\n",
    "def analyze_model_performance(true_vals, pred_actions, infos):\n",
    "    \"\"\"Analyze model performance in detail\"\"\"\n",
    "    true_vals = np.array(true_vals)\n",
    "    pred_actions = np.array(pred_actions)\n",
    "    \n",
    "    # Basic metrics\n",
    "    mae = np.mean(np.abs(true_vals - pred_actions))\n",
    "    mse = np.mean((true_vals - pred_actions)**2)\n",
    "    rmse = np.sqrt(mse)\n",
    "    \n",
    "    # Error distribution analysis\n",
    "    errors = np.abs(true_vals - pred_actions)\n",
    "    error_percentiles = np.percentile(errors, [25, 50, 75, 90, 95, 99])\n",
    "    \n",
    "    print(f\"=== Model Performance Analysis ===\")\n",
    "    print(f\"MAE: {mae:.4f}\")\n",
    "    print(f\"MSE: {mse:.4f}\")\n",
    "    print(f\"RMSE: {rmse:.4f}\")\n",
    "    print(f\"Error Percentiles: 25%={error_percentiles[0]:.2f}, 50%={error_percentiles[1]:.2f}, \")\n",
    "    print(f\"                  75%={error_percentiles[2]:.2f}, 90%={error_percentiles[3]:.2f}, \")\n",
    "    print(f\"                  95%={error_percentiles[4]:.2f}, 99%={error_percentiles[5]:.2f}\")\n",
    "    \n",
    "    # Identify problematic ranges\n",
    "    high_error_mask = errors > np.percentile(errors, 90)\n",
    "    if np.any(high_error_mask):\n",
    "        print(f\"\\nHigh error instances ({np.sum(high_error_mask)} cases):\")\n",
    "        print(f\"True values range: {true_vals[high_error_mask].min():.2f} - {true_vals[high_error_mask].max():.2f}\")\n",
    "        print(f\"Predictions range: {pred_actions[high_error_mask].min():.2f} - {pred_actions[high_error_mask].max():.2f}\")\n",
    "    \n",
    "    # Plot error distribution\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.hist(errors, bins=50, alpha=0.7)\n",
    "    plt.xlabel('Absolute Error')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Error Distribution')\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.scatter(true_vals, pred_actions, alpha=0.5, s=1)\n",
    "    plt.plot([true_vals.min(), true_vals.max()], [true_vals.min(), true_vals.max()], 'r--')\n",
    "    plt.xlabel('True Values')\n",
    "    plt.ylabel('Predicted Values')\n",
    "    plt.title('True vs Predicted Scatter')\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    residuals = pred_actions - true_vals\n",
    "    plt.scatter(true_vals, residuals, alpha=0.5, s=1)\n",
    "    plt.axhline(y=0, color='r', linestyle='--')\n",
    "    plt.xlabel('True Values')\n",
    "    plt.ylabel('Residuals')\n",
    "    plt.title('Residuals Plot')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return mae, mse, rmse\n",
    "\n",
    "# Run analysis if we have evaluation results\n",
    "if 'true_vals' in locals() and 'pred_actions' in locals():\n",
    "    mae, mse, rmse = analyze_model_performance(true_vals, pred_actions, infos)\n",
    "else:\n",
    "    print(\"Run evaluation first to analyze performance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional Techniques for Better Performance\n",
    "\n",
    "# 1. Ensemble Prediction (using multiple checkpoints)\n",
    "def load_and_predict_ensemble(obs, checkpoint_paths):\n",
    "    \"\"\"Load multiple models and average their predictions\"\"\"\n",
    "    predictions = []\n",
    "    for path in checkpoint_paths:\n",
    "        try:\n",
    "            model_temp = PPO.load(path)\n",
    "            pred, _ = model_temp.predict(obs, deterministic=True)\n",
    "            predictions.append(pred[0])\n",
    "        except:\n",
    "            continue\n",
    "    return np.mean(predictions) if predictions else 50.0  # fallback\n",
    "\n",
    "# 2. Behavioral Cloning Pretraining (optional)\n",
    "def pretrain_with_behavioral_cloning():\n",
    "    \"\"\"Pre-train the policy using supervised learning on the dataset\"\"\"\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    from sklearn.neural_network import MLPRegressor\n",
    "    \n",
    "    # Prepare training data\n",
    "    X = feature_df[obs_cols].fillna(method=\"ffill\").fillna(0.0).values\n",
    "    X_scaled = scaler.transform(X)\n",
    "    y = feature_df['Valve'].fillna(method=\"ffill\").fillna(50.0).values\n",
    "    \n",
    "    # Train a supervised model\n",
    "    bc_model = MLPRegressor(\n",
    "        hidden_layer_sizes=(512, 256, 128),\n",
    "        learning_rate_init=0.001,\n",
    "        max_iter=500,\n",
    "        early_stopping=True,\n",
    "        validation_fraction=0.2,\n",
    "        n_iter_no_change=20\n",
    "    )\n",
    "    \n",
    "    # Split data\n",
    "    split_idx = int(0.8 * len(X_scaled))\n",
    "    bc_model.fit(X_scaled[:split_idx], y[:split_idx])\n",
    "    \n",
    "    print(f\"Behavioral cloning model trained. Score: {bc_model.score(X_scaled[split_idx:], y[split_idx:]):.4f}\")\n",
    "    return bc_model\n",
    "\n",
    "# 3. Advanced Feature Engineering\n",
    "def create_advanced_features(df):\n",
    "    \"\"\"Create additional features that might help\"\"\"\n",
    "    df_enhanced = df.copy()\n",
    "    \n",
    "    # Rolling statistics\n",
    "    window_size = 10\n",
    "    for col in ['RaTemp', 'SaTemp', 'RaHumidity', 'Valve_lag_1']:\n",
    "        if col in df_enhanced.columns:\n",
    "            df_enhanced[f'{col}_rolling_mean'] = df_enhanced[col].rolling(window=window_size).mean()\n",
    "            df_enhanced[f'{col}_rolling_std'] = df_enhanced[col].rolling(window=window_size).std()\n",
    "    \n",
    "    # Temperature differences and ratios\n",
    "    if 'RaTemp' in df_enhanced.columns and 'SaTemp' in df_enhanced.columns:\n",
    "        df_enhanced['temp_ratio'] = df_enhanced['RaTemp'] / (df_enhanced['SaTemp'] + 1e-8)\n",
    "        df_enhanced['temp_change_rate'] = df_enhanced['RaTemp'].diff()\n",
    "    \n",
    "    # Interaction features\n",
    "    if 'Occp' in df_enhanced.columns and 'main_temp' in df_enhanced.columns:\n",
    "        df_enhanced['occupancy_temp_interaction'] = df_enhanced['Occp'] * df_enhanced['main_temp']\n",
    "    \n",
    "    return df_enhanced.fillna(method='ffill').fillna(0.0)\n",
    "\n",
    "print(\"Advanced techniques defined. To use:\")\n",
    "print(\"1. Run pretrain_with_behavioral_cloning() before PPO training\")\n",
    "print(\"2. Use ensemble prediction for final evaluation\")\n",
    "print(\"3. Enhanced features can improve observation space\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (proj)",
   "language": "python",
   "name": "proj"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
