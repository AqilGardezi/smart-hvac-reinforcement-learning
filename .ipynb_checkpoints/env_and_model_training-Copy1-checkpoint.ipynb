{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# Gymnasium imports\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "# Scikit-Learn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# stable-baselines3 (PPO)\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback, EvalCallback\n",
    "\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Advanced feature engineering functions defined.\n",
      "This will be applied when loading the data in the next cell.\n"
     ]
    }
   ],
   "source": [
    "# Create additional features that can help the agent learn better patterns\n",
    "\n",
    "def engineer_advanced_features(df):\n",
    "    \"\"\"Create advanced features for better learning\"\"\"\n",
    "    df_enhanced = df.copy()\n",
    "    \n",
    "    # 1. Rolling statistics (smoothed features)\n",
    "    window_sizes = [5, 10, 20]\n",
    "    rolling_cols = ['RaTemp', 'SaTemp', 'RaHumidity', 'ThermEnergy']\n",
    "    \n",
    "    for window in window_sizes:\n",
    "        for col in rolling_cols:\n",
    "            if col in df_enhanced.columns:\n",
    "                df_enhanced[f'{col}_ma_{window}'] = df_enhanced[col].rolling(window=window, min_periods=1).mean()\n",
    "                df_enhanced[f'{col}_std_{window}'] = df_enhanced[col].rolling(window=window, min_periods=1).std()\n",
    "    \n",
    "    # 2. Rate of change features\n",
    "    change_cols = ['RaTemp', 'SaTemp', 'RaHumidity', 'Valve']\n",
    "    for col in change_cols:\n",
    "        if col in df_enhanced.columns:\n",
    "            df_enhanced[f'{col}_change'] = df_enhanced[col].diff()\n",
    "            df_enhanced[f'{col}_change_2'] = df_enhanced[col].diff(2)\n",
    "    \n",
    "    # 3. Interaction features\n",
    "    if 'Occp' in df_enhanced.columns:\n",
    "        df_enhanced['occupancy_weighted_temp'] = df_enhanced['Occp'] * df_enhanced.get('RaTemp', 0)\n",
    "        df_enhanced['occupancy_weighted_humidity'] = df_enhanced['Occp'] * df_enhanced.get('RaHumidity', 0)\n",
    "    \n",
    "    # 4. Time-based seasonal features\n",
    "    if 'hour_sin' in df_enhanced.columns and 'hour_cos' in df_enhanced.columns:\n",
    "        df_enhanced['time_of_day_intensity'] = np.sqrt(df_enhanced['hour_sin']**2 + df_enhanced['hour_cos']**2)\n",
    "    \n",
    "    # Fill NaN values\n",
    "    df_enhanced = df_enhanced.fillna(method='ffill').fillna(0.0)\n",
    "    \n",
    "    return df_enhanced\n",
    "\n",
    "print(\"Advanced feature engineering functions defined.\")\n",
    "print(\"This will be applied when loading the data in the next cell.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#INITIATE A CUSTOM GYMNASIUM ENVIRONMENT FOR HVAC (Valve control)\n",
    "\n",
    "class HVACValveEnv(gym.Env):\n",
    "    \"\"\"Custom Gymnasium environment for HVAC Valve control.\n",
    "\n",
    "    - Data-driven: uses a pandas DataFrame with features and a ground truth 'Valve' column.\n",
    "    - Continuous action: single value in [0, 100].\n",
    "    - Observation: numeric features vector for the current timestep.\n",
    "    \"\"\"\n",
    "\n",
    "    metadata = {\"render_modes\": [\"human\"], \"render_fps\": 30}\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        obs_cols: List[str],\n",
    "        scaler: StandardScaler,\n",
    "        episode_length: int = None,\n",
    "        start_index: int = 0,\n",
    "        smoothness_penalty_coef: float = 0.01,\n",
    "        energy_penalty_coef: float = 0.001,\n",
    "        reward_scaling: float = 0.01,  # Scale rewards for better learning\n",
    "        adaptive_rewards: bool = True,  # Use adaptive reward scaling\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.obs_cols = obs_cols\n",
    "        self.scaler = scaler\n",
    "        self.episode_length = episode_length or len(df)\n",
    "        self.start_index = start_index\n",
    "        self.smoothness_penalty_coef = smoothness_penalty_coef\n",
    "        self.energy_penalty_coef = energy_penalty_coef\n",
    "        self.reward_scaling = reward_scaling\n",
    "        self.adaptive_rewards = adaptive_rewards\n",
    "\n",
    "        # Action space: single continuous action 0-100\n",
    "        self.action_space = spaces.Box(low=0.0, high=100.0, shape=(1,), dtype=np.float32)\n",
    "\n",
    "        # Observation space: enhanced with more context\n",
    "        obs_shape = (len(self.obs_cols) + 2,)  # +2 for last action and step progress\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=obs_shape, dtype=np.float32)\n",
    "\n",
    "        # Internal pointers\n",
    "        self.current_step = None\n",
    "        self.episode_step = None\n",
    "        self.last_action = None\n",
    "        self.error_history = []  # Track recent errors for adaptive rewards\n",
    "        self.max_history = 10\n",
    "\n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        # Option: choose a random start index so episodes cover different parts of data\n",
    "        max_start = max(0, len(self.df) - self.episode_length)\n",
    "        self.start_index = self.start_index if self.start_index <= max_start else np.random.randint(0, max_start + 1)\n",
    "        self.current_step = self.start_index\n",
    "        self.episode_step = 0\n",
    "        self.last_action = np.array([50.0], dtype=np.float32)  # neutral starting valve\n",
    "        self.error_history = []\n",
    "\n",
    "        obs = self._get_obs(self.current_step)\n",
    "        return obs, {}\n",
    "\n",
    "    def _get_obs(self, idx: int) -> np.ndarray:\n",
    "        row = self.df.loc[idx, self.obs_cols].values.astype(float)\n",
    "        row_scaled = self.scaler.transform(row.reshape(1, -1)).astype(np.float32).reshape(-1)\n",
    "        \n",
    "        # Add context: last action and episode progress\n",
    "        last_action_scaled = (float(self.last_action[0]) - 50.0) / 50.0  # Normalize to [-1, 1]\n",
    "        episode_progress = self.episode_step / self.episode_length\n",
    "        \n",
    "        # Concatenate all features\n",
    "        obs = np.concatenate([row_scaled, [last_action_scaled, episode_progress]])\n",
    "        return obs.astype(np.float32)\n",
    "\n",
    "    def step(self, action: np.ndarray):\n",
    "        # action is array-like of shape (1,)\n",
    "        a = float(action[0])\n",
    "        # clip into action space\n",
    "        a = float(np.clip(a, self.action_space.low[0], self.action_space.high[0]))\n",
    "\n",
    "        # Ground truth\n",
    "        gt = float(self.df.loc[self.current_step, \"Valve\"])\n",
    "\n",
    "        # Primary reward: negative absolute error (we want action close to gt)\n",
    "        err = abs(a - gt)\n",
    "        \n",
    "        # Enhanced reward shaping\n",
    "        if err < 5.0:  # Bonus for very accurate predictions\n",
    "            reward_primary = -err + 5.0  # Bonus reward\n",
    "        elif err < 10.0:  # Moderate accuracy\n",
    "            reward_primary = -err + 2.0\n",
    "        else:\n",
    "            reward_primary = -err\n",
    "        \n",
    "        # Adaptive reward scaling based on recent performance\n",
    "        if self.adaptive_rewards and len(self.error_history) > 5:\n",
    "            avg_recent_error = np.mean(self.error_history[-5:])\n",
    "            if avg_recent_error > 20:\n",
    "                reward_primary *= 1.5  # Increase reward signal when struggling\n",
    "        \n",
    "        # Track error history\n",
    "        self.error_history.append(err)\n",
    "        if len(self.error_history) > self.max_history:\n",
    "            self.error_history.pop(0)\n",
    "\n",
    "        # Energy penalty: encourage lower ThermalEnergy when possible\n",
    "        if \"ThermalEnergy\" in self.df.columns:\n",
    "            energy = float(self.df.loc[self.current_step, \"ThermalEnergy\"])\n",
    "        else:\n",
    "            energy = 0.0\n",
    "        energy_penalty = - self.energy_penalty_coef * energy\n",
    "\n",
    "        # Smoothness penalty: penalize large changes from last action\n",
    "        smoothness_penalty = - self.smoothness_penalty_coef * abs(a - float(self.last_action))\n",
    "        \n",
    "        # Momentum reward: encourage consistency in direction\n",
    "        if self.episode_step > 1 and hasattr(self, 'prev_action'):\n",
    "            direction_current = a - float(self.last_action)\n",
    "            direction_prev = float(self.last_action) - self.prev_action\n",
    "            if abs(direction_current) > 1 and abs(direction_prev) > 1:  # Only if significant changes\n",
    "                momentum_reward = 0.5 if (direction_current * direction_prev) > 0 else -0.5\n",
    "            else:\n",
    "                momentum_reward = 0.0\n",
    "        else:\n",
    "            momentum_reward = 0.0\n",
    "\n",
    "        # Combine rewards with scaling\n",
    "        reward = (reward_primary + energy_penalty + smoothness_penalty + momentum_reward) * self.reward_scaling\n",
    "\n",
    "        # record\n",
    "        self.prev_action = float(self.last_action) if hasattr(self, 'last_action') else 50.0\n",
    "        self.last_action = np.array([a], dtype=np.float32)\n",
    "\n",
    "        # advance\n",
    "        self.current_step += 1\n",
    "        self.episode_step += 1\n",
    "\n",
    "        # termination\n",
    "        done = False\n",
    "        if self.episode_step >= self.episode_length or self.current_step >= len(self.df):\n",
    "            done = True\n",
    "\n",
    "        obs = self._get_obs(self.current_step) if not done else np.zeros_like(self._get_obs(max(len(self.df)-1,0)))\n",
    "\n",
    "        info = {\"gt_valve\": gt, \"error\": err, \"energy\": energy, \"avg_recent_error\": np.mean(self.error_history) if self.error_history else err}\n",
    "\n",
    "        return obs, float(reward), done, False, info\n",
    "\n",
    "    def render(self, mode=\"human\"):\n",
    "        # optional: implement a simple print\n",
    "        print(f\"Step: {self.current_step}, Last action: {self.last_action}\")\n",
    "\n",
    "    def close(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataframe shape: (6190, 77)\n",
      "Using cpu device\n",
      "Starting with episode length: 128\n",
      "PPO model initialized with enhanced hyperparameters\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# Initialize PPO model (Stable-Baselines3)\n",
    "# ---------------------------------------------------------------------------\n",
    "# First prepare data, scaler, and the wrapped environment for SB3. Then will create PPO.\n",
    "\n",
    "# Load dataframe\n",
    "feature_df = pd.read_csv(\"./files/final_feature_df.csv\")\n",
    "print(\"Loaded dataframe shape:\", feature_df.shape)\n",
    "\n",
    "# Choose observation columns: all numeric columns minus target and timestamp if present\n",
    "exclude_cols = {\"Valve\", \"ts\"}\n",
    "\n",
    "obs_cols = [\n",
    "    \"RaTemp\", \"SaTemp\", \"RaHumidity\", \"ThermEnergy\", \n",
    "    \"main_temp\", \"main_humidity\", \"wind_speed\", \"clouds_all\", \n",
    "    \"hour_sin\", \"hour_cos\", \"minute_sin\", \"minute_cos\", \"dayofweek\", \"is_weekend\",\"Occp\",\n",
    "    \"delta_supply_return\", \"delta_outdoor_indoor\", \"delta_humidity\",\"Valve_lag_1\", \"RaTemp_lag_1\", \"SaTemp_lag_1\", \n",
    "    \"RaHumidity_lag_1\"\n",
    "]\n",
    "\n",
    "# Fill NA (simple strategy) and fit scaler\n",
    "df_for_scaler = feature_df[obs_cols].fillna(method=\"ffill\").fillna(0.0)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(df_for_scaler.values)\n",
    "\n",
    "# Curriculum Learning: Start with shorter episodes, gradually increase\n",
    "STARTING_EPISODE_LENGTH = 128  # Start smaller\n",
    "MAX_EPISODE_LENGTH = 512       # Gradually increase to this\n",
    "CURRICULUM_STEPS = 500000      # Steps to reach max episode length\n",
    "\n",
    "def get_episode_length(timestep):\n",
    "    \"\"\"Curriculum learning: gradually increase episode length\"\"\"\n",
    "    if timestep < CURRICULUM_STEPS:\n",
    "        progress = timestep / CURRICULUM_STEPS\n",
    "        length = int(STARTING_EPISODE_LENGTH + progress * (MAX_EPISODE_LENGTH - STARTING_EPISODE_LENGTH))\n",
    "        return length\n",
    "    return MAX_EPISODE_LENGTH\n",
    "\n",
    "# Create environment factory for DummyVecEnv with curriculum learning\n",
    "def make_env(start_index=0, episode_length=256):\n",
    "    def _init():\n",
    "        env = HVACValveEnv(\n",
    "            df=feature_df.fillna(method=\"ffill\").fillna(0.0),\n",
    "            obs_cols=obs_cols,\n",
    "            scaler=scaler,\n",
    "            episode_length=episode_length,\n",
    "            start_index=start_index,\n",
    "            smoothness_penalty_coef=0.1,    # Further increased for smoother control\n",
    "            energy_penalty_coef=0.0002,     # Further decreased to focus on accuracy\n",
    "            reward_scaling=0.01,            # Scale rewards for better learning\n",
    "            adaptive_rewards=True,          # Enable adaptive reward scaling\n",
    "        )\n",
    "        return env\n",
    "    return _init\n",
    "\n",
    "# Start with curriculum learning\n",
    "EPISODE_LENGTH = STARTING_EPISODE_LENGTH\n",
    "\n",
    "# Vectorized environment (wrap with DummyVecEnv)\n",
    "n_envs = 4\n",
    "env_fns = [make_env(start_index=i * EPISODE_LENGTH, episode_length=EPISODE_LENGTH) for i in range(n_envs)]\n",
    "vec_env = DummyVecEnv(env_fns)\n",
    "\n",
    "# Enhanced VecNormalize settings\n",
    "vec_env = VecNormalize(vec_env, norm_obs=True, norm_reward=True, clip_obs=10.0, clip_reward=10.0)\n",
    "\n",
    "# Create PPO agent with further optimized hyperparameters\n",
    "policy_kwargs = dict(\n",
    "    activation_fn=nn.LeakyReLU,  # LeakyReLU often works better than ReLU\n",
    "    net_arch=[dict(pi=[1024, 512, 256], vf=[1024, 512, 256])],  # Deeper network\n",
    "    ortho_init=False,  # Disable orthogonal initialization for better exploration\n",
    ")\n",
    "\n",
    "model = PPO(\n",
    "    \"MlpPolicy\",\n",
    "    vec_env,\n",
    "    verbose=1,\n",
    "    learning_rate=5e-5,          # Even lower learning rate for more stable learning\n",
    "    n_steps=8192 // n_envs,      # More steps per update\n",
    "    batch_size=256,              # Larger batch size\n",
    "    n_epochs=15,                 # More epochs per update\n",
    "    clip_range=0.15,             # Slightly tighter clipping\n",
    "    clip_range_vf=None,          # No value function clipping\n",
    "    ent_coef=0.001,              # Small entropy bonus for exploration\n",
    "    vf_coef=0.25,                # Reduced value function loss weight\n",
    "    max_grad_norm=0.5,           # Gradient clipping\n",
    "    gae_lambda=0.98,             # Higher GAE lambda for longer-term rewards\n",
    "    policy_kwargs=policy_kwargs,\n",
    ")\n",
    "\n",
    "print(f\"Starting with episode length: {EPISODE_LENGTH}\")\n",
    "print(f\"PPO model initialized with enhanced hyperparameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# obs_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 566  |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 14   |\n",
      "|    total_timesteps | 8192 |\n",
      "-----------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 357          |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 45           |\n",
      "|    total_timesteps      | 16384        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022500916 |\n",
      "|    clip_fraction        | 0.0325       |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | -0.0239      |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | -0.00434     |\n",
      "|    n_updates            | 15           |\n",
      "|    policy_gradient_loss | -0.00486     |\n",
      "|    std                  | 0.996        |\n",
      "|    value_loss           | 0.236        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=-217.10 +/- 0.00\n",
      "Episode length: 256.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 256         |\n",
      "|    mean_reward          | -217        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 20000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002794262 |\n",
      "|    clip_fraction        | 0.0444      |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -1.42       |\n",
      "|    explained_variance   | 0.811       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.00636    |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00429    |\n",
      "|    std                  | 0.999       |\n",
      "|    value_loss           | 0.0073      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 296   |\n",
      "|    iterations      | 3     |\n",
      "|    time_elapsed    | 82    |\n",
      "|    total_timesteps | 24576 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 291         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 112         |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002787059 |\n",
      "|    clip_fraction        | 0.0471      |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -1.42       |\n",
      "|    explained_variance   | 0.964       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0087     |\n",
      "|    n_updates            | 45          |\n",
      "|    policy_gradient_loss | -0.0054     |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 0.00154     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=-217.07 +/- 0.00\n",
      "Episode length: 256.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 256          |\n",
      "|    mean_reward          | -217         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 40000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030511052 |\n",
      "|    clip_fraction        | 0.0459       |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0.993        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | -0.0121      |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.00499     |\n",
      "|    std                  | 0.999        |\n",
      "|    value_loss           | 0.000445     |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 270   |\n",
      "|    iterations      | 5     |\n",
      "|    time_elapsed    | 151   |\n",
      "|    total_timesteps | 40960 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 272          |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 180          |\n",
      "|    total_timesteps      | 49152        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027929412 |\n",
      "|    clip_fraction        | 0.0479       |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0.999        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | -0.00803     |\n",
      "|    n_updates            | 75           |\n",
      "|    policy_gradient_loss | -0.00496     |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 0.00016      |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 273          |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 209          |\n",
      "|    total_timesteps      | 57344        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032715558 |\n",
      "|    clip_fraction        | 0.0592       |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 1            |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | -0.00697     |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.00591     |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 7.97e-05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=60000, episode_reward=-216.94 +/- 0.00\n",
      "Episode length: 256.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 256          |\n",
      "|    mean_reward          | -217         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 60000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035156892 |\n",
      "|    clip_fraction        | 0.073        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 1            |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | -0.00929     |\n",
      "|    n_updates            | 105          |\n",
      "|    policy_gradient_loss | -0.00578     |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 4.37e-05     |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 266   |\n",
      "|    iterations      | 8     |\n",
      "|    time_elapsed    | 245   |\n",
      "|    total_timesteps | 65536 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 267          |\n",
      "|    iterations           | 9            |\n",
      "|    time_elapsed         | 275          |\n",
      "|    total_timesteps      | 73728        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046830997 |\n",
      "|    clip_fraction        | 0.0936       |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 1            |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | -0.0102      |\n",
      "|    n_updates            | 120          |\n",
      "|    policy_gradient_loss | -0.00682     |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 2.61e-05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=80000, episode_reward=-216.57 +/- 0.00\n",
      "Episode length: 256.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 256          |\n",
      "|    mean_reward          | -217         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 80000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053770803 |\n",
      "|    clip_fraction        | 0.119        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -1.43        |\n",
      "|    explained_variance   | 1            |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | -0.0241      |\n",
      "|    n_updates            | 135          |\n",
      "|    policy_gradient_loss | -0.0089      |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 1.95e-05     |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 262   |\n",
      "|    iterations      | 10    |\n",
      "|    time_elapsed    | 311   |\n",
      "|    total_timesteps | 81920 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 264          |\n",
      "|    iterations           | 11           |\n",
      "|    time_elapsed         | 341          |\n",
      "|    total_timesteps      | 90112        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0049944627 |\n",
      "|    clip_fraction        | 0.109        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 1            |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | -0.0176      |\n",
      "|    n_updates            | 150          |\n",
      "|    policy_gradient_loss | -0.00931     |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.59e-05     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 262          |\n",
      "|    iterations           | 12           |\n",
      "|    time_elapsed         | 374          |\n",
      "|    total_timesteps      | 98304        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058969907 |\n",
      "|    clip_fraction        | 0.126        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 1            |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | -0.0155      |\n",
      "|    n_updates            | 165          |\n",
      "|    policy_gradient_loss | -0.0103      |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.22e-05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=100000, episode_reward=-215.79 +/- 0.00\n",
      "Episode length: 256.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 256         |\n",
      "|    mean_reward          | -216        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 100000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005842086 |\n",
      "|    clip_fraction        | 0.13        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -1.42       |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.00319    |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.00935    |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 1.39e-05    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 259    |\n",
      "|    iterations      | 13     |\n",
      "|    time_elapsed    | 410    |\n",
      "|    total_timesteps | 106496 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 260         |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 440         |\n",
      "|    total_timesteps      | 114688      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006854389 |\n",
      "|    clip_fraction        | 0.15        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -1.43       |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0101     |\n",
      "|    n_updates            | 195         |\n",
      "|    policy_gradient_loss | -0.0103     |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 1.11e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=120000, episode_reward=-215.21 +/- 0.00\n",
      "Episode length: 256.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 256         |\n",
      "|    mean_reward          | -215        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 120000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006794434 |\n",
      "|    clip_fraction        | 0.152       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -1.43       |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.00703    |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.0106     |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 1.55e-05    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 258    |\n",
      "|    iterations      | 15     |\n",
      "|    time_elapsed    | 476    |\n",
      "|    total_timesteps | 122880 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 259         |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 505         |\n",
      "|    total_timesteps      | 131072      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008088568 |\n",
      "|    clip_fraction        | 0.163       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -1.43       |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0121     |\n",
      "|    n_updates            | 225         |\n",
      "|    policy_gradient_loss | -0.0118     |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 1.57e-05    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 260          |\n",
      "|    iterations           | 17           |\n",
      "|    time_elapsed         | 534          |\n",
      "|    total_timesteps      | 139264       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069165775 |\n",
      "|    clip_fraction        | 0.162        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -1.43        |\n",
      "|    explained_variance   | 1            |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 0.00211      |\n",
      "|    n_updates            | 240          |\n",
      "|    policy_gradient_loss | -0.012       |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 1.29e-05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=140000, episode_reward=-214.32 +/- 0.00\n",
      "Episode length: 256.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 256         |\n",
      "|    mean_reward          | -214        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 140000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007826944 |\n",
      "|    clip_fraction        | 0.168       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -1.42       |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0139     |\n",
      "|    n_updates            | 255         |\n",
      "|    policy_gradient_loss | -0.0136     |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 1.7e-05     |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 258    |\n",
      "|    iterations      | 18     |\n",
      "|    time_elapsed    | 570    |\n",
      "|    total_timesteps | 147456 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 259         |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 600         |\n",
      "|    total_timesteps      | 155648      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007585873 |\n",
      "|    clip_fraction        | 0.177       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -1.42       |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0209     |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | -0.0116     |\n",
      "|    std                  | 0.999       |\n",
      "|    value_loss           | 1.46e-05    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=160000, episode_reward=-213.60 +/- 0.00\n",
      "Episode length: 256.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 256         |\n",
      "|    mean_reward          | -214        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 160000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008442197 |\n",
      "|    clip_fraction        | 0.178       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -1.42       |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.00487    |\n",
      "|    n_updates            | 285         |\n",
      "|    policy_gradient_loss | -0.012      |\n",
      "|    std                  | 0.997       |\n",
      "|    value_loss           | 1.93e-05    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 257    |\n",
      "|    iterations      | 20     |\n",
      "|    time_elapsed    | 637    |\n",
      "|    total_timesteps | 163840 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 258         |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 666         |\n",
      "|    total_timesteps      | 172032      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007450751 |\n",
      "|    clip_fraction        | 0.153       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -1.42       |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0174     |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.0115     |\n",
      "|    std                  | 0.996       |\n",
      "|    value_loss           | 1.83e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=180000, episode_reward=-213.27 +/- 0.01\n",
      "Episode length: 256.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 256         |\n",
      "|    mean_reward          | -213        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 180000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010168714 |\n",
      "|    clip_fraction        | 0.185       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -1.41       |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0178     |\n",
      "|    n_updates            | 315         |\n",
      "|    policy_gradient_loss | -0.0148     |\n",
      "|    std                  | 0.99        |\n",
      "|    value_loss           | 1.12e-05    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 256    |\n",
      "|    iterations      | 22     |\n",
      "|    time_elapsed    | 703    |\n",
      "|    total_timesteps | 180224 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 257         |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 732         |\n",
      "|    total_timesteps      | 188416      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008671448 |\n",
      "|    clip_fraction        | 0.177       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -1.41       |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.017      |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | -0.0143     |\n",
      "|    std                  | 0.982       |\n",
      "|    value_loss           | 2.11e-05    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 258         |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 761         |\n",
      "|    total_timesteps      | 196608      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009577194 |\n",
      "|    clip_fraction        | 0.177       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -1.4        |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.012      |\n",
      "|    n_updates            | 345         |\n",
      "|    policy_gradient_loss | -0.0143     |\n",
      "|    std                  | 0.978       |\n",
      "|    value_loss           | 1.15e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=200000, episode_reward=-212.57 +/- 0.00\n",
      "Episode length: 256.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 256         |\n",
      "|    mean_reward          | -213        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 200000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008138864 |\n",
      "|    clip_fraction        | 0.175       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -1.39       |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.00993    |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.0149     |\n",
      "|    std                  | 0.966       |\n",
      "|    value_loss           | 1.34e-05    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 256    |\n",
      "|    iterations      | 25     |\n",
      "|    time_elapsed    | 797    |\n",
      "|    total_timesteps | 204800 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 257         |\n",
      "|    iterations           | 26          |\n",
      "|    time_elapsed         | 827         |\n",
      "|    total_timesteps      | 212992      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008755555 |\n",
      "|    clip_fraction        | 0.188       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -1.38       |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0156     |\n",
      "|    n_updates            | 375         |\n",
      "|    policy_gradient_loss | -0.0147     |\n",
      "|    std                  | 0.962       |\n",
      "|    value_loss           | 2.31e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=220000, episode_reward=-212.18 +/- 0.00\n",
      "Episode length: 256.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 256          |\n",
      "|    mean_reward          | -212         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 220000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0089980345 |\n",
      "|    clip_fraction        | 0.192        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -1.38        |\n",
      "|    explained_variance   | 1            |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | -0.017       |\n",
      "|    n_updates            | 390          |\n",
      "|    policy_gradient_loss | -0.0144      |\n",
      "|    std                  | 0.961        |\n",
      "|    value_loss           | 9.57e-06     |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 256    |\n",
      "|    iterations      | 27     |\n",
      "|    time_elapsed    | 863    |\n",
      "|    total_timesteps | 221184 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 256         |\n",
      "|    iterations           | 28          |\n",
      "|    time_elapsed         | 892         |\n",
      "|    total_timesteps      | 229376      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009559344 |\n",
      "|    clip_fraction        | 0.189       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -1.38       |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0329     |\n",
      "|    n_updates            | 405         |\n",
      "|    policy_gradient_loss | -0.0152     |\n",
      "|    std                  | 0.954       |\n",
      "|    value_loss           | 1.6e-05     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 257          |\n",
      "|    iterations           | 29           |\n",
      "|    time_elapsed         | 921          |\n",
      "|    total_timesteps      | 237568       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0106422985 |\n",
      "|    clip_fraction        | 0.19         |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -1.37        |\n",
      "|    explained_variance   | 1            |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | -0.00976     |\n",
      "|    n_updates            | 420          |\n",
      "|    policy_gradient_loss | -0.0143      |\n",
      "|    std                  | 0.95         |\n",
      "|    value_loss           | 1.14e-05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=240000, episode_reward=-211.24 +/- 0.01\n",
      "Episode length: 256.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 256         |\n",
      "|    mean_reward          | -211        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 240000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010806273 |\n",
      "|    clip_fraction        | 0.194       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -1.36       |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.00873    |\n",
      "|    n_updates            | 435         |\n",
      "|    policy_gradient_loss | -0.0144     |\n",
      "|    std                  | 0.944       |\n",
      "|    value_loss           | 1.11e-05    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 256    |\n",
      "|    iterations      | 30     |\n",
      "|    time_elapsed    | 957    |\n",
      "|    total_timesteps | 245760 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 257         |\n",
      "|    iterations           | 31          |\n",
      "|    time_elapsed         | 986         |\n",
      "|    total_timesteps      | 253952      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010117451 |\n",
      "|    clip_fraction        | 0.203       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -1.36       |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0164     |\n",
      "|    n_updates            | 450         |\n",
      "|    policy_gradient_loss | -0.0145     |\n",
      "|    std                  | 0.94        |\n",
      "|    value_loss           | 5.12e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=260000, episode_reward=-210.76 +/- 0.00\n",
      "Episode length: 256.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 256         |\n",
      "|    mean_reward          | -211        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 260000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009203822 |\n",
      "|    clip_fraction        | 0.181       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -1.35       |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0168     |\n",
      "|    n_updates            | 465         |\n",
      "|    policy_gradient_loss | -0.0138     |\n",
      "|    std                  | 0.933       |\n",
      "|    value_loss           | 8.59e-06    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 256    |\n",
      "|    iterations      | 32     |\n",
      "|    time_elapsed    | 1022   |\n",
      "|    total_timesteps | 262144 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 256         |\n",
      "|    iterations           | 33          |\n",
      "|    time_elapsed         | 1052        |\n",
      "|    total_timesteps      | 270336      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009910947 |\n",
      "|    clip_fraction        | 0.202       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -1.35       |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.00808    |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | -0.0161     |\n",
      "|    std                  | 0.931       |\n",
      "|    value_loss           | 9.72e-06    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 257         |\n",
      "|    iterations           | 34          |\n",
      "|    time_elapsed         | 1081        |\n",
      "|    total_timesteps      | 278528      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010268407 |\n",
      "|    clip_fraction        | 0.199       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -1.34       |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0231     |\n",
      "|    n_updates            | 495         |\n",
      "|    policy_gradient_loss | -0.015      |\n",
      "|    std                  | 0.923       |\n",
      "|    value_loss           | 1.96e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=280000, episode_reward=-209.92 +/- 0.00\n",
      "Episode length: 256.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 256         |\n",
      "|    mean_reward          | -210        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 280000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009093862 |\n",
      "|    clip_fraction        | 0.171       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -1.34       |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0232     |\n",
      "|    n_updates            | 510         |\n",
      "|    policy_gradient_loss | -0.0112     |\n",
      "|    std                  | 0.918       |\n",
      "|    value_loss           | 9.25e-06    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 256    |\n",
      "|    iterations      | 35     |\n",
      "|    time_elapsed    | 1118   |\n",
      "|    total_timesteps | 286720 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 256         |\n",
      "|    iterations           | 36          |\n",
      "|    time_elapsed         | 1148        |\n",
      "|    total_timesteps      | 294912      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012056533 |\n",
      "|    clip_fraction        | 0.206       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -1.33       |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0162     |\n",
      "|    n_updates            | 525         |\n",
      "|    policy_gradient_loss | -0.0158     |\n",
      "|    std                  | 0.917       |\n",
      "|    value_loss           | 3.22e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=300000, episode_reward=-209.52 +/- 0.00\n",
      "Episode length: 256.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 256          |\n",
      "|    mean_reward          | -210         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 300000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0094868615 |\n",
      "|    clip_fraction        | 0.197        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -1.33        |\n",
      "|    explained_variance   | 1            |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | -0.0191      |\n",
      "|    n_updates            | 540          |\n",
      "|    policy_gradient_loss | -0.0148      |\n",
      "|    std                  | 0.912        |\n",
      "|    value_loss           | 1.13e-05     |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best mean reward!\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 255    |\n",
      "|    iterations      | 37     |\n",
      "|    time_elapsed    | 1185   |\n",
      "|    total_timesteps | 303104 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 256         |\n",
      "|    iterations           | 38          |\n",
      "|    time_elapsed         | 1214        |\n",
      "|    total_timesteps      | 311296      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011665786 |\n",
      "|    clip_fraction        | 0.2         |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -1.32       |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.000807   |\n",
      "|    n_updates            | 555         |\n",
      "|    policy_gradient_loss | -0.0153     |\n",
      "|    std                  | 0.906       |\n",
      "|    value_loss           | 9.54e-06    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 256         |\n",
      "|    iterations           | 39          |\n",
      "|    time_elapsed         | 1244        |\n",
      "|    total_timesteps      | 319488      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011261601 |\n",
      "|    clip_fraction        | 0.216       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -1.32       |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.00829    |\n",
      "|    n_updates            | 570         |\n",
      "|    policy_gradient_loss | -0.0156     |\n",
      "|    std                  | 0.9         |\n",
      "|    value_loss           | 5.64e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=320000, episode_reward=-208.25 +/- 0.01\n",
      "Episode length: 256.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 256       |\n",
      "|    mean_reward          | -208      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 320000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0100761 |\n",
      "|    clip_fraction        | 0.188     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | -1.31     |\n",
      "|    explained_variance   | 1         |\n",
      "|    learning_rate        | 5e-05     |\n",
      "|    loss                 | -0.031    |\n",
      "|    n_updates            | 585       |\n",
      "|    policy_gradient_loss | -0.0141   |\n",
      "|    std                  | 0.89      |\n",
      "|    value_loss           | 7.93e-06  |\n",
      "---------------------------------------\n",
      "New best mean reward!\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 255    |\n",
      "|    iterations      | 40     |\n",
      "|    time_elapsed    | 1281   |\n",
      "|    total_timesteps | 327680 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 256         |\n",
      "|    iterations           | 41          |\n",
      "|    time_elapsed         | 1311        |\n",
      "|    total_timesteps      | 335872      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010468647 |\n",
      "|    clip_fraction        | 0.202       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -1.3        |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.00869    |\n",
      "|    n_updates            | 600         |\n",
      "|    policy_gradient_loss | -0.0135     |\n",
      "|    std                  | 0.884       |\n",
      "|    value_loss           | 2.46e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=340000, episode_reward=-207.63 +/- 0.01\n",
      "Episode length: 256.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 256         |\n",
      "|    mean_reward          | -208        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 340000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011552908 |\n",
      "|    clip_fraction        | 0.211       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -1.29       |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0332     |\n",
      "|    n_updates            | 615         |\n",
      "|    policy_gradient_loss | -0.0143     |\n",
      "|    std                  | 0.878       |\n",
      "|    value_loss           | 9.53e-06    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 254    |\n",
      "|    iterations      | 42     |\n",
      "|    time_elapsed    | 1349   |\n",
      "|    total_timesteps | 344064 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 255         |\n",
      "|    iterations           | 43          |\n",
      "|    time_elapsed         | 1378        |\n",
      "|    total_timesteps      | 352256      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011743629 |\n",
      "|    clip_fraction        | 0.211       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -1.28       |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.00042    |\n",
      "|    n_updates            | 630         |\n",
      "|    policy_gradient_loss | -0.0149     |\n",
      "|    std                  | 0.869       |\n",
      "|    value_loss           | 9.98e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=360000, episode_reward=-207.00 +/- 0.01\n",
      "Episode length: 256.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 256         |\n",
      "|    mean_reward          | -207        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 360000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009787219 |\n",
      "|    clip_fraction        | 0.198       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -1.27       |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0181     |\n",
      "|    n_updates            | 645         |\n",
      "|    policy_gradient_loss | -0.0127     |\n",
      "|    std                  | 0.86        |\n",
      "|    value_loss           | 1.02e-05    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 254    |\n",
      "|    iterations      | 44     |\n",
      "|    time_elapsed    | 1416   |\n",
      "|    total_timesteps | 360448 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 254         |\n",
      "|    iterations           | 45          |\n",
      "|    time_elapsed         | 1446        |\n",
      "|    total_timesteps      | 368640      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012442269 |\n",
      "|    clip_fraction        | 0.21        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -1.27       |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.00497    |\n",
      "|    n_updates            | 660         |\n",
      "|    policy_gradient_loss | -0.0134     |\n",
      "|    std                  | 0.856       |\n",
      "|    value_loss           | 9.52e-06    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 255         |\n",
      "|    iterations           | 46          |\n",
      "|    time_elapsed         | 1475        |\n",
      "|    total_timesteps      | 376832      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011860889 |\n",
      "|    clip_fraction        | 0.227       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -1.26       |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0135     |\n",
      "|    n_updates            | 675         |\n",
      "|    policy_gradient_loss | -0.0144     |\n",
      "|    std                  | 0.852       |\n",
      "|    value_loss           | 1.36e-05    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=380000, episode_reward=-206.38 +/- 0.01\n",
      "Episode length: 256.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 256         |\n",
      "|    mean_reward          | -206        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 380000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012461622 |\n",
      "|    clip_fraction        | 0.227       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -1.25       |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0282     |\n",
      "|    n_updates            | 690         |\n",
      "|    policy_gradient_loss | -0.0155     |\n",
      "|    std                  | 0.844       |\n",
      "|    value_loss           | 1.15e-05    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 254    |\n",
      "|    iterations      | 47     |\n",
      "|    time_elapsed    | 1514   |\n",
      "|    total_timesteps | 385024 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 254         |\n",
      "|    iterations           | 48          |\n",
      "|    time_elapsed         | 1545        |\n",
      "|    total_timesteps      | 393216      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010236334 |\n",
      "|    clip_fraction        | 0.212       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -1.25       |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0153     |\n",
      "|    n_updates            | 705         |\n",
      "|    policy_gradient_loss | -0.0132     |\n",
      "|    std                  | 0.842       |\n",
      "|    value_loss           | 1.64e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=400000, episode_reward=-206.27 +/- 0.01\n",
      "Episode length: 256.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 256         |\n",
      "|    mean_reward          | -206        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 400000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014488722 |\n",
      "|    clip_fraction        | 0.228       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -1.24       |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0166     |\n",
      "|    n_updates            | 720         |\n",
      "|    policy_gradient_loss | -0.0144     |\n",
      "|    std                  | 0.835       |\n",
      "|    value_loss           | 2.49e-05    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 253    |\n",
      "|    iterations      | 49     |\n",
      "|    time_elapsed    | 1582   |\n",
      "|    total_timesteps | 401408 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 253         |\n",
      "|    iterations           | 50          |\n",
      "|    time_elapsed         | 1614        |\n",
      "|    total_timesteps      | 409600      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011906662 |\n",
      "|    clip_fraction        | 0.225       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -1.24       |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0127     |\n",
      "|    n_updates            | 735         |\n",
      "|    policy_gradient_loss | -0.0152     |\n",
      "|    std                  | 0.832       |\n",
      "|    value_loss           | 1.49e-05    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 254         |\n",
      "|    iterations           | 51          |\n",
      "|    time_elapsed         | 1643        |\n",
      "|    total_timesteps      | 417792      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012450673 |\n",
      "|    clip_fraction        | 0.205       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -1.23       |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.00863    |\n",
      "|    n_updates            | 750         |\n",
      "|    policy_gradient_loss | -0.0139     |\n",
      "|    std                  | 0.824       |\n",
      "|    value_loss           | 2.39e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=420000, episode_reward=-205.35 +/- 0.01\n",
      "Episode length: 256.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 256         |\n",
      "|    mean_reward          | -205        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 420000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012246355 |\n",
      "|    clip_fraction        | 0.221       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -1.23       |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0136     |\n",
      "|    n_updates            | 765         |\n",
      "|    policy_gradient_loss | -0.015      |\n",
      "|    std                  | 0.824       |\n",
      "|    value_loss           | 7.86e-06    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 253    |\n",
      "|    iterations      | 52     |\n",
      "|    time_elapsed    | 1679   |\n",
      "|    total_timesteps | 425984 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 253         |\n",
      "|    iterations           | 53          |\n",
      "|    time_elapsed         | 1709        |\n",
      "|    total_timesteps      | 434176      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012762033 |\n",
      "|    clip_fraction        | 0.214       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -1.22       |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0152     |\n",
      "|    n_updates            | 780         |\n",
      "|    policy_gradient_loss | -0.0138     |\n",
      "|    std                  | 0.817       |\n",
      "|    value_loss           | 8.32e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=440000, episode_reward=-204.57 +/- 0.01\n",
      "Episode length: 256.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 256         |\n",
      "|    mean_reward          | -205        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 440000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012940727 |\n",
      "|    clip_fraction        | 0.225       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -1.21       |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0121     |\n",
      "|    n_updates            | 795         |\n",
      "|    policy_gradient_loss | -0.0158     |\n",
      "|    std                  | 0.811       |\n",
      "|    value_loss           | 9.11e-06    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 253    |\n",
      "|    iterations      | 54     |\n",
      "|    time_elapsed    | 1745   |\n",
      "|    total_timesteps | 442368 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 253         |\n",
      "|    iterations           | 55          |\n",
      "|    time_elapsed         | 1774        |\n",
      "|    total_timesteps      | 450560      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012216549 |\n",
      "|    clip_fraction        | 0.21        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -1.2        |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0164     |\n",
      "|    n_updates            | 810         |\n",
      "|    policy_gradient_loss | -0.012      |\n",
      "|    std                  | 0.804       |\n",
      "|    value_loss           | 1.98e-05    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 254         |\n",
      "|    iterations           | 56          |\n",
      "|    time_elapsed         | 1805        |\n",
      "|    total_timesteps      | 458752      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015169288 |\n",
      "|    clip_fraction        | 0.229       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -1.2        |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0104     |\n",
      "|    n_updates            | 825         |\n",
      "|    policy_gradient_loss | -0.0142     |\n",
      "|    std                  | 0.798       |\n",
      "|    value_loss           | 8.24e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=460000, episode_reward=-203.52 +/- 0.01\n",
      "Episode length: 256.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 256         |\n",
      "|    mean_reward          | -204        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 460000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013590962 |\n",
      "|    clip_fraction        | 0.21        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -1.19       |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0265     |\n",
      "|    n_updates            | 840         |\n",
      "|    policy_gradient_loss | -0.0128     |\n",
      "|    std                  | 0.792       |\n",
      "|    value_loss           | 1.81e-05    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 253    |\n",
      "|    iterations      | 57     |\n",
      "|    time_elapsed    | 1842   |\n",
      "|    total_timesteps | 466944 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 253         |\n",
      "|    iterations           | 58          |\n",
      "|    time_elapsed         | 1872        |\n",
      "|    total_timesteps      | 475136      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012218183 |\n",
      "|    clip_fraction        | 0.213       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -1.18       |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.00936    |\n",
      "|    n_updates            | 855         |\n",
      "|    policy_gradient_loss | -0.0114     |\n",
      "|    std                  | 0.785       |\n",
      "|    value_loss           | 1.72e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=480000, episode_reward=-203.12 +/- 0.01\n",
      "Episode length: 256.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 256        |\n",
      "|    mean_reward          | -203       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 480000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02070866 |\n",
      "|    clip_fraction        | 0.232      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -1.17      |\n",
      "|    explained_variance   | 1          |\n",
      "|    learning_rate        | 5e-05      |\n",
      "|    loss                 | -0.0186    |\n",
      "|    n_updates            | 870        |\n",
      "|    policy_gradient_loss | -0.013     |\n",
      "|    std                  | 0.781      |\n",
      "|    value_loss           | 9.79e-06   |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 252    |\n",
      "|    iterations      | 59     |\n",
      "|    time_elapsed    | 1913   |\n",
      "|    total_timesteps | 483328 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 252         |\n",
      "|    iterations           | 60          |\n",
      "|    time_elapsed         | 1943        |\n",
      "|    total_timesteps      | 491520      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014016883 |\n",
      "|    clip_fraction        | 0.244       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -1.17       |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.018      |\n",
      "|    n_updates            | 885         |\n",
      "|    policy_gradient_loss | -0.0154     |\n",
      "|    std                  | 0.773       |\n",
      "|    value_loss           | 1.83e-05    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 253         |\n",
      "|    iterations           | 61          |\n",
      "|    time_elapsed         | 1974        |\n",
      "|    total_timesteps      | 499712      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011462249 |\n",
      "|    clip_fraction        | 0.212       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -1.16       |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0113     |\n",
      "|    n_updates            | 900         |\n",
      "|    policy_gradient_loss | -0.0136     |\n",
      "|    std                  | 0.765       |\n",
      "|    value_loss           | 6.52e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=500000, episode_reward=-201.92 +/- 0.01\n",
      "Episode length: 256.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 256         |\n",
      "|    mean_reward          | -202        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 500000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012018353 |\n",
      "|    clip_fraction        | 0.22        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -1.15       |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0101     |\n",
      "|    n_updates            | 915         |\n",
      "|    policy_gradient_loss | -0.0133     |\n",
      "|    std                  | 0.759       |\n",
      "|    value_loss           | 2.06e-05    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 252    |\n",
      "|    iterations      | 62     |\n",
      "|    time_elapsed    | 2012   |\n",
      "|    total_timesteps | 507904 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 252         |\n",
      "|    iterations           | 63          |\n",
      "|    time_elapsed         | 2042        |\n",
      "|    total_timesteps      | 516096      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013335835 |\n",
      "|    clip_fraction        | 0.214       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -1.14       |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0163     |\n",
      "|    n_updates            | 930         |\n",
      "|    policy_gradient_loss | -0.0122     |\n",
      "|    std                  | 0.757       |\n",
      "|    value_loss           | 6.86e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=520000, episode_reward=-201.50 +/- 0.01\n",
      "Episode length: 256.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 256         |\n",
      "|    mean_reward          | -202        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 520000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012970516 |\n",
      "|    clip_fraction        | 0.216       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -1.14       |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0204     |\n",
      "|    n_updates            | 945         |\n",
      "|    policy_gradient_loss | -0.0123     |\n",
      "|    std                  | 0.755       |\n",
      "|    value_loss           | 9.01e-06    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best mean reward!\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 251    |\n",
      "|    iterations      | 64     |\n",
      "|    time_elapsed    | 2080   |\n",
      "|    total_timesteps | 524288 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 252         |\n",
      "|    iterations           | 65          |\n",
      "|    time_elapsed         | 2111        |\n",
      "|    total_timesteps      | 532480      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013583843 |\n",
      "|    clip_fraction        | 0.241       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -1.13       |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0112     |\n",
      "|    n_updates            | 960         |\n",
      "|    policy_gradient_loss | -0.015      |\n",
      "|    std                  | 0.75        |\n",
      "|    value_loss           | 7.62e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=540000, episode_reward=-201.07 +/- 0.01\n",
      "Episode length: 256.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 256        |\n",
      "|    mean_reward          | -201       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 540000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01374938 |\n",
      "|    clip_fraction        | 0.234      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -1.13      |\n",
      "|    explained_variance   | 1          |\n",
      "|    learning_rate        | 5e-05      |\n",
      "|    loss                 | -0.0182    |\n",
      "|    n_updates            | 975        |\n",
      "|    policy_gradient_loss | -0.0124    |\n",
      "|    std                  | 0.749      |\n",
      "|    value_loss           | 1.17e-05   |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 251    |\n",
      "|    iterations      | 66     |\n",
      "|    time_elapsed    | 2147   |\n",
      "|    total_timesteps | 540672 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 252         |\n",
      "|    iterations           | 67          |\n",
      "|    time_elapsed         | 2177        |\n",
      "|    total_timesteps      | 548864      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012094744 |\n",
      "|    clip_fraction        | 0.209       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -1.13       |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.00953    |\n",
      "|    n_updates            | 990         |\n",
      "|    policy_gradient_loss | -0.0113     |\n",
      "|    std                  | 0.747       |\n",
      "|    value_loss           | 1.37e-05    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 252         |\n",
      "|    iterations           | 68          |\n",
      "|    time_elapsed         | 2207        |\n",
      "|    total_timesteps      | 557056      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012484439 |\n",
      "|    clip_fraction        | 0.231       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -1.12       |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0269     |\n",
      "|    n_updates            | 1005        |\n",
      "|    policy_gradient_loss | -0.0114     |\n",
      "|    std                  | 0.743       |\n",
      "|    value_loss           | 1.64e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=560000, episode_reward=-200.53 +/- 0.01\n",
      "Episode length: 256.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 256         |\n",
      "|    mean_reward          | -201        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 560000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012595922 |\n",
      "|    clip_fraction        | 0.231       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -1.12       |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0164     |\n",
      "|    n_updates            | 1020        |\n",
      "|    policy_gradient_loss | -0.0129     |\n",
      "|    std                  | 0.738       |\n",
      "|    value_loss           | 8.18e-06    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 251    |\n",
      "|    iterations      | 69     |\n",
      "|    time_elapsed    | 2246   |\n",
      "|    total_timesteps | 565248 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 251         |\n",
      "|    iterations           | 70          |\n",
      "|    time_elapsed         | 2277        |\n",
      "|    total_timesteps      | 573440      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011567384 |\n",
      "|    clip_fraction        | 0.229       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -1.11       |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0246     |\n",
      "|    n_updates            | 1035        |\n",
      "|    policy_gradient_loss | -0.0116     |\n",
      "|    std                  | 0.731       |\n",
      "|    value_loss           | 1.71e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=580000, episode_reward=-200.32 +/- 0.01\n",
      "Episode length: 256.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 256         |\n",
      "|    mean_reward          | -200        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 580000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014272241 |\n",
      "|    clip_fraction        | 0.232       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -1.1        |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.00503    |\n",
      "|    n_updates            | 1050        |\n",
      "|    policy_gradient_loss | -0.0125     |\n",
      "|    std                  | 0.726       |\n",
      "|    value_loss           | 1.71e-05    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 251    |\n",
      "|    iterations      | 71     |\n",
      "|    time_elapsed    | 2315   |\n",
      "|    total_timesteps | 581632 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 251         |\n",
      "|    iterations           | 72          |\n",
      "|    time_elapsed         | 2345        |\n",
      "|    total_timesteps      | 589824      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013906704 |\n",
      "|    clip_fraction        | 0.234       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -1.1        |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0124     |\n",
      "|    n_updates            | 1065        |\n",
      "|    policy_gradient_loss | -0.0127     |\n",
      "|    std                  | 0.722       |\n",
      "|    value_loss           | 1.68e-05    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 251         |\n",
      "|    iterations           | 73          |\n",
      "|    time_elapsed         | 2375        |\n",
      "|    total_timesteps      | 598016      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011809203 |\n",
      "|    clip_fraction        | 0.225       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -1.09       |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.023      |\n",
      "|    n_updates            | 1080        |\n",
      "|    policy_gradient_loss | -0.0129     |\n",
      "|    std                  | 0.715       |\n",
      "|    value_loss           | 1.54e-05    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=600000, episode_reward=-199.63 +/- 0.01\n",
      "Episode length: 256.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 256         |\n",
      "|    mean_reward          | -200        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 600000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012202382 |\n",
      "|    clip_fraction        | 0.229       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -1.08       |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0213     |\n",
      "|    n_updates            | 1095        |\n",
      "|    policy_gradient_loss | -0.0103     |\n",
      "|    std                  | 0.711       |\n",
      "|    value_loss           | 8.47e-06    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 251    |\n",
      "|    iterations      | 74     |\n",
      "|    time_elapsed    | 2413   |\n",
      "|    total_timesteps | 606208 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 251         |\n",
      "|    iterations           | 75          |\n",
      "|    time_elapsed         | 2445        |\n",
      "|    total_timesteps      | 614400      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013545433 |\n",
      "|    clip_fraction        | 0.235       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -1.07       |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0194     |\n",
      "|    n_updates            | 1110        |\n",
      "|    policy_gradient_loss | -0.0122     |\n",
      "|    std                  | 0.706       |\n",
      "|    value_loss           | 1.63e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=620000, episode_reward=-199.32 +/- 0.01\n",
      "Episode length: 256.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 256          |\n",
      "|    mean_reward          | -199         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 620000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0121601755 |\n",
      "|    clip_fraction        | 0.228        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -1.07        |\n",
      "|    explained_variance   | 1            |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | -0.0101      |\n",
      "|    n_updates            | 1125         |\n",
      "|    policy_gradient_loss | -0.0113      |\n",
      "|    std                  | 0.704        |\n",
      "|    value_loss           | 1.33e-05     |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 250    |\n",
      "|    iterations      | 76     |\n",
      "|    time_elapsed    | 2482   |\n",
      "|    total_timesteps | 622592 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 251         |\n",
      "|    iterations           | 77          |\n",
      "|    time_elapsed         | 2512        |\n",
      "|    total_timesteps      | 630784      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013057813 |\n",
      "|    clip_fraction        | 0.221       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -1.07       |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0166     |\n",
      "|    n_updates            | 1140        |\n",
      "|    policy_gradient_loss | -0.00949    |\n",
      "|    std                  | 0.701       |\n",
      "|    value_loss           | 9.7e-06     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 249         |\n",
      "|    iterations           | 78          |\n",
      "|    time_elapsed         | 2557        |\n",
      "|    total_timesteps      | 638976      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016382504 |\n",
      "|    clip_fraction        | 0.235       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -1.06       |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0104     |\n",
      "|    n_updates            | 1155        |\n",
      "|    policy_gradient_loss | -0.0121     |\n",
      "|    std                  | 0.699       |\n",
      "|    value_loss           | 1.43e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=640000, episode_reward=-198.76 +/- 0.01\n",
      "Episode length: 256.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 256         |\n",
      "|    mean_reward          | -199        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 640000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017664913 |\n",
      "|    clip_fraction        | 0.234       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -1.06       |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0136     |\n",
      "|    n_updates            | 1170        |\n",
      "|    policy_gradient_loss | -0.0107     |\n",
      "|    std                  | 0.694       |\n",
      "|    value_loss           | 1.11e-05    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 247    |\n",
      "|    iterations      | 79     |\n",
      "|    time_elapsed    | 2614   |\n",
      "|    total_timesteps | 647168 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 247         |\n",
      "|    iterations           | 80          |\n",
      "|    time_elapsed         | 2646        |\n",
      "|    total_timesteps      | 655360      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014340859 |\n",
      "|    clip_fraction        | 0.234       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -1.05       |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0146     |\n",
      "|    n_updates            | 1185        |\n",
      "|    policy_gradient_loss | -0.0123     |\n",
      "|    std                  | 0.69        |\n",
      "|    value_loss           | 9.36e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=660000, episode_reward=-198.06 +/- 0.00\n",
      "Episode length: 256.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 256         |\n",
      "|    mean_reward          | -198        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 660000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014135273 |\n",
      "|    clip_fraction        | 0.25        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -1.05       |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0131     |\n",
      "|    n_updates            | 1200        |\n",
      "|    policy_gradient_loss | -0.0127     |\n",
      "|    std                  | 0.69        |\n",
      "|    value_loss           | 9.74e-06    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 247    |\n",
      "|    iterations      | 81     |\n",
      "|    time_elapsed    | 2683   |\n",
      "|    total_timesteps | 663552 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 247         |\n",
      "|    iterations           | 82          |\n",
      "|    time_elapsed         | 2714        |\n",
      "|    total_timesteps      | 671744      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012772206 |\n",
      "|    clip_fraction        | 0.221       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -1.05       |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0133     |\n",
      "|    n_updates            | 1215        |\n",
      "|    policy_gradient_loss | -0.012      |\n",
      "|    std                  | 0.688       |\n",
      "|    value_loss           | 3.4e-05     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 247         |\n",
      "|    iterations           | 83          |\n",
      "|    time_elapsed         | 2745        |\n",
      "|    total_timesteps      | 679936      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012693645 |\n",
      "|    clip_fraction        | 0.228       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -1.04       |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0112     |\n",
      "|    n_updates            | 1230        |\n",
      "|    policy_gradient_loss | -0.0113     |\n",
      "|    std                  | 0.687       |\n",
      "|    value_loss           | 6.85e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=680000, episode_reward=-197.42 +/- 0.00\n",
      "Episode length: 256.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 256       |\n",
      "|    mean_reward          | -197      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 680000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0132854 |\n",
      "|    clip_fraction        | 0.221     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | -1.04     |\n",
      "|    explained_variance   | 1         |\n",
      "|    learning_rate        | 5e-05     |\n",
      "|    loss                 | 0.00024   |\n",
      "|    n_updates            | 1245      |\n",
      "|    policy_gradient_loss | -0.0118   |\n",
      "|    std                  | 0.681     |\n",
      "|    value_loss           | 5.76e-06  |\n",
      "---------------------------------------\n",
      "New best mean reward!\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 247    |\n",
      "|    iterations      | 84     |\n",
      "|    time_elapsed    | 2784   |\n",
      "|    total_timesteps | 688128 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 247         |\n",
      "|    iterations           | 85          |\n",
      "|    time_elapsed         | 2817        |\n",
      "|    total_timesteps      | 696320      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013299443 |\n",
      "|    clip_fraction        | 0.224       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -1.03       |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0114     |\n",
      "|    n_updates            | 1260        |\n",
      "|    policy_gradient_loss | -0.0102     |\n",
      "|    std                  | 0.675       |\n",
      "|    value_loss           | 6.77e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=700000, episode_reward=-197.09 +/- 0.00\n",
      "Episode length: 256.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 256         |\n",
      "|    mean_reward          | -197        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 700000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015982227 |\n",
      "|    clip_fraction        | 0.249       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -1.02       |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0182     |\n",
      "|    n_updates            | 1275        |\n",
      "|    policy_gradient_loss | -0.012      |\n",
      "|    std                  | 0.671       |\n",
      "|    value_loss           | 7.91e-06    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 246    |\n",
      "|    iterations      | 86     |\n",
      "|    time_elapsed    | 2856   |\n",
      "|    total_timesteps | 704512 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 246         |\n",
      "|    iterations           | 87          |\n",
      "|    time_elapsed         | 2888        |\n",
      "|    total_timesteps      | 712704      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018916873 |\n",
      "|    clip_fraction        | 0.256       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -1.02       |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0087     |\n",
      "|    n_updates            | 1290        |\n",
      "|    policy_gradient_loss | -0.0113     |\n",
      "|    std                  | 0.669       |\n",
      "|    value_loss           | 1.44e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=720000, episode_reward=-196.80 +/- 0.00\n",
      "Episode length: 256.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 256         |\n",
      "|    mean_reward          | -197        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 720000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014629613 |\n",
      "|    clip_fraction        | 0.23        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -1.01       |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0218     |\n",
      "|    n_updates            | 1305        |\n",
      "|    policy_gradient_loss | -0.0111     |\n",
      "|    std                  | 0.666       |\n",
      "|    value_loss           | 2.39e-05    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 245    |\n",
      "|    iterations      | 88     |\n",
      "|    time_elapsed    | 2934   |\n",
      "|    total_timesteps | 720896 |\n",
      "-------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 245        |\n",
      "|    iterations           | 89         |\n",
      "|    time_elapsed         | 2966       |\n",
      "|    total_timesteps      | 729088     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08315076 |\n",
      "|    clip_fraction        | 0.231      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -1.01      |\n",
      "|    explained_variance   | 1          |\n",
      "|    learning_rate        | 5e-05      |\n",
      "|    loss                 | -0.00755   |\n",
      "|    n_updates            | 1320       |\n",
      "|    policy_gradient_loss | -0.00859   |\n",
      "|    std                  | 0.666      |\n",
      "|    value_loss           | 8.01e-06   |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 245         |\n",
      "|    iterations           | 90          |\n",
      "|    time_elapsed         | 2999        |\n",
      "|    total_timesteps      | 737280      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016543958 |\n",
      "|    clip_fraction        | 0.249       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -1.01       |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0114     |\n",
      "|    n_updates            | 1335        |\n",
      "|    policy_gradient_loss | -0.0092     |\n",
      "|    std                  | 0.66        |\n",
      "|    value_loss           | 8.18e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=740000, episode_reward=-195.90 +/- 0.00\n",
      "Episode length: 256.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 256         |\n",
      "|    mean_reward          | -196        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 740000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017608307 |\n",
      "|    clip_fraction        | 0.248       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -1          |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0229     |\n",
      "|    n_updates            | 1350        |\n",
      "|    policy_gradient_loss | -0.0122     |\n",
      "|    std                  | 0.658       |\n",
      "|    value_loss           | 4.85e-06    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best mean reward!\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 245    |\n",
      "|    iterations      | 91     |\n",
      "|    time_elapsed    | 3036   |\n",
      "|    total_timesteps | 745472 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 245         |\n",
      "|    iterations           | 92          |\n",
      "|    time_elapsed         | 3067        |\n",
      "|    total_timesteps      | 753664      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013218155 |\n",
      "|    clip_fraction        | 0.226       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.995      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0163     |\n",
      "|    n_updates            | 1365        |\n",
      "|    policy_gradient_loss | -0.0109     |\n",
      "|    std                  | 0.651       |\n",
      "|    value_loss           | 2.62e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=760000, episode_reward=-195.30 +/- 0.01\n",
      "Episode length: 256.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 256          |\n",
      "|    mean_reward          | -195         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 760000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0129425535 |\n",
      "|    clip_fraction        | 0.203        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -0.989       |\n",
      "|    explained_variance   | 1            |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | -0.0125      |\n",
      "|    n_updates            | 1380         |\n",
      "|    policy_gradient_loss | -0.00674     |\n",
      "|    std                  | 0.65         |\n",
      "|    value_loss           | 6.12e-06     |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 245    |\n",
      "|    iterations      | 93     |\n",
      "|    time_elapsed    | 3105   |\n",
      "|    total_timesteps | 761856 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 245         |\n",
      "|    iterations           | 94          |\n",
      "|    time_elapsed         | 3138        |\n",
      "|    total_timesteps      | 770048      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015491347 |\n",
      "|    clip_fraction        | 0.228       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.988      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0139     |\n",
      "|    n_updates            | 1395        |\n",
      "|    policy_gradient_loss | -0.00889    |\n",
      "|    std                  | 0.65        |\n",
      "|    value_loss           | 5.93e-06    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 245         |\n",
      "|    iterations           | 95          |\n",
      "|    time_elapsed         | 3168        |\n",
      "|    total_timesteps      | 778240      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017956093 |\n",
      "|    clip_fraction        | 0.234       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.984      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0218     |\n",
      "|    n_updates            | 1410        |\n",
      "|    policy_gradient_loss | -0.0101     |\n",
      "|    std                  | 0.645       |\n",
      "|    value_loss           | 6.66e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=780000, episode_reward=-194.53 +/- 0.01\n",
      "Episode length: 256.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 256         |\n",
      "|    mean_reward          | -195        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 780000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014409589 |\n",
      "|    clip_fraction        | 0.231       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.98       |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | 0.00891     |\n",
      "|    n_updates            | 1425        |\n",
      "|    policy_gradient_loss | -0.00993    |\n",
      "|    std                  | 0.645       |\n",
      "|    value_loss           | 5.5e-06     |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 245    |\n",
      "|    iterations      | 96     |\n",
      "|    time_elapsed    | 3205   |\n",
      "|    total_timesteps | 786432 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 245         |\n",
      "|    iterations           | 97          |\n",
      "|    time_elapsed         | 3235        |\n",
      "|    total_timesteps      | 794624      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017311241 |\n",
      "|    clip_fraction        | 0.243       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.977      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.019      |\n",
      "|    n_updates            | 1440        |\n",
      "|    policy_gradient_loss | -0.00953    |\n",
      "|    std                  | 0.64        |\n",
      "|    value_loss           | 6.34e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=800000, episode_reward=-193.60 +/- 0.01\n",
      "Episode length: 256.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 256         |\n",
      "|    mean_reward          | -194        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 800000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019632835 |\n",
      "|    clip_fraction        | 0.256       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.972      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.00826    |\n",
      "|    n_updates            | 1455        |\n",
      "|    policy_gradient_loss | -0.0113     |\n",
      "|    std                  | 0.639       |\n",
      "|    value_loss           | 4.69e-05    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 245    |\n",
      "|    iterations      | 98     |\n",
      "|    time_elapsed    | 3273   |\n",
      "|    total_timesteps | 802816 |\n",
      "-------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 245        |\n",
      "|    iterations           | 99         |\n",
      "|    time_elapsed         | 3303       |\n",
      "|    total_timesteps      | 811008     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01589709 |\n",
      "|    clip_fraction        | 0.245      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -0.969     |\n",
      "|    explained_variance   | 1          |\n",
      "|    learning_rate        | 5e-05      |\n",
      "|    loss                 | -0.00729   |\n",
      "|    n_updates            | 1470       |\n",
      "|    policy_gradient_loss | -0.00944   |\n",
      "|    std                  | 0.637      |\n",
      "|    value_loss           | 4.46e-06   |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 245         |\n",
      "|    iterations           | 100         |\n",
      "|    time_elapsed         | 3333        |\n",
      "|    total_timesteps      | 819200      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015837967 |\n",
      "|    clip_fraction        | 0.235       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.966      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0198     |\n",
      "|    n_updates            | 1485        |\n",
      "|    policy_gradient_loss | -0.0111     |\n",
      "|    std                  | 0.634       |\n",
      "|    value_loss           | 3.65e-06    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=820000, episode_reward=-192.40 +/- 0.01\n",
      "Episode length: 256.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 256         |\n",
      "|    mean_reward          | -192        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 820000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021223128 |\n",
      "|    clip_fraction        | 0.271       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.961      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.00738    |\n",
      "|    n_updates            | 1500        |\n",
      "|    policy_gradient_loss | -0.0146     |\n",
      "|    std                  | 0.631       |\n",
      "|    value_loss           | 7.44e-06    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 245    |\n",
      "|    iterations      | 101    |\n",
      "|    time_elapsed    | 3372   |\n",
      "|    total_timesteps | 827392 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 245         |\n",
      "|    iterations           | 102         |\n",
      "|    time_elapsed         | 3402        |\n",
      "|    total_timesteps      | 835584      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013939932 |\n",
      "|    clip_fraction        | 0.236       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.958      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | 0.00147     |\n",
      "|    n_updates            | 1515        |\n",
      "|    policy_gradient_loss | -0.00942    |\n",
      "|    std                  | 0.63        |\n",
      "|    value_loss           | 5.23e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=840000, episode_reward=-192.16 +/- 0.01\n",
      "Episode length: 256.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 256         |\n",
      "|    mean_reward          | -192        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 840000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018858507 |\n",
      "|    clip_fraction        | 0.274       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.954      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.00505    |\n",
      "|    n_updates            | 1530        |\n",
      "|    policy_gradient_loss | -0.0119     |\n",
      "|    std                  | 0.625       |\n",
      "|    value_loss           | 1.54e-05    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 245    |\n",
      "|    iterations      | 103    |\n",
      "|    time_elapsed    | 3437   |\n",
      "|    total_timesteps | 843776 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 245         |\n",
      "|    iterations           | 104         |\n",
      "|    time_elapsed         | 3467        |\n",
      "|    total_timesteps      | 851968      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014717173 |\n",
      "|    clip_fraction        | 0.246       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.947      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | 0.00951     |\n",
      "|    n_updates            | 1545        |\n",
      "|    policy_gradient_loss | -0.0109     |\n",
      "|    std                  | 0.622       |\n",
      "|    value_loss           | 7.09e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=860000, episode_reward=-191.91 +/- 0.01\n",
      "Episode length: 256.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 256         |\n",
      "|    mean_reward          | -192        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 860000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016062427 |\n",
      "|    clip_fraction        | 0.263       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.942      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0182     |\n",
      "|    n_updates            | 1560        |\n",
      "|    policy_gradient_loss | -0.0104     |\n",
      "|    std                  | 0.62        |\n",
      "|    value_loss           | 1.33e-05    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 245    |\n",
      "|    iterations      | 105    |\n",
      "|    time_elapsed    | 3505   |\n",
      "|    total_timesteps | 860160 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 245         |\n",
      "|    iterations           | 106         |\n",
      "|    time_elapsed         | 3533        |\n",
      "|    total_timesteps      | 868352      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016326755 |\n",
      "|    clip_fraction        | 0.253       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.938      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.00183    |\n",
      "|    n_updates            | 1575        |\n",
      "|    policy_gradient_loss | -0.00698    |\n",
      "|    std                  | 0.617       |\n",
      "|    value_loss           | 9.34e-06    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 245         |\n",
      "|    iterations           | 107         |\n",
      "|    time_elapsed         | 3563        |\n",
      "|    total_timesteps      | 876544      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016782347 |\n",
      "|    clip_fraction        | 0.229       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.932      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.00447    |\n",
      "|    n_updates            | 1590        |\n",
      "|    policy_gradient_loss | -0.00734    |\n",
      "|    std                  | 0.612       |\n",
      "|    value_loss           | 7.11e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=880000, episode_reward=-191.29 +/- 0.01\n",
      "Episode length: 256.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 256         |\n",
      "|    mean_reward          | -191        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 880000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018275656 |\n",
      "|    clip_fraction        | 0.272       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.927      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0164     |\n",
      "|    n_updates            | 1605        |\n",
      "|    policy_gradient_loss | -0.011      |\n",
      "|    std                  | 0.61        |\n",
      "|    value_loss           | 7.03e-06    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 245    |\n",
      "|    iterations      | 108    |\n",
      "|    time_elapsed    | 3601   |\n",
      "|    total_timesteps | 884736 |\n",
      "-------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 245       |\n",
      "|    iterations           | 109       |\n",
      "|    time_elapsed         | 3630      |\n",
      "|    total_timesteps      | 892928    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0194562 |\n",
      "|    clip_fraction        | 0.274     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | -0.922    |\n",
      "|    explained_variance   | 1         |\n",
      "|    learning_rate        | 5e-05     |\n",
      "|    loss                 | -0.0173   |\n",
      "|    n_updates            | 1620      |\n",
      "|    policy_gradient_loss | -0.0129   |\n",
      "|    std                  | 0.607     |\n",
      "|    value_loss           | 7.85e-06  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=900000, episode_reward=-190.57 +/- 0.01\n",
      "Episode length: 256.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 256         |\n",
      "|    mean_reward          | -191        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 900000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015639007 |\n",
      "|    clip_fraction        | 0.25        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.916      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0164     |\n",
      "|    n_updates            | 1635        |\n",
      "|    policy_gradient_loss | -0.0104     |\n",
      "|    std                  | 0.603       |\n",
      "|    value_loss           | 1.17e-05    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 245    |\n",
      "|    iterations      | 110    |\n",
      "|    time_elapsed    | 3670   |\n",
      "|    total_timesteps | 901120 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 245         |\n",
      "|    iterations           | 111         |\n",
      "|    time_elapsed         | 3701        |\n",
      "|    total_timesteps      | 909312      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016903348 |\n",
      "|    clip_fraction        | 0.234       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.912      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.000566   |\n",
      "|    n_updates            | 1650        |\n",
      "|    policy_gradient_loss | -0.00713    |\n",
      "|    std                  | 0.602       |\n",
      "|    value_loss           | 5.11e-05    |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 245        |\n",
      "|    iterations           | 112        |\n",
      "|    time_elapsed         | 3731       |\n",
      "|    total_timesteps      | 917504     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01753184 |\n",
      "|    clip_fraction        | 0.266      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -0.907     |\n",
      "|    explained_variance   | 1          |\n",
      "|    learning_rate        | 5e-05      |\n",
      "|    loss                 | -0.0212    |\n",
      "|    n_updates            | 1665       |\n",
      "|    policy_gradient_loss | -0.0114    |\n",
      "|    std                  | 0.598      |\n",
      "|    value_loss           | 4.98e-06   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=920000, episode_reward=-190.18 +/- 0.01\n",
      "Episode length: 256.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 256        |\n",
      "|    mean_reward          | -190       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 920000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01907982 |\n",
      "|    clip_fraction        | 0.255      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -0.903     |\n",
      "|    explained_variance   | 1          |\n",
      "|    learning_rate        | 5e-05      |\n",
      "|    loss                 | -0.00468   |\n",
      "|    n_updates            | 1680       |\n",
      "|    policy_gradient_loss | -0.0109    |\n",
      "|    std                  | 0.596      |\n",
      "|    value_loss           | 6.38e-06   |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 245    |\n",
      "|    iterations      | 113    |\n",
      "|    time_elapsed    | 3769   |\n",
      "|    total_timesteps | 925696 |\n",
      "-------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 245        |\n",
      "|    iterations           | 114        |\n",
      "|    time_elapsed         | 3798       |\n",
      "|    total_timesteps      | 933888     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01550749 |\n",
      "|    clip_fraction        | 0.254      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -0.9       |\n",
      "|    explained_variance   | 1          |\n",
      "|    learning_rate        | 5e-05      |\n",
      "|    loss                 | -0.017     |\n",
      "|    n_updates            | 1695       |\n",
      "|    policy_gradient_loss | -0.011     |\n",
      "|    std                  | 0.594      |\n",
      "|    value_loss           | 5.78e-06   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=940000, episode_reward=-189.70 +/- 0.01\n",
      "Episode length: 256.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 256          |\n",
      "|    mean_reward          | -190         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 940000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0145000415 |\n",
      "|    clip_fraction        | 0.251        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -0.896       |\n",
      "|    explained_variance   | 1            |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | -0.000303    |\n",
      "|    n_updates            | 1710         |\n",
      "|    policy_gradient_loss | -0.00928     |\n",
      "|    std                  | 0.592        |\n",
      "|    value_loss           | 6.14e-06     |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 245    |\n",
      "|    iterations      | 115    |\n",
      "|    time_elapsed    | 3835   |\n",
      "|    total_timesteps | 942080 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 245         |\n",
      "|    iterations           | 116         |\n",
      "|    time_elapsed         | 3865        |\n",
      "|    total_timesteps      | 950272      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016455183 |\n",
      "|    clip_fraction        | 0.248       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.894      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | 0.00525     |\n",
      "|    n_updates            | 1725        |\n",
      "|    policy_gradient_loss | -0.0119     |\n",
      "|    std                  | 0.59        |\n",
      "|    value_loss           | 6.47e-06    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 246         |\n",
      "|    iterations           | 117         |\n",
      "|    time_elapsed         | 3895        |\n",
      "|    total_timesteps      | 958464      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019754894 |\n",
      "|    clip_fraction        | 0.254       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.893      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0101     |\n",
      "|    n_updates            | 1740        |\n",
      "|    policy_gradient_loss | -0.00864    |\n",
      "|    std                  | 0.592       |\n",
      "|    value_loss           | 4.57e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=960000, episode_reward=-189.34 +/- 0.01\n",
      "Episode length: 256.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 256         |\n",
      "|    mean_reward          | -189        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 960000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022222199 |\n",
      "|    clip_fraction        | 0.279       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.89       |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0136     |\n",
      "|    n_updates            | 1755        |\n",
      "|    policy_gradient_loss | -0.0122     |\n",
      "|    std                  | 0.587       |\n",
      "|    value_loss           | 9.71e-06    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best mean reward!\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 245    |\n",
      "|    iterations      | 118    |\n",
      "|    time_elapsed    | 3931   |\n",
      "|    total_timesteps | 966656 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 246         |\n",
      "|    iterations           | 119         |\n",
      "|    time_elapsed         | 3962        |\n",
      "|    total_timesteps      | 974848      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023674715 |\n",
      "|    clip_fraction        | 0.276       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.884      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.00886    |\n",
      "|    n_updates            | 1770        |\n",
      "|    policy_gradient_loss | -0.0118     |\n",
      "|    std                  | 0.584       |\n",
      "|    value_loss           | 5.38e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=980000, episode_reward=-188.68 +/- 0.01\n",
      "Episode length: 256.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 256         |\n",
      "|    mean_reward          | -189        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 980000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016348898 |\n",
      "|    clip_fraction        | 0.261       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.88       |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0229     |\n",
      "|    n_updates            | 1785        |\n",
      "|    policy_gradient_loss | -0.0135     |\n",
      "|    std                  | 0.582       |\n",
      "|    value_loss           | 3.58e-06    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 245    |\n",
      "|    iterations      | 120    |\n",
      "|    time_elapsed    | 3998   |\n",
      "|    total_timesteps | 983040 |\n",
      "-------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 246        |\n",
      "|    iterations           | 121        |\n",
      "|    time_elapsed         | 4028       |\n",
      "|    total_timesteps      | 991232     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02017083 |\n",
      "|    clip_fraction        | 0.266      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -0.875     |\n",
      "|    explained_variance   | 1          |\n",
      "|    learning_rate        | 5e-05      |\n",
      "|    loss                 | -0.0213    |\n",
      "|    n_updates            | 1800       |\n",
      "|    policy_gradient_loss | -0.0137    |\n",
      "|    std                  | 0.579      |\n",
      "|    value_loss           | 6.04e-06   |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 246        |\n",
      "|    iterations           | 122        |\n",
      "|    time_elapsed         | 4058       |\n",
      "|    total_timesteps      | 999424     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01639005 |\n",
      "|    clip_fraction        | 0.258      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -0.87      |\n",
      "|    explained_variance   | 1          |\n",
      "|    learning_rate        | 5e-05      |\n",
      "|    loss                 | -0.0266    |\n",
      "|    n_updates            | 1815       |\n",
      "|    policy_gradient_loss | -0.0121    |\n",
      "|    std                  | 0.576      |\n",
      "|    value_loss           | 3.39e-06   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=1000000, episode_reward=-187.82 +/- 0.01\n",
      "Episode length: 256.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 256         |\n",
      "|    mean_reward          | -188        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1000000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019904766 |\n",
      "|    clip_fraction        | 0.267       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.864      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0128     |\n",
      "|    n_updates            | 1830        |\n",
      "|    policy_gradient_loss | -0.0131     |\n",
      "|    std                  | 0.572       |\n",
      "|    value_loss           | 2.47e-05    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 246     |\n",
      "|    iterations      | 123     |\n",
      "|    time_elapsed    | 4094    |\n",
      "|    total_timesteps | 1007616 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 246         |\n",
      "|    iterations           | 124         |\n",
      "|    time_elapsed         | 4125        |\n",
      "|    total_timesteps      | 1015808     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019102309 |\n",
      "|    clip_fraction        | 0.266       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.857      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0257     |\n",
      "|    n_updates            | 1845        |\n",
      "|    policy_gradient_loss | -0.00915    |\n",
      "|    std                  | 0.568       |\n",
      "|    value_loss           | 2.92e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1020000, episode_reward=-188.04 +/- 0.01\n",
      "Episode length: 256.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 256        |\n",
      "|    mean_reward          | -188       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1020000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02066798 |\n",
      "|    clip_fraction        | 0.269      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -0.851     |\n",
      "|    explained_variance   | 1          |\n",
      "|    learning_rate        | 5e-05      |\n",
      "|    loss                 | -0.0173    |\n",
      "|    n_updates            | 1860       |\n",
      "|    policy_gradient_loss | -0.0115    |\n",
      "|    std                  | 0.565      |\n",
      "|    value_loss           | 3.62e-06   |\n",
      "----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 246     |\n",
      "|    iterations      | 125     |\n",
      "|    time_elapsed    | 4161    |\n",
      "|    total_timesteps | 1024000 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 246         |\n",
      "|    iterations           | 126         |\n",
      "|    time_elapsed         | 4190        |\n",
      "|    total_timesteps      | 1032192     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016654138 |\n",
      "|    clip_fraction        | 0.25        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.846      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.02       |\n",
      "|    n_updates            | 1875        |\n",
      "|    policy_gradient_loss | -0.00897    |\n",
      "|    std                  | 0.563       |\n",
      "|    value_loss           | 3.03e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1040000, episode_reward=-187.80 +/- 0.00\n",
      "Episode length: 256.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 256         |\n",
      "|    mean_reward          | -188        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1040000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017130762 |\n",
      "|    clip_fraction        | 0.27        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.842      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0192     |\n",
      "|    n_updates            | 1890        |\n",
      "|    policy_gradient_loss | -0.0116     |\n",
      "|    std                  | 0.56        |\n",
      "|    value_loss           | 8.01e-06    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best mean reward!\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 246     |\n",
      "|    iterations      | 127     |\n",
      "|    time_elapsed    | 4227    |\n",
      "|    total_timesteps | 1040384 |\n",
      "--------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 246        |\n",
      "|    iterations           | 128        |\n",
      "|    time_elapsed         | 4257       |\n",
      "|    total_timesteps      | 1048576    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02480932 |\n",
      "|    clip_fraction        | 0.287      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -0.839     |\n",
      "|    explained_variance   | 1          |\n",
      "|    learning_rate        | 5e-05      |\n",
      "|    loss                 | -0.028     |\n",
      "|    n_updates            | 1905       |\n",
      "|    policy_gradient_loss | -0.0128    |\n",
      "|    std                  | 0.56       |\n",
      "|    value_loss           | 6.94e-06   |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 246         |\n",
      "|    iterations           | 129         |\n",
      "|    time_elapsed         | 4287        |\n",
      "|    total_timesteps      | 1056768     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024447914 |\n",
      "|    clip_fraction        | 0.264       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.838      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0208     |\n",
      "|    n_updates            | 1920        |\n",
      "|    policy_gradient_loss | -0.011      |\n",
      "|    std                  | 0.559       |\n",
      "|    value_loss           | 3.07e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1060000, episode_reward=-186.55 +/- 0.01\n",
      "Episode length: 256.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 256        |\n",
      "|    mean_reward          | -187       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1060000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01998726 |\n",
      "|    clip_fraction        | 0.27       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -0.836     |\n",
      "|    explained_variance   | 1          |\n",
      "|    learning_rate        | 5e-05      |\n",
      "|    loss                 | -0.018     |\n",
      "|    n_updates            | 1935       |\n",
      "|    policy_gradient_loss | -0.0107    |\n",
      "|    std                  | 0.558      |\n",
      "|    value_loss           | 1.03e-05   |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 246     |\n",
      "|    iterations      | 130     |\n",
      "|    time_elapsed    | 4326    |\n",
      "|    total_timesteps | 1064960 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 246         |\n",
      "|    iterations           | 131         |\n",
      "|    time_elapsed         | 4356        |\n",
      "|    total_timesteps      | 1073152     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018656911 |\n",
      "|    clip_fraction        | 0.255       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.832      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.00984    |\n",
      "|    n_updates            | 1950        |\n",
      "|    policy_gradient_loss | -0.00977    |\n",
      "|    std                  | 0.555       |\n",
      "|    value_loss           | 5.08e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1080000, episode_reward=-186.47 +/- 0.01\n",
      "Episode length: 256.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 256         |\n",
      "|    mean_reward          | -186        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1080000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018250937 |\n",
      "|    clip_fraction        | 0.258       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.827      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0108     |\n",
      "|    n_updates            | 1965        |\n",
      "|    policy_gradient_loss | -0.00913    |\n",
      "|    std                  | 0.552       |\n",
      "|    value_loss           | 3.21e-05    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 246     |\n",
      "|    iterations      | 132     |\n",
      "|    time_elapsed    | 4394    |\n",
      "|    total_timesteps | 1081344 |\n",
      "--------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 246        |\n",
      "|    iterations           | 133        |\n",
      "|    time_elapsed         | 4423       |\n",
      "|    total_timesteps      | 1089536    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01899018 |\n",
      "|    clip_fraction        | 0.254      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -0.822     |\n",
      "|    explained_variance   | 1          |\n",
      "|    learning_rate        | 5e-05      |\n",
      "|    loss                 | 0.00371    |\n",
      "|    n_updates            | 1980       |\n",
      "|    policy_gradient_loss | -0.00875   |\n",
      "|    std                  | 0.549      |\n",
      "|    value_loss           | 4.82e-06   |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 246         |\n",
      "|    iterations           | 134         |\n",
      "|    time_elapsed         | 4455        |\n",
      "|    total_timesteps      | 1097728     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019657183 |\n",
      "|    clip_fraction        | 0.253       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.82       |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0266     |\n",
      "|    n_updates            | 1995        |\n",
      "|    policy_gradient_loss | -0.00833    |\n",
      "|    std                  | 0.549       |\n",
      "|    value_loss           | 3.86e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1100000, episode_reward=-186.27 +/- 0.00\n",
      "Episode length: 256.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 256         |\n",
      "|    mean_reward          | -186        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1100000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020618692 |\n",
      "|    clip_fraction        | 0.269       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.818      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0144     |\n",
      "|    n_updates            | 2010        |\n",
      "|    policy_gradient_loss | -0.0106     |\n",
      "|    std                  | 0.548       |\n",
      "|    value_loss           | 4.21e-06    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 246     |\n",
      "|    iterations      | 135     |\n",
      "|    time_elapsed    | 4491    |\n",
      "|    total_timesteps | 1105920 |\n",
      "--------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 246        |\n",
      "|    iterations           | 136        |\n",
      "|    time_elapsed         | 4521       |\n",
      "|    total_timesteps      | 1114112    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01628165 |\n",
      "|    clip_fraction        | 0.262      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -0.815     |\n",
      "|    explained_variance   | 1          |\n",
      "|    learning_rate        | 5e-05      |\n",
      "|    loss                 | -0.0188    |\n",
      "|    n_updates            | 2025       |\n",
      "|    policy_gradient_loss | -0.01      |\n",
      "|    std                  | 0.545      |\n",
      "|    value_loss           | 3.07e-06   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=1120000, episode_reward=-186.14 +/- 0.00\n",
      "Episode length: 256.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 256         |\n",
      "|    mean_reward          | -186        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1120000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020224195 |\n",
      "|    clip_fraction        | 0.266       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.812      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0139     |\n",
      "|    n_updates            | 2040        |\n",
      "|    policy_gradient_loss | -0.0111     |\n",
      "|    std                  | 0.545       |\n",
      "|    value_loss           | 4.22e-06    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 246     |\n",
      "|    iterations      | 137     |\n",
      "|    time_elapsed    | 4557    |\n",
      "|    total_timesteps | 1122304 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 246         |\n",
      "|    iterations           | 138         |\n",
      "|    time_elapsed         | 4587        |\n",
      "|    total_timesteps      | 1130496     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020320017 |\n",
      "|    clip_fraction        | 0.26        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.81       |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.00616    |\n",
      "|    n_updates            | 2055        |\n",
      "|    policy_gradient_loss | -0.011      |\n",
      "|    std                  | 0.543       |\n",
      "|    value_loss           | 5.1e-06     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 246         |\n",
      "|    iterations           | 139         |\n",
      "|    time_elapsed         | 4617        |\n",
      "|    total_timesteps      | 1138688     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021107072 |\n",
      "|    clip_fraction        | 0.284       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.806      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0137     |\n",
      "|    n_updates            | 2070        |\n",
      "|    policy_gradient_loss | -0.0118     |\n",
      "|    std                  | 0.54        |\n",
      "|    value_loss           | 2.73e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1140000, episode_reward=-185.38 +/- 0.00\n",
      "Episode length: 256.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 256         |\n",
      "|    mean_reward          | -185        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1140000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027610712 |\n",
      "|    clip_fraction        | 0.284       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.802      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.00312    |\n",
      "|    n_updates            | 2085        |\n",
      "|    policy_gradient_loss | -0.00905    |\n",
      "|    std                  | 0.539       |\n",
      "|    value_loss           | 5.3e-06     |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 246     |\n",
      "|    iterations      | 140     |\n",
      "|    time_elapsed    | 4655    |\n",
      "|    total_timesteps | 1146880 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 246         |\n",
      "|    iterations           | 141         |\n",
      "|    time_elapsed         | 4687        |\n",
      "|    total_timesteps      | 1155072     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020526268 |\n",
      "|    clip_fraction        | 0.269       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.8        |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0127     |\n",
      "|    n_updates            | 2100        |\n",
      "|    policy_gradient_loss | -0.00958    |\n",
      "|    std                  | 0.538       |\n",
      "|    value_loss           | 2.92e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1160000, episode_reward=-184.62 +/- 0.00\n",
      "Episode length: 256.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 256         |\n",
      "|    mean_reward          | -185        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1160000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021848358 |\n",
      "|    clip_fraction        | 0.274       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.795      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0152     |\n",
      "|    n_updates            | 2115        |\n",
      "|    policy_gradient_loss | -0.0115     |\n",
      "|    std                  | 0.533       |\n",
      "|    value_loss           | 3.8e-06     |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 246     |\n",
      "|    iterations      | 142     |\n",
      "|    time_elapsed    | 4725    |\n",
      "|    total_timesteps | 1163264 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 246         |\n",
      "|    iterations           | 143         |\n",
      "|    time_elapsed         | 4755        |\n",
      "|    total_timesteps      | 1171456     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023686979 |\n",
      "|    clip_fraction        | 0.28        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.786      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.00517    |\n",
      "|    n_updates            | 2130        |\n",
      "|    policy_gradient_loss | -0.0109     |\n",
      "|    std                  | 0.529       |\n",
      "|    value_loss           | 2.89e-06    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 246         |\n",
      "|    iterations           | 144         |\n",
      "|    time_elapsed         | 4785        |\n",
      "|    total_timesteps      | 1179648     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019874832 |\n",
      "|    clip_fraction        | 0.271       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.782      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0128     |\n",
      "|    n_updates            | 2145        |\n",
      "|    policy_gradient_loss | -0.0109     |\n",
      "|    std                  | 0.528       |\n",
      "|    value_loss           | 2.23e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1180000, episode_reward=-184.14 +/- 0.00\n",
      "Episode length: 256.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 256         |\n",
      "|    mean_reward          | -184        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1180000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019577589 |\n",
      "|    clip_fraction        | 0.271       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.78       |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0296     |\n",
      "|    n_updates            | 2160        |\n",
      "|    policy_gradient_loss | -0.012      |\n",
      "|    std                  | 0.528       |\n",
      "|    value_loss           | 3.85e-06    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best mean reward!\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 246     |\n",
      "|    iterations      | 145     |\n",
      "|    time_elapsed    | 4823    |\n",
      "|    total_timesteps | 1187840 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 246         |\n",
      "|    iterations           | 146         |\n",
      "|    time_elapsed         | 4854        |\n",
      "|    total_timesteps      | 1196032     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019672718 |\n",
      "|    clip_fraction        | 0.282       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.778      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0147     |\n",
      "|    n_updates            | 2175        |\n",
      "|    policy_gradient_loss | -0.0119     |\n",
      "|    std                  | 0.526       |\n",
      "|    value_loss           | 2.49e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1200000, episode_reward=-183.38 +/- 0.00\n",
      "Episode length: 256.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 256         |\n",
      "|    mean_reward          | -183        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1200000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020329677 |\n",
      "|    clip_fraction        | 0.279       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.771      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0133     |\n",
      "|    n_updates            | 2190        |\n",
      "|    policy_gradient_loss | -0.011      |\n",
      "|    std                  | 0.521       |\n",
      "|    value_loss           | 3.18e-06    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 245     |\n",
      "|    iterations      | 147     |\n",
      "|    time_elapsed    | 4896    |\n",
      "|    total_timesteps | 1204224 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 246         |\n",
      "|    iterations           | 148         |\n",
      "|    time_elapsed         | 4925        |\n",
      "|    total_timesteps      | 1212416     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018114733 |\n",
      "|    clip_fraction        | 0.268       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.762      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0217     |\n",
      "|    n_updates            | 2205        |\n",
      "|    policy_gradient_loss | -0.0107     |\n",
      "|    std                  | 0.516       |\n",
      "|    value_loss           | 4.16e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1220000, episode_reward=-182.98 +/- 0.00\n",
      "Episode length: 256.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 256         |\n",
      "|    mean_reward          | -183        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1220000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017399266 |\n",
      "|    clip_fraction        | 0.265       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.755      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0198     |\n",
      "|    n_updates            | 2220        |\n",
      "|    policy_gradient_loss | -0.00946    |\n",
      "|    std                  | 0.513       |\n",
      "|    value_loss           | 3.47e-06    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 245     |\n",
      "|    iterations      | 149     |\n",
      "|    time_elapsed    | 4962    |\n",
      "|    total_timesteps | 1220608 |\n",
      "--------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 246        |\n",
      "|    iterations           | 150        |\n",
      "|    time_elapsed         | 4991       |\n",
      "|    total_timesteps      | 1228800    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01867317 |\n",
      "|    clip_fraction        | 0.291      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -0.752     |\n",
      "|    explained_variance   | 1          |\n",
      "|    learning_rate        | 5e-05      |\n",
      "|    loss                 | -0.0162    |\n",
      "|    n_updates            | 2235       |\n",
      "|    policy_gradient_loss | -0.0114    |\n",
      "|    std                  | 0.513      |\n",
      "|    value_loss           | 2.37e-06   |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 246         |\n",
      "|    iterations           | 151         |\n",
      "|    time_elapsed         | 5020        |\n",
      "|    total_timesteps      | 1236992     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022567729 |\n",
      "|    clip_fraction        | 0.276       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.752      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.00759    |\n",
      "|    n_updates            | 2250        |\n",
      "|    policy_gradient_loss | -0.0096     |\n",
      "|    std                  | 0.513       |\n",
      "|    value_loss           | 2.48e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1240000, episode_reward=-182.22 +/- 0.01\n",
      "Episode length: 256.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 256         |\n",
      "|    mean_reward          | -182        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1240000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019116756 |\n",
      "|    clip_fraction        | 0.269       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.749      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.00663    |\n",
      "|    n_updates            | 2265        |\n",
      "|    policy_gradient_loss | -0.0094     |\n",
      "|    std                  | 0.51        |\n",
      "|    value_loss           | 3.84e-06    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 246     |\n",
      "|    iterations      | 152     |\n",
      "|    time_elapsed    | 5058    |\n",
      "|    total_timesteps | 1245184 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 246         |\n",
      "|    iterations           | 153         |\n",
      "|    time_elapsed         | 5089        |\n",
      "|    total_timesteps      | 1253376     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020305444 |\n",
      "|    clip_fraction        | 0.264       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.746      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0142     |\n",
      "|    n_updates            | 2280        |\n",
      "|    policy_gradient_loss | -0.00887    |\n",
      "|    std                  | 0.51        |\n",
      "|    value_loss           | 4.48e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1260000, episode_reward=-181.83 +/- 0.00\n",
      "Episode length: 256.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 256         |\n",
      "|    mean_reward          | -182        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1260000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018806927 |\n",
      "|    clip_fraction        | 0.274       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.744      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0286     |\n",
      "|    n_updates            | 2295        |\n",
      "|    policy_gradient_loss | -0.00932    |\n",
      "|    std                  | 0.508       |\n",
      "|    value_loss           | 2.81e-06    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best mean reward!\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 246     |\n",
      "|    iterations      | 154     |\n",
      "|    time_elapsed    | 5127    |\n",
      "|    total_timesteps | 1261568 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 246         |\n",
      "|    iterations           | 155         |\n",
      "|    time_elapsed         | 5157        |\n",
      "|    total_timesteps      | 1269760     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025746843 |\n",
      "|    clip_fraction        | 0.293       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.742      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0191     |\n",
      "|    n_updates            | 2310        |\n",
      "|    policy_gradient_loss | -0.0122     |\n",
      "|    std                  | 0.507       |\n",
      "|    value_loss           | 4.82e-06    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 246         |\n",
      "|    iterations           | 156         |\n",
      "|    time_elapsed         | 5190        |\n",
      "|    total_timesteps      | 1277952     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017659843 |\n",
      "|    clip_fraction        | 0.269       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.738      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | 0.00529     |\n",
      "|    n_updates            | 2325        |\n",
      "|    policy_gradient_loss | -0.00779    |\n",
      "|    std                  | 0.505       |\n",
      "|    value_loss           | 2.18e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1280000, episode_reward=-181.36 +/- 0.00\n",
      "Episode length: 256.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 256         |\n",
      "|    mean_reward          | -181        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1280000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026709303 |\n",
      "|    clip_fraction        | 0.291       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.733      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | 0.00185     |\n",
      "|    n_updates            | 2340        |\n",
      "|    policy_gradient_loss | -0.0119     |\n",
      "|    std                  | 0.503       |\n",
      "|    value_loss           | 2.36e-06    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 245     |\n",
      "|    iterations      | 157     |\n",
      "|    time_elapsed    | 5232    |\n",
      "|    total_timesteps | 1286144 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 245         |\n",
      "|    iterations           | 158         |\n",
      "|    time_elapsed         | 5266        |\n",
      "|    total_timesteps      | 1294336     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020109765 |\n",
      "|    clip_fraction        | 0.294       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.731      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0139     |\n",
      "|    n_updates            | 2355        |\n",
      "|    policy_gradient_loss | -0.0122     |\n",
      "|    std                  | 0.502       |\n",
      "|    value_loss           | 3.23e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1300000, episode_reward=-180.78 +/- 0.00\n",
      "Episode length: 256.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 256         |\n",
      "|    mean_reward          | -181        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1300000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021629015 |\n",
      "|    clip_fraction        | 0.285       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.729      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0123     |\n",
      "|    n_updates            | 2370        |\n",
      "|    policy_gradient_loss | -0.013      |\n",
      "|    std                  | 0.501       |\n",
      "|    value_loss           | 3.68e-06    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 245     |\n",
      "|    iterations      | 159     |\n",
      "|    time_elapsed    | 5306    |\n",
      "|    total_timesteps | 1302528 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 245         |\n",
      "|    iterations           | 160         |\n",
      "|    time_elapsed         | 5345        |\n",
      "|    total_timesteps      | 1310720     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022648726 |\n",
      "|    clip_fraction        | 0.279       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.726      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0105     |\n",
      "|    n_updates            | 2385        |\n",
      "|    policy_gradient_loss | -0.00879    |\n",
      "|    std                  | 0.499       |\n",
      "|    value_loss           | 7.69e-06    |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 244        |\n",
      "|    iterations           | 161        |\n",
      "|    time_elapsed         | 5396       |\n",
      "|    total_timesteps      | 1318912    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02258168 |\n",
      "|    clip_fraction        | 0.247      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -0.723     |\n",
      "|    explained_variance   | 1          |\n",
      "|    learning_rate        | 5e-05      |\n",
      "|    loss                 | -0.0202    |\n",
      "|    n_updates            | 2400       |\n",
      "|    policy_gradient_loss | -0.00476   |\n",
      "|    std                  | 0.498      |\n",
      "|    value_loss           | 1.26e-05   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=1320000, episode_reward=-180.56 +/- 0.00\n",
      "Episode length: 256.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 256         |\n",
      "|    mean_reward          | -181        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1320000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016276926 |\n",
      "|    clip_fraction        | 0.265       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.724      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0152     |\n",
      "|    n_updates            | 2415        |\n",
      "|    policy_gradient_loss | -0.007      |\n",
      "|    std                  | 0.5         |\n",
      "|    value_loss           | 1.96e-05    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 243     |\n",
      "|    iterations      | 162     |\n",
      "|    time_elapsed    | 5440    |\n",
      "|    total_timesteps | 1327104 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 244         |\n",
      "|    iterations           | 163         |\n",
      "|    time_elapsed         | 5471        |\n",
      "|    total_timesteps      | 1335296     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020618934 |\n",
      "|    clip_fraction        | 0.295       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.727      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0115     |\n",
      "|    n_updates            | 2430        |\n",
      "|    policy_gradient_loss | -0.0102     |\n",
      "|    std                  | 0.501       |\n",
      "|    value_loss           | 2.06e-06    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=1340000, episode_reward=-180.12 +/- 0.00\n",
      "Episode length: 256.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 256         |\n",
      "|    mean_reward          | -180        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1340000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018507574 |\n",
      "|    clip_fraction        | 0.27        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.727      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0291     |\n",
      "|    n_updates            | 2445        |\n",
      "|    policy_gradient_loss | -0.0092     |\n",
      "|    std                  | 0.5         |\n",
      "|    value_loss           | 1.94e-06    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 243     |\n",
      "|    iterations      | 164     |\n",
      "|    time_elapsed    | 5511    |\n",
      "|    total_timesteps | 1343488 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 243         |\n",
      "|    iterations           | 165         |\n",
      "|    time_elapsed         | 5542        |\n",
      "|    total_timesteps      | 1351680     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025723659 |\n",
      "|    clip_fraction        | 0.294       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.724      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0194     |\n",
      "|    n_updates            | 2460        |\n",
      "|    policy_gradient_loss | -0.0116     |\n",
      "|    std                  | 0.498       |\n",
      "|    value_loss           | 5.29e-06    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 243         |\n",
      "|    iterations           | 166         |\n",
      "|    time_elapsed         | 5578        |\n",
      "|    total_timesteps      | 1359872     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023397248 |\n",
      "|    clip_fraction        | 0.283       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.721      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | 0.00058     |\n",
      "|    n_updates            | 2475        |\n",
      "|    policy_gradient_loss | -0.0078     |\n",
      "|    std                  | 0.497       |\n",
      "|    value_loss           | 2.48e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1360000, episode_reward=-179.36 +/- 0.00\n",
      "Episode length: 256.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 256         |\n",
      "|    mean_reward          | -179        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1360000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026106028 |\n",
      "|    clip_fraction        | 0.294       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.719      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | 0.00482     |\n",
      "|    n_updates            | 2490        |\n",
      "|    policy_gradient_loss | -0.0101     |\n",
      "|    std                  | 0.496       |\n",
      "|    value_loss           | 3.63e-06    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 243     |\n",
      "|    iterations      | 167     |\n",
      "|    time_elapsed    | 5615    |\n",
      "|    total_timesteps | 1368064 |\n",
      "--------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 243        |\n",
      "|    iterations           | 168        |\n",
      "|    time_elapsed         | 5648       |\n",
      "|    total_timesteps      | 1376256    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02046309 |\n",
      "|    clip_fraction        | 0.281      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -0.717     |\n",
      "|    explained_variance   | 1          |\n",
      "|    learning_rate        | 5e-05      |\n",
      "|    loss                 | -0.0146    |\n",
      "|    n_updates            | 2505       |\n",
      "|    policy_gradient_loss | -0.00823   |\n",
      "|    std                  | 0.496      |\n",
      "|    value_loss           | 4.4e-06    |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=1380000, episode_reward=-179.31 +/- 0.00\n",
      "Episode length: 256.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 256         |\n",
      "|    mean_reward          | -179        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1380000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022953246 |\n",
      "|    clip_fraction        | 0.271       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.716      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0102     |\n",
      "|    n_updates            | 2520        |\n",
      "|    policy_gradient_loss | -0.00925    |\n",
      "|    std                  | 0.494       |\n",
      "|    value_loss           | 2.75e-05    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 243     |\n",
      "|    iterations      | 169     |\n",
      "|    time_elapsed    | 5691    |\n",
      "|    total_timesteps | 1384448 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 243         |\n",
      "|    iterations           | 170         |\n",
      "|    time_elapsed         | 5725        |\n",
      "|    total_timesteps      | 1392640     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024123266 |\n",
      "|    clip_fraction        | 0.283       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.713      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.00971    |\n",
      "|    n_updates            | 2535        |\n",
      "|    policy_gradient_loss | -0.00614    |\n",
      "|    std                  | 0.493       |\n",
      "|    value_loss           | 4.75e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1400000, episode_reward=-178.87 +/- 0.00\n",
      "Episode length: 256.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 256         |\n",
      "|    mean_reward          | -179        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1400000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018295767 |\n",
      "|    clip_fraction        | 0.278       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.711      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.00596    |\n",
      "|    n_updates            | 2550        |\n",
      "|    policy_gradient_loss | -0.00523    |\n",
      "|    std                  | 0.492       |\n",
      "|    value_loss           | 5.33e-06    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 243     |\n",
      "|    iterations      | 171     |\n",
      "|    time_elapsed    | 5763    |\n",
      "|    total_timesteps | 1400832 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 243         |\n",
      "|    iterations           | 172         |\n",
      "|    time_elapsed         | 5794        |\n",
      "|    total_timesteps      | 1409024     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018133976 |\n",
      "|    clip_fraction        | 0.277       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.708      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0101     |\n",
      "|    n_updates            | 2565        |\n",
      "|    policy_gradient_loss | -0.00629    |\n",
      "|    std                  | 0.49        |\n",
      "|    value_loss           | 6.96e-06    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 243         |\n",
      "|    iterations           | 173         |\n",
      "|    time_elapsed         | 5831        |\n",
      "|    total_timesteps      | 1417216     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023722347 |\n",
      "|    clip_fraction        | 0.277       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.707      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.00411    |\n",
      "|    n_updates            | 2580        |\n",
      "|    policy_gradient_loss | -0.007      |\n",
      "|    std                  | 0.491       |\n",
      "|    value_loss           | 7.55e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1420000, episode_reward=-178.36 +/- 0.00\n",
      "Episode length: 256.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 256         |\n",
      "|    mean_reward          | -178        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1420000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019883312 |\n",
      "|    clip_fraction        | 0.272       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.707      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.021      |\n",
      "|    n_updates            | 2595        |\n",
      "|    policy_gradient_loss | -0.00808    |\n",
      "|    std                  | 0.49        |\n",
      "|    value_loss           | 5.06e-06    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 242     |\n",
      "|    iterations      | 174     |\n",
      "|    time_elapsed    | 5868    |\n",
      "|    total_timesteps | 1425408 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 243         |\n",
      "|    iterations           | 175         |\n",
      "|    time_elapsed         | 5898        |\n",
      "|    total_timesteps      | 1433600     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020094328 |\n",
      "|    clip_fraction        | 0.268       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.705      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0189     |\n",
      "|    n_updates            | 2610        |\n",
      "|    policy_gradient_loss | -0.00623    |\n",
      "|    std                  | 0.49        |\n",
      "|    value_loss           | 3.66e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1440000, episode_reward=-177.81 +/- 0.00\n",
      "Episode length: 256.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 256        |\n",
      "|    mean_reward          | -178       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1440000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02233577 |\n",
      "|    clip_fraction        | 0.272      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -0.703     |\n",
      "|    explained_variance   | 1          |\n",
      "|    learning_rate        | 5e-05      |\n",
      "|    loss                 | -0.00332   |\n",
      "|    n_updates            | 2625       |\n",
      "|    policy_gradient_loss | -0.00563   |\n",
      "|    std                  | 0.488      |\n",
      "|    value_loss           | 2.88e-06   |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 242     |\n",
      "|    iterations      | 176     |\n",
      "|    time_elapsed    | 5942    |\n",
      "|    total_timesteps | 1441792 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 242         |\n",
      "|    iterations           | 177         |\n",
      "|    time_elapsed         | 5975        |\n",
      "|    total_timesteps      | 1449984     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020636484 |\n",
      "|    clip_fraction        | 0.28        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.701      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.00945    |\n",
      "|    n_updates            | 2640        |\n",
      "|    policy_gradient_loss | -0.0097     |\n",
      "|    std                  | 0.488       |\n",
      "|    value_loss           | 2.87e-06    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 242         |\n",
      "|    iterations           | 178         |\n",
      "|    time_elapsed         | 6005        |\n",
      "|    total_timesteps      | 1458176     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022287834 |\n",
      "|    clip_fraction        | 0.292       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.701      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.00902    |\n",
      "|    n_updates            | 2655        |\n",
      "|    policy_gradient_loss | -0.00943    |\n",
      "|    std                  | 0.487       |\n",
      "|    value_loss           | 6.68e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1460000, episode_reward=-177.31 +/- 0.01\n",
      "Episode length: 256.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 256         |\n",
      "|    mean_reward          | -177        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1460000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023020826 |\n",
      "|    clip_fraction        | 0.241       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.701      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0123     |\n",
      "|    n_updates            | 2670        |\n",
      "|    policy_gradient_loss | -0.00289    |\n",
      "|    std                  | 0.488       |\n",
      "|    value_loss           | 3.22e-06    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 242     |\n",
      "|    iterations      | 179     |\n",
      "|    time_elapsed    | 6043    |\n",
      "|    total_timesteps | 1466368 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 242         |\n",
      "|    iterations           | 180         |\n",
      "|    time_elapsed         | 6072        |\n",
      "|    total_timesteps      | 1474560     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021742929 |\n",
      "|    clip_fraction        | 0.277       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.699      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0109     |\n",
      "|    n_updates            | 2685        |\n",
      "|    policy_gradient_loss | -0.00598    |\n",
      "|    std                  | 0.486       |\n",
      "|    value_loss           | 2.59e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1480000, episode_reward=-176.87 +/- 0.01\n",
      "Episode length: 256.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 256         |\n",
      "|    mean_reward          | -177        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1480000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023212891 |\n",
      "|    clip_fraction        | 0.283       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.695      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0127     |\n",
      "|    n_updates            | 2700        |\n",
      "|    policy_gradient_loss | -0.00783    |\n",
      "|    std                  | 0.484       |\n",
      "|    value_loss           | 4.25e-06    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best mean reward!\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 242     |\n",
      "|    iterations      | 181     |\n",
      "|    time_elapsed    | 6110    |\n",
      "|    total_timesteps | 1482752 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 242         |\n",
      "|    iterations           | 182         |\n",
      "|    time_elapsed         | 6140        |\n",
      "|    total_timesteps      | 1490944     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023434756 |\n",
      "|    clip_fraction        | 0.302       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.694      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.00695    |\n",
      "|    n_updates            | 2715        |\n",
      "|    policy_gradient_loss | -0.00925    |\n",
      "|    std                  | 0.484       |\n",
      "|    value_loss           | 2.85e-06    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 242         |\n",
      "|    iterations           | 183         |\n",
      "|    time_elapsed         | 6170        |\n",
      "|    total_timesteps      | 1499136     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022976039 |\n",
      "|    clip_fraction        | 0.273       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.694      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.00822    |\n",
      "|    n_updates            | 2730        |\n",
      "|    policy_gradient_loss | -0.00698    |\n",
      "|    std                  | 0.484       |\n",
      "|    value_loss           | 3.17e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1500000, episode_reward=-176.20 +/- 0.01\n",
      "Episode length: 256.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 256        |\n",
      "|    mean_reward          | -176       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1500000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02532117 |\n",
      "|    clip_fraction        | 0.291      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -0.691     |\n",
      "|    explained_variance   | 1          |\n",
      "|    learning_rate        | 5e-05      |\n",
      "|    loss                 | -0.0141    |\n",
      "|    n_updates            | 2745       |\n",
      "|    policy_gradient_loss | -0.00634   |\n",
      "|    std                  | 0.482      |\n",
      "|    value_loss           | 5.81e-06   |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 242     |\n",
      "|    iterations      | 184     |\n",
      "|    time_elapsed    | 6207    |\n",
      "|    total_timesteps | 1507328 |\n",
      "--------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 242        |\n",
      "|    iterations           | 185        |\n",
      "|    time_elapsed         | 6238       |\n",
      "|    total_timesteps      | 1515520    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02203042 |\n",
      "|    clip_fraction        | 0.286      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -0.688     |\n",
      "|    explained_variance   | 1          |\n",
      "|    learning_rate        | 5e-05      |\n",
      "|    loss                 | -0.00314   |\n",
      "|    n_updates            | 2760       |\n",
      "|    policy_gradient_loss | -0.00416   |\n",
      "|    std                  | 0.481      |\n",
      "|    value_loss           | 5.05e-06   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=1520000, episode_reward=-175.92 +/- 0.00\n",
      "Episode length: 256.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 256         |\n",
      "|    mean_reward          | -176        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1520000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020250909 |\n",
      "|    clip_fraction        | 0.277       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.682      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0136     |\n",
      "|    n_updates            | 2775        |\n",
      "|    policy_gradient_loss | -0.00752    |\n",
      "|    std                  | 0.477       |\n",
      "|    value_loss           | 5.68e-06    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 242     |\n",
      "|    iterations      | 186     |\n",
      "|    time_elapsed    | 6281    |\n",
      "|    total_timesteps | 1523712 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 242         |\n",
      "|    iterations           | 187         |\n",
      "|    time_elapsed         | 6316        |\n",
      "|    total_timesteps      | 1531904     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027157886 |\n",
      "|    clip_fraction        | 0.298       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.677      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | 0.00126     |\n",
      "|    n_updates            | 2790        |\n",
      "|    policy_gradient_loss | -0.00425    |\n",
      "|    std                  | 0.475       |\n",
      "|    value_loss           | 5.34e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1540000, episode_reward=-175.65 +/- 0.00\n",
      "Episode length: 256.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 256        |\n",
      "|    mean_reward          | -176       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1540000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02892074 |\n",
      "|    clip_fraction        | 0.297      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -0.672     |\n",
      "|    explained_variance   | 1          |\n",
      "|    learning_rate        | 5e-05      |\n",
      "|    loss                 | -0.00695   |\n",
      "|    n_updates            | 2805       |\n",
      "|    policy_gradient_loss | -0.00431   |\n",
      "|    std                  | 0.473      |\n",
      "|    value_loss           | 5.21e-06   |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 242     |\n",
      "|    iterations      | 188     |\n",
      "|    time_elapsed    | 6357    |\n",
      "|    total_timesteps | 1540096 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 241         |\n",
      "|    iterations           | 189         |\n",
      "|    time_elapsed         | 6400        |\n",
      "|    total_timesteps      | 1548288     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020337932 |\n",
      "|    clip_fraction        | 0.293       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.666      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0154     |\n",
      "|    n_updates            | 2820        |\n",
      "|    policy_gradient_loss | -0.00735    |\n",
      "|    std                  | 0.469       |\n",
      "|    value_loss           | 4.82e-06    |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 241        |\n",
      "|    iterations           | 190        |\n",
      "|    time_elapsed         | 6437       |\n",
      "|    total_timesteps      | 1556480    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01905534 |\n",
      "|    clip_fraction        | 0.276      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -0.66      |\n",
      "|    explained_variance   | 1          |\n",
      "|    learning_rate        | 5e-05      |\n",
      "|    loss                 | -0.0106    |\n",
      "|    n_updates            | 2835       |\n",
      "|    policy_gradient_loss | -0.00301   |\n",
      "|    std                  | 0.468      |\n",
      "|    value_loss           | 4.47e-06   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=1560000, episode_reward=-174.97 +/- 0.01\n",
      "Episode length: 256.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 256         |\n",
      "|    mean_reward          | -175        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1560000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025631912 |\n",
      "|    clip_fraction        | 0.277       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.658      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.00611    |\n",
      "|    n_updates            | 2850        |\n",
      "|    policy_gradient_loss | -0.00443    |\n",
      "|    std                  | 0.467       |\n",
      "|    value_loss           | 2.83e-06    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 241     |\n",
      "|    iterations      | 191     |\n",
      "|    time_elapsed    | 6482    |\n",
      "|    total_timesteps | 1564672 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 241         |\n",
      "|    iterations           | 192         |\n",
      "|    time_elapsed         | 6512        |\n",
      "|    total_timesteps      | 1572864     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022289868 |\n",
      "|    clip_fraction        | 0.285       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.658      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0137     |\n",
      "|    n_updates            | 2865        |\n",
      "|    policy_gradient_loss | -0.00724    |\n",
      "|    std                  | 0.467       |\n",
      "|    value_loss           | 5.21e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1580000, episode_reward=-174.70 +/- 0.01\n",
      "Episode length: 256.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 256        |\n",
      "|    mean_reward          | -175       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1580000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03250273 |\n",
      "|    clip_fraction        | 0.291      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -0.657     |\n",
      "|    explained_variance   | 1          |\n",
      "|    learning_rate        | 5e-05      |\n",
      "|    loss                 | 0.00587    |\n",
      "|    n_updates            | 2880       |\n",
      "|    policy_gradient_loss | -0.00625   |\n",
      "|    std                  | 0.466      |\n",
      "|    value_loss           | 3.32e-06   |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 241     |\n",
      "|    iterations      | 193     |\n",
      "|    time_elapsed    | 6554    |\n",
      "|    total_timesteps | 1581056 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 241         |\n",
      "|    iterations           | 194         |\n",
      "|    time_elapsed         | 6588        |\n",
      "|    total_timesteps      | 1589248     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023879295 |\n",
      "|    clip_fraction        | 0.288       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.656      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0133     |\n",
      "|    n_updates            | 2895        |\n",
      "|    policy_gradient_loss | -0.00687    |\n",
      "|    std                  | 0.466       |\n",
      "|    value_loss           | 9.07e-06    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 241         |\n",
      "|    iterations           | 195         |\n",
      "|    time_elapsed         | 6623        |\n",
      "|    total_timesteps      | 1597440     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022731207 |\n",
      "|    clip_fraction        | 0.293       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.658      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | 0.0011      |\n",
      "|    n_updates            | 2910        |\n",
      "|    policy_gradient_loss | -0.00834    |\n",
      "|    std                  | 0.468       |\n",
      "|    value_loss           | 2.54e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1600000, episode_reward=-174.25 +/- 0.01\n",
      "Episode length: 256.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 256         |\n",
      "|    mean_reward          | -174        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1600000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025577713 |\n",
      "|    clip_fraction        | 0.295       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.66       |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.00643    |\n",
      "|    n_updates            | 2925        |\n",
      "|    policy_gradient_loss | -0.00785    |\n",
      "|    std                  | 0.468       |\n",
      "|    value_loss           | 2.4e-06     |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 240     |\n",
      "|    iterations      | 196     |\n",
      "|    time_elapsed    | 6663    |\n",
      "|    total_timesteps | 1605632 |\n",
      "--------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 240        |\n",
      "|    iterations           | 197        |\n",
      "|    time_elapsed         | 6701       |\n",
      "|    total_timesteps      | 1613824    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03508727 |\n",
      "|    clip_fraction        | 0.294      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -0.661     |\n",
      "|    explained_variance   | 1          |\n",
      "|    learning_rate        | 5e-05      |\n",
      "|    loss                 | -1.6e-05   |\n",
      "|    n_updates            | 2940       |\n",
      "|    policy_gradient_loss | -0.00839   |\n",
      "|    std                  | 0.469      |\n",
      "|    value_loss           | 2.84e-06   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=1620000, episode_reward=-173.54 +/- 0.01\n",
      "Episode length: 256.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 256         |\n",
      "|    mean_reward          | -174        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1620000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021325132 |\n",
      "|    clip_fraction        | 0.283       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.659      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0109     |\n",
      "|    n_updates            | 2955        |\n",
      "|    policy_gradient_loss | -0.00866    |\n",
      "|    std                  | 0.467       |\n",
      "|    value_loss           | 3.55e-06    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 240     |\n",
      "|    iterations      | 198     |\n",
      "|    time_elapsed    | 6744    |\n",
      "|    total_timesteps | 1622016 |\n",
      "--------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 240        |\n",
      "|    iterations           | 199        |\n",
      "|    time_elapsed         | 6777       |\n",
      "|    total_timesteps      | 1630208    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02943182 |\n",
      "|    clip_fraction        | 0.284      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -0.656     |\n",
      "|    explained_variance   | 1          |\n",
      "|    learning_rate        | 5e-05      |\n",
      "|    loss                 | -0.0145    |\n",
      "|    n_updates            | 2970       |\n",
      "|    policy_gradient_loss | -0.00621   |\n",
      "|    std                  | 0.466      |\n",
      "|    value_loss           | 3.77e-06   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 240         |\n",
      "|    iterations           | 200         |\n",
      "|    time_elapsed         | 6810        |\n",
      "|    total_timesteps      | 1638400     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022075407 |\n",
      "|    clip_fraction        | 0.287       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.655      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0055     |\n",
      "|    n_updates            | 2985        |\n",
      "|    policy_gradient_loss | -0.00299    |\n",
      "|    std                  | 0.466       |\n",
      "|    value_loss           | 5.87e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1640000, episode_reward=-172.92 +/- 0.01\n",
      "Episode length: 256.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 256        |\n",
      "|    mean_reward          | -173       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1640000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02232327 |\n",
      "|    clip_fraction        | 0.284      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -0.654     |\n",
      "|    explained_variance   | 1          |\n",
      "|    learning_rate        | 5e-05      |\n",
      "|    loss                 | 0.00303    |\n",
      "|    n_updates            | 3000       |\n",
      "|    policy_gradient_loss | -0.0065    |\n",
      "|    std                  | 0.464      |\n",
      "|    value_loss           | 2.11e-06   |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 240     |\n",
      "|    iterations      | 201     |\n",
      "|    time_elapsed    | 6855    |\n",
      "|    total_timesteps | 1646592 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 240         |\n",
      "|    iterations           | 202         |\n",
      "|    time_elapsed         | 6891        |\n",
      "|    total_timesteps      | 1654784     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023139168 |\n",
      "|    clip_fraction        | 0.303       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.653      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0227     |\n",
      "|    n_updates            | 3015        |\n",
      "|    policy_gradient_loss | -0.00707    |\n",
      "|    std                  | 0.466       |\n",
      "|    value_loss           | 3.5e-06     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1660000, episode_reward=-172.72 +/- 0.01\n",
      "Episode length: 256.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 256         |\n",
      "|    mean_reward          | -173        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1660000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025550481 |\n",
      "|    clip_fraction        | 0.289       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.655      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0135     |\n",
      "|    n_updates            | 3030        |\n",
      "|    policy_gradient_loss | -0.00423    |\n",
      "|    std                  | 0.466       |\n",
      "|    value_loss           | 7.09e-06    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 239     |\n",
      "|    iterations      | 203     |\n",
      "|    time_elapsed    | 6935    |\n",
      "|    total_timesteps | 1662976 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 239         |\n",
      "|    iterations           | 204         |\n",
      "|    time_elapsed         | 6969        |\n",
      "|    total_timesteps      | 1671168     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018770931 |\n",
      "|    clip_fraction        | 0.278       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.653      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.00776    |\n",
      "|    n_updates            | 3045        |\n",
      "|    policy_gradient_loss | -0.00517    |\n",
      "|    std                  | 0.464       |\n",
      "|    value_loss           | 3.31e-06    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 239         |\n",
      "|    iterations           | 205         |\n",
      "|    time_elapsed         | 7003        |\n",
      "|    total_timesteps      | 1679360     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023587648 |\n",
      "|    clip_fraction        | 0.297       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.65       |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0187     |\n",
      "|    n_updates            | 3060        |\n",
      "|    policy_gradient_loss | -0.00806    |\n",
      "|    std                  | 0.463       |\n",
      "|    value_loss           | 3e-06       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1680000, episode_reward=-172.25 +/- 0.01\n",
      "Episode length: 256.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 256         |\n",
      "|    mean_reward          | -172        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1680000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025074352 |\n",
      "|    clip_fraction        | 0.299       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.648      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0216     |\n",
      "|    n_updates            | 3075        |\n",
      "|    policy_gradient_loss | -0.00827    |\n",
      "|    std                  | 0.462       |\n",
      "|    value_loss           | 4.52e-06    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 239     |\n",
      "|    iterations      | 206     |\n",
      "|    time_elapsed    | 7044    |\n",
      "|    total_timesteps | 1687552 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 239         |\n",
      "|    iterations           | 207         |\n",
      "|    time_elapsed         | 7075        |\n",
      "|    total_timesteps      | 1695744     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027815294 |\n",
      "|    clip_fraction        | 0.308       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.644      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.00205    |\n",
      "|    n_updates            | 3090        |\n",
      "|    policy_gradient_loss | -0.00553    |\n",
      "|    std                  | 0.46        |\n",
      "|    value_loss           | 4.01e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1700000, episode_reward=-172.18 +/- 0.01\n",
      "Episode length: 256.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 256         |\n",
      "|    mean_reward          | -172        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1700000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.037412126 |\n",
      "|    clip_fraction        | 0.304       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.641      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | 0.00215     |\n",
      "|    n_updates            | 3105        |\n",
      "|    policy_gradient_loss | -0.00521    |\n",
      "|    std                  | 0.459       |\n",
      "|    value_loss           | 2.56e-06    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best mean reward!\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 239     |\n",
      "|    iterations      | 208     |\n",
      "|    time_elapsed    | 7117    |\n",
      "|    total_timesteps | 1703936 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 239         |\n",
      "|    iterations           | 209         |\n",
      "|    time_elapsed         | 7154        |\n",
      "|    total_timesteps      | 1712128     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028164813 |\n",
      "|    clip_fraction        | 0.298       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.639      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.00271    |\n",
      "|    n_updates            | 3120        |\n",
      "|    policy_gradient_loss | -0.0082     |\n",
      "|    std                  | 0.458       |\n",
      "|    value_loss           | 6.33e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1720000, episode_reward=-171.87 +/- 0.01\n",
      "Episode length: 256.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 256         |\n",
      "|    mean_reward          | -172        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1720000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022032592 |\n",
      "|    clip_fraction        | 0.288       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.635      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0117     |\n",
      "|    n_updates            | 3135        |\n",
      "|    policy_gradient_loss | -0.00719    |\n",
      "|    std                  | 0.456       |\n",
      "|    value_loss           | 3.1e-06     |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 238     |\n",
      "|    iterations      | 210     |\n",
      "|    time_elapsed    | 7198    |\n",
      "|    total_timesteps | 1720320 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 238         |\n",
      "|    iterations           | 211         |\n",
      "|    time_elapsed         | 7232        |\n",
      "|    total_timesteps      | 1728512     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022419885 |\n",
      "|    clip_fraction        | 0.303       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.632      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0132     |\n",
      "|    n_updates            | 3150        |\n",
      "|    policy_gradient_loss | -0.00749    |\n",
      "|    std                  | 0.455       |\n",
      "|    value_loss           | 2.25e-06    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 238         |\n",
      "|    iterations           | 212         |\n",
      "|    time_elapsed         | 7269        |\n",
      "|    total_timesteps      | 1736704     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028999483 |\n",
      "|    clip_fraction        | 0.293       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.628      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.015      |\n",
      "|    n_updates            | 3165        |\n",
      "|    policy_gradient_loss | -0.008      |\n",
      "|    std                  | 0.452       |\n",
      "|    value_loss           | 2.55e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1740000, episode_reward=-171.22 +/- 0.01\n",
      "Episode length: 256.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 256        |\n",
      "|    mean_reward          | -171       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1740000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02346401 |\n",
      "|    clip_fraction        | 0.291      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -0.622     |\n",
      "|    explained_variance   | 1          |\n",
      "|    learning_rate        | 5e-05      |\n",
      "|    loss                 | -0.0249    |\n",
      "|    n_updates            | 3180       |\n",
      "|    policy_gradient_loss | -0.0078    |\n",
      "|    std                  | 0.449      |\n",
      "|    value_loss           | 3.35e-06   |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 238     |\n",
      "|    iterations      | 213     |\n",
      "|    time_elapsed    | 7311    |\n",
      "|    total_timesteps | 1744896 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 238         |\n",
      "|    iterations           | 214         |\n",
      "|    time_elapsed         | 7348        |\n",
      "|    total_timesteps      | 1753088     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022180807 |\n",
      "|    clip_fraction        | 0.296       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.615      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.00994    |\n",
      "|    n_updates            | 3195        |\n",
      "|    policy_gradient_loss | -0.00826    |\n",
      "|    std                  | 0.447       |\n",
      "|    value_loss           | 4.11e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1760000, episode_reward=-171.49 +/- 0.01\n",
      "Episode length: 256.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 256         |\n",
      "|    mean_reward          | -171        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1760000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028809639 |\n",
      "|    clip_fraction        | 0.282       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.614      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | 0.0233      |\n",
      "|    n_updates            | 3210        |\n",
      "|    policy_gradient_loss | -0.00718    |\n",
      "|    std                  | 0.448       |\n",
      "|    value_loss           | 2.29e-06    |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 237     |\n",
      "|    iterations      | 215     |\n",
      "|    time_elapsed    | 7406    |\n",
      "|    total_timesteps | 1761280 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 237         |\n",
      "|    iterations           | 216         |\n",
      "|    time_elapsed         | 7449        |\n",
      "|    total_timesteps      | 1769472     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024292126 |\n",
      "|    clip_fraction        | 0.291       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.617      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.018      |\n",
      "|    n_updates            | 3225        |\n",
      "|    policy_gradient_loss | -0.00697    |\n",
      "|    std                  | 0.449       |\n",
      "|    value_loss           | 2.47e-06    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 237         |\n",
      "|    iterations           | 217         |\n",
      "|    time_elapsed         | 7494        |\n",
      "|    total_timesteps      | 1777664     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029776685 |\n",
      "|    clip_fraction        | 0.298       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.616      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0201     |\n",
      "|    n_updates            | 3240        |\n",
      "|    policy_gradient_loss | -0.00793    |\n",
      "|    std                  | 0.447       |\n",
      "|    value_loss           | 2.56e-06    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=1780000, episode_reward=-170.92 +/- 0.01\n",
      "Episode length: 256.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 256       |\n",
      "|    mean_reward          | -171      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 1780000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0190512 |\n",
      "|    clip_fraction        | 0.311     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | -0.614    |\n",
      "|    explained_variance   | 1         |\n",
      "|    learning_rate        | 5e-05     |\n",
      "|    loss                 | 0.0136    |\n",
      "|    n_updates            | 3255      |\n",
      "|    policy_gradient_loss | -0.00877  |\n",
      "|    std                  | 0.447     |\n",
      "|    value_loss           | 2.65e-06  |\n",
      "---------------------------------------\n",
      "New best mean reward!\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 237     |\n",
      "|    iterations      | 218     |\n",
      "|    time_elapsed    | 7534    |\n",
      "|    total_timesteps | 1785856 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 237         |\n",
      "|    iterations           | 219         |\n",
      "|    time_elapsed         | 7567        |\n",
      "|    total_timesteps      | 1794048     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025279814 |\n",
      "|    clip_fraction        | 0.318       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.615      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | 0.00315     |\n",
      "|    n_updates            | 3270        |\n",
      "|    policy_gradient_loss | -0.00797    |\n",
      "|    std                  | 0.448       |\n",
      "|    value_loss           | 1.1e-05     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1800000, episode_reward=-170.97 +/- 0.01\n",
      "Episode length: 256.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 256        |\n",
      "|    mean_reward          | -171       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1800000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02422921 |\n",
      "|    clip_fraction        | 0.291      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -0.616     |\n",
      "|    explained_variance   | 1          |\n",
      "|    learning_rate        | 5e-05      |\n",
      "|    loss                 | -0.000587  |\n",
      "|    n_updates            | 3285       |\n",
      "|    policy_gradient_loss | -0.00627   |\n",
      "|    std                  | 0.448      |\n",
      "|    value_loss           | 4.72e-06   |\n",
      "----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 236     |\n",
      "|    iterations      | 220     |\n",
      "|    time_elapsed    | 7605    |\n",
      "|    total_timesteps | 1802240 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 237         |\n",
      "|    iterations           | 221         |\n",
      "|    time_elapsed         | 7638        |\n",
      "|    total_timesteps      | 1810432     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025631139 |\n",
      "|    clip_fraction        | 0.299       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.615      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0201     |\n",
      "|    n_updates            | 3300        |\n",
      "|    policy_gradient_loss | -0.00721    |\n",
      "|    std                  | 0.447       |\n",
      "|    value_loss           | 2.83e-06    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 237         |\n",
      "|    iterations           | 222         |\n",
      "|    time_elapsed         | 7671        |\n",
      "|    total_timesteps      | 1818624     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025053613 |\n",
      "|    clip_fraction        | 0.299       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.613      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.00515    |\n",
      "|    n_updates            | 3315        |\n",
      "|    policy_gradient_loss | -0.00877    |\n",
      "|    std                  | 0.446       |\n",
      "|    value_loss           | 9.83e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1820000, episode_reward=-170.27 +/- 0.01\n",
      "Episode length: 256.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 256         |\n",
      "|    mean_reward          | -170        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1820000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021887816 |\n",
      "|    clip_fraction        | 0.307       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.61       |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0145     |\n",
      "|    n_updates            | 3330        |\n",
      "|    policy_gradient_loss | -0.00767    |\n",
      "|    std                  | 0.445       |\n",
      "|    value_loss           | 2.67e-06    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 236     |\n",
      "|    iterations      | 223     |\n",
      "|    time_elapsed    | 7712    |\n",
      "|    total_timesteps | 1826816 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 236         |\n",
      "|    iterations           | 224         |\n",
      "|    time_elapsed         | 7746        |\n",
      "|    total_timesteps      | 1835008     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025434727 |\n",
      "|    clip_fraction        | 0.291       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.609      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.00134    |\n",
      "|    n_updates            | 3345        |\n",
      "|    policy_gradient_loss | -0.0056     |\n",
      "|    std                  | 0.444       |\n",
      "|    value_loss           | 2.33e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1840000, episode_reward=-170.03 +/- 0.01\n",
      "Episode length: 256.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 256         |\n",
      "|    mean_reward          | -170        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1840000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024993008 |\n",
      "|    clip_fraction        | 0.297       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.608      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0149     |\n",
      "|    n_updates            | 3360        |\n",
      "|    policy_gradient_loss | -0.00875    |\n",
      "|    std                  | 0.444       |\n",
      "|    value_loss           | 3.12e-06    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 236     |\n",
      "|    iterations      | 225     |\n",
      "|    time_elapsed    | 7791    |\n",
      "|    total_timesteps | 1843200 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 236         |\n",
      "|    iterations           | 226         |\n",
      "|    time_elapsed         | 7826        |\n",
      "|    total_timesteps      | 1851392     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024339247 |\n",
      "|    clip_fraction        | 0.292       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.606      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.00405    |\n",
      "|    n_updates            | 3375        |\n",
      "|    policy_gradient_loss | -0.00664    |\n",
      "|    std                  | 0.443       |\n",
      "|    value_loss           | 2.25e-06    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 236        |\n",
      "|    iterations           | 227        |\n",
      "|    time_elapsed         | 7861       |\n",
      "|    total_timesteps      | 1859584    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02594838 |\n",
      "|    clip_fraction        | 0.304      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -0.603     |\n",
      "|    explained_variance   | 1          |\n",
      "|    learning_rate        | 5e-05      |\n",
      "|    loss                 | -0.00284   |\n",
      "|    n_updates            | 3390       |\n",
      "|    policy_gradient_loss | -0.00784   |\n",
      "|    std                  | 0.441      |\n",
      "|    value_loss           | 2.37e-06   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=1860000, episode_reward=-169.47 +/- 0.01\n",
      "Episode length: 256.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 256         |\n",
      "|    mean_reward          | -169        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1860000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027720049 |\n",
      "|    clip_fraction        | 0.293       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.603      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.00146    |\n",
      "|    n_updates            | 3405        |\n",
      "|    policy_gradient_loss | -0.00725    |\n",
      "|    std                  | 0.443       |\n",
      "|    value_loss           | 3.84e-06    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 236     |\n",
      "|    iterations      | 228     |\n",
      "|    time_elapsed    | 7901    |\n",
      "|    total_timesteps | 1867776 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 236         |\n",
      "|    iterations           | 229         |\n",
      "|    time_elapsed         | 7931        |\n",
      "|    total_timesteps      | 1875968     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025293417 |\n",
      "|    clip_fraction        | 0.305       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.603      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | 0.0184      |\n",
      "|    n_updates            | 3420        |\n",
      "|    policy_gradient_loss | -0.00653    |\n",
      "|    std                  | 0.441       |\n",
      "|    value_loss           | 1.66e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1880000, episode_reward=-169.00 +/- 0.01\n",
      "Episode length: 256.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 256         |\n",
      "|    mean_reward          | -169        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1880000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023590736 |\n",
      "|    clip_fraction        | 0.284       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.601      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0144     |\n",
      "|    n_updates            | 3435        |\n",
      "|    policy_gradient_loss | -0.00517    |\n",
      "|    std                  | 0.441       |\n",
      "|    value_loss           | 2.42e-06    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 236     |\n",
      "|    iterations      | 230     |\n",
      "|    time_elapsed    | 7969    |\n",
      "|    total_timesteps | 1884160 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 236         |\n",
      "|    iterations           | 231         |\n",
      "|    time_elapsed         | 8003        |\n",
      "|    total_timesteps      | 1892352     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025302926 |\n",
      "|    clip_fraction        | 0.295       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.601      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | 0.00613     |\n",
      "|    n_updates            | 3450        |\n",
      "|    policy_gradient_loss | -0.00619    |\n",
      "|    std                  | 0.442       |\n",
      "|    value_loss           | 2.58e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1900000, episode_reward=-168.81 +/- 0.01\n",
      "Episode length: 256.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 256         |\n",
      "|    mean_reward          | -169        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1900000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030017864 |\n",
      "|    clip_fraction        | 0.292       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.601      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | 0.00533     |\n",
      "|    n_updates            | 3465        |\n",
      "|    policy_gradient_loss | -0.00367    |\n",
      "|    std                  | 0.441       |\n",
      "|    value_loss           | 2.51e-06    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 236     |\n",
      "|    iterations      | 232     |\n",
      "|    time_elapsed    | 8042    |\n",
      "|    total_timesteps | 1900544 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 236         |\n",
      "|    iterations           | 233         |\n",
      "|    time_elapsed         | 8076        |\n",
      "|    total_timesteps      | 1908736     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023507692 |\n",
      "|    clip_fraction        | 0.312       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.601      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0132     |\n",
      "|    n_updates            | 3480        |\n",
      "|    policy_gradient_loss | -0.00352    |\n",
      "|    std                  | 0.441       |\n",
      "|    value_loss           | 2.59e-06    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 236         |\n",
      "|    iterations           | 234         |\n",
      "|    time_elapsed         | 8108        |\n",
      "|    total_timesteps      | 1916928     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027932096 |\n",
      "|    clip_fraction        | 0.3         |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.6        |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | 0.00203     |\n",
      "|    n_updates            | 3495        |\n",
      "|    policy_gradient_loss | -0.00595    |\n",
      "|    std                  | 0.441       |\n",
      "|    value_loss           | 1.74e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1920000, episode_reward=-167.99 +/- 0.01\n",
      "Episode length: 256.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 256         |\n",
      "|    mean_reward          | -168        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1920000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026541263 |\n",
      "|    clip_fraction        | 0.313       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.598      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.00929    |\n",
      "|    n_updates            | 3510        |\n",
      "|    policy_gradient_loss | -0.00707    |\n",
      "|    std                  | 0.439       |\n",
      "|    value_loss           | 5.73e-06    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best mean reward!\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 236     |\n",
      "|    iterations      | 235     |\n",
      "|    time_elapsed    | 8150    |\n",
      "|    total_timesteps | 1925120 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 236         |\n",
      "|    iterations           | 236         |\n",
      "|    time_elapsed         | 8185        |\n",
      "|    total_timesteps      | 1933312     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027491085 |\n",
      "|    clip_fraction        | 0.314       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.596      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0135     |\n",
      "|    n_updates            | 3525        |\n",
      "|    policy_gradient_loss | -0.00938    |\n",
      "|    std                  | 0.439       |\n",
      "|    value_loss           | 1.92e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1940000, episode_reward=-167.55 +/- 0.01\n",
      "Episode length: 256.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 256         |\n",
      "|    mean_reward          | -168        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1940000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027725372 |\n",
      "|    clip_fraction        | 0.313       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.594      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.00386    |\n",
      "|    n_updates            | 3540        |\n",
      "|    policy_gradient_loss | -0.00742    |\n",
      "|    std                  | 0.437       |\n",
      "|    value_loss           | 0.000146    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 235     |\n",
      "|    iterations      | 237     |\n",
      "|    time_elapsed    | 8232    |\n",
      "|    total_timesteps | 1941504 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 235         |\n",
      "|    iterations           | 238         |\n",
      "|    time_elapsed         | 8268        |\n",
      "|    total_timesteps      | 1949696     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023904372 |\n",
      "|    clip_fraction        | 0.308       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.593      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.00322    |\n",
      "|    n_updates            | 3555        |\n",
      "|    policy_gradient_loss | -0.00779    |\n",
      "|    std                  | 0.438       |\n",
      "|    value_loss           | 2.22e-06    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 235         |\n",
      "|    iterations           | 239         |\n",
      "|    time_elapsed         | 8300        |\n",
      "|    total_timesteps      | 1957888     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026824724 |\n",
      "|    clip_fraction        | 0.318       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.595      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0105     |\n",
      "|    n_updates            | 3570        |\n",
      "|    policy_gradient_loss | -0.00805    |\n",
      "|    std                  | 0.439       |\n",
      "|    value_loss           | 2.26e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1960000, episode_reward=-167.14 +/- 0.01\n",
      "Episode length: 256.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 256         |\n",
      "|    mean_reward          | -167        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1960000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022487944 |\n",
      "|    clip_fraction        | 0.306       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.597      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0172     |\n",
      "|    n_updates            | 3585        |\n",
      "|    policy_gradient_loss | -0.00791    |\n",
      "|    std                  | 0.441       |\n",
      "|    value_loss           | 1.75e-06    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 235     |\n",
      "|    iterations      | 240     |\n",
      "|    time_elapsed    | 8347    |\n",
      "|    total_timesteps | 1966080 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 235         |\n",
      "|    iterations           | 241         |\n",
      "|    time_elapsed         | 8384        |\n",
      "|    total_timesteps      | 1974272     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028188353 |\n",
      "|    clip_fraction        | 0.312       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.6        |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.00921    |\n",
      "|    n_updates            | 3600        |\n",
      "|    policy_gradient_loss | -0.00519    |\n",
      "|    std                  | 0.441       |\n",
      "|    value_loss           | 1.82e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1980000, episode_reward=-167.08 +/- 0.01\n",
      "Episode length: 256.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 256        |\n",
      "|    mean_reward          | -167       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1980000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02447847 |\n",
      "|    clip_fraction        | 0.304      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -0.601     |\n",
      "|    explained_variance   | 1          |\n",
      "|    learning_rate        | 5e-05      |\n",
      "|    loss                 | 0.007      |\n",
      "|    n_updates            | 3615       |\n",
      "|    policy_gradient_loss | -0.00656   |\n",
      "|    std                  | 0.441      |\n",
      "|    value_loss           | 1.93e-06   |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 235     |\n",
      "|    iterations      | 242     |\n",
      "|    time_elapsed    | 8421    |\n",
      "|    total_timesteps | 1982464 |\n",
      "--------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 235        |\n",
      "|    iterations           | 243        |\n",
      "|    time_elapsed         | 8452       |\n",
      "|    total_timesteps      | 1990656    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03082298 |\n",
      "|    clip_fraction        | 0.301      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -0.599     |\n",
      "|    explained_variance   | 1          |\n",
      "|    learning_rate        | 5e-05      |\n",
      "|    loss                 | -0.0188    |\n",
      "|    n_updates            | 3630       |\n",
      "|    policy_gradient_loss | -0.00621   |\n",
      "|    std                  | 0.44       |\n",
      "|    value_loss           | 2.07e-06   |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 235        |\n",
      "|    iterations           | 244        |\n",
      "|    time_elapsed         | 8485       |\n",
      "|    total_timesteps      | 1998848    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03167851 |\n",
      "|    clip_fraction        | 0.307      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -0.6       |\n",
      "|    explained_variance   | 1          |\n",
      "|    learning_rate        | 5e-05      |\n",
      "|    loss                 | -0.0181    |\n",
      "|    n_updates            | 3645       |\n",
      "|    policy_gradient_loss | -0.00453   |\n",
      "|    std                  | 0.442      |\n",
      "|    value_loss           | 2.42e-06   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2000000, episode_reward=-166.25 +/- 0.01\n",
      "Episode length: 256.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 256        |\n",
      "|    mean_reward          | -166       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 2000000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03234448 |\n",
      "|    clip_fraction        | 0.311      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -0.603     |\n",
      "|    explained_variance   | 1          |\n",
      "|    learning_rate        | 5e-05      |\n",
      "|    loss                 | -0.0243    |\n",
      "|    n_updates            | 3660       |\n",
      "|    policy_gradient_loss | -0.00826   |\n",
      "|    std                  | 0.441      |\n",
      "|    value_loss           | 2.42e-06   |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 235     |\n",
      "|    iterations      | 245     |\n",
      "|    time_elapsed    | 8532    |\n",
      "|    total_timesteps | 2007040 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 235         |\n",
      "|    iterations           | 246         |\n",
      "|    time_elapsed         | 8565        |\n",
      "|    total_timesteps      | 2015232     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023442712 |\n",
      "|    clip_fraction        | 0.308       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.602      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.00771    |\n",
      "|    n_updates            | 3675        |\n",
      "|    policy_gradient_loss | -0.00649    |\n",
      "|    std                  | 0.442       |\n",
      "|    value_loss           | 3.97e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2020000, episode_reward=-166.14 +/- 0.01\n",
      "Episode length: 256.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 256         |\n",
      "|    mean_reward          | -166        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2020000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029359354 |\n",
      "|    clip_fraction        | 0.291       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.601      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.00387    |\n",
      "|    n_updates            | 3690        |\n",
      "|    policy_gradient_loss | -0.00386    |\n",
      "|    std                  | 0.441       |\n",
      "|    value_loss           | 2.49e-06    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 235     |\n",
      "|    iterations      | 247     |\n",
      "|    time_elapsed    | 8607    |\n",
      "|    total_timesteps | 2023424 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 235         |\n",
      "|    iterations           | 248         |\n",
      "|    time_elapsed         | 8641        |\n",
      "|    total_timesteps      | 2031616     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022251673 |\n",
      "|    clip_fraction        | 0.318       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.601      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.00921    |\n",
      "|    n_updates            | 3705        |\n",
      "|    policy_gradient_loss | -0.00473    |\n",
      "|    std                  | 0.442       |\n",
      "|    value_loss           | 3.55e-06    |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 235        |\n",
      "|    iterations           | 249        |\n",
      "|    time_elapsed         | 8675       |\n",
      "|    total_timesteps      | 2039808    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02719099 |\n",
      "|    clip_fraction        | 0.297      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -0.602     |\n",
      "|    explained_variance   | 1          |\n",
      "|    learning_rate        | 5e-05      |\n",
      "|    loss                 | -0.00966   |\n",
      "|    n_updates            | 3720       |\n",
      "|    policy_gradient_loss | -0.00499   |\n",
      "|    std                  | 0.442      |\n",
      "|    value_loss           | 3.61e-06   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=2040000, episode_reward=-165.64 +/- 0.01\n",
      "Episode length: 256.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 256         |\n",
      "|    mean_reward          | -166        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2040000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028456638 |\n",
      "|    clip_fraction        | 0.317       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.603      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0211     |\n",
      "|    n_updates            | 3735        |\n",
      "|    policy_gradient_loss | -0.00677    |\n",
      "|    std                  | 0.442       |\n",
      "|    value_loss           | 3.18e-06    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 235     |\n",
      "|    iterations      | 250     |\n",
      "|    time_elapsed    | 8714    |\n",
      "|    total_timesteps | 2048000 |\n",
      "--------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 235        |\n",
      "|    iterations           | 251        |\n",
      "|    time_elapsed         | 8748       |\n",
      "|    total_timesteps      | 2056192    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02724092 |\n",
      "|    clip_fraction        | 0.31       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -0.6       |\n",
      "|    explained_variance   | 1          |\n",
      "|    learning_rate        | 5e-05      |\n",
      "|    loss                 | -0.00929   |\n",
      "|    n_updates            | 3750       |\n",
      "|    policy_gradient_loss | -0.00619   |\n",
      "|    std                  | 0.44       |\n",
      "|    value_loss           | 6.73e-06   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=2060000, episode_reward=-165.61 +/- 0.01\n",
      "Episode length: 256.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 256         |\n",
      "|    mean_reward          | -166        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2060000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022569742 |\n",
      "|    clip_fraction        | 0.315       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.598      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | 0.0026      |\n",
      "|    n_updates            | 3765        |\n",
      "|    policy_gradient_loss | -0.00595    |\n",
      "|    std                  | 0.439       |\n",
      "|    value_loss           | 3.29e-06    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 234     |\n",
      "|    iterations      | 252     |\n",
      "|    time_elapsed    | 8784    |\n",
      "|    total_timesteps | 2064384 |\n",
      "--------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 235        |\n",
      "|    iterations           | 253        |\n",
      "|    time_elapsed         | 8815       |\n",
      "|    total_timesteps      | 2072576    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02669065 |\n",
      "|    clip_fraction        | 0.296      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -0.597     |\n",
      "|    explained_variance   | 1          |\n",
      "|    learning_rate        | 5e-05      |\n",
      "|    loss                 | -0.00344   |\n",
      "|    n_updates            | 3780       |\n",
      "|    policy_gradient_loss | -0.00547   |\n",
      "|    std                  | 0.44       |\n",
      "|    value_loss           | 2.01e-06   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2080000, episode_reward=-165.31 +/- 0.01\n",
      "Episode length: 256.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 256        |\n",
      "|    mean_reward          | -165       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 2080000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03034385 |\n",
      "|    clip_fraction        | 0.312      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -0.597     |\n",
      "|    explained_variance   | 1          |\n",
      "|    learning_rate        | 5e-05      |\n",
      "|    loss                 | -0.023     |\n",
      "|    n_updates            | 3795       |\n",
      "|    policy_gradient_loss | -0.00704   |\n",
      "|    std                  | 0.439      |\n",
      "|    value_loss           | 2.77e-06   |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 234     |\n",
      "|    iterations      | 254     |\n",
      "|    time_elapsed    | 8854    |\n",
      "|    total_timesteps | 2080768 |\n",
      "--------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 235        |\n",
      "|    iterations           | 255        |\n",
      "|    time_elapsed         | 8887       |\n",
      "|    total_timesteps      | 2088960    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02961791 |\n",
      "|    clip_fraction        | 0.32       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -0.596     |\n",
      "|    explained_variance   | 1          |\n",
      "|    learning_rate        | 5e-05      |\n",
      "|    loss                 | 0.00832    |\n",
      "|    n_updates            | 3810       |\n",
      "|    policy_gradient_loss | -0.00716   |\n",
      "|    std                  | 0.439      |\n",
      "|    value_loss           | 3.92e-06   |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 235         |\n",
      "|    iterations           | 256         |\n",
      "|    time_elapsed         | 8920        |\n",
      "|    total_timesteps      | 2097152     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031096598 |\n",
      "|    clip_fraction        | 0.326       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.594      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.00531    |\n",
      "|    n_updates            | 3825        |\n",
      "|    policy_gradient_loss | -0.00348    |\n",
      "|    std                  | 0.437       |\n",
      "|    value_loss           | 3.79e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2100000, episode_reward=-164.96 +/- 0.01\n",
      "Episode length: 256.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 256         |\n",
      "|    mean_reward          | -165        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2100000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030998606 |\n",
      "|    clip_fraction        | 0.3         |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.59       |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0166     |\n",
      "|    n_updates            | 3840        |\n",
      "|    policy_gradient_loss | -0.00352    |\n",
      "|    std                  | 0.436       |\n",
      "|    value_loss           | 3.59e-06    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 234     |\n",
      "|    iterations      | 257     |\n",
      "|    time_elapsed    | 8961    |\n",
      "|    total_timesteps | 2105344 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 234         |\n",
      "|    iterations           | 258         |\n",
      "|    time_elapsed         | 9000        |\n",
      "|    total_timesteps      | 2113536     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023195136 |\n",
      "|    clip_fraction        | 0.288       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.588      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0142     |\n",
      "|    n_updates            | 3855        |\n",
      "|    policy_gradient_loss | -0.00408    |\n",
      "|    std                  | 0.436       |\n",
      "|    value_loss           | 3.66e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2120000, episode_reward=-164.71 +/- 0.01\n",
      "Episode length: 256.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 256         |\n",
      "|    mean_reward          | -165        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2120000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025633607 |\n",
      "|    clip_fraction        | 0.304       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.589      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | 0.0137      |\n",
      "|    n_updates            | 3870        |\n",
      "|    policy_gradient_loss | -0.00768    |\n",
      "|    std                  | 0.436       |\n",
      "|    value_loss           | 3.53e-06    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 234     |\n",
      "|    iterations      | 259     |\n",
      "|    time_elapsed    | 9038    |\n",
      "|    total_timesteps | 2121728 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 234         |\n",
      "|    iterations           | 260         |\n",
      "|    time_elapsed         | 9071        |\n",
      "|    total_timesteps      | 2129920     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029104985 |\n",
      "|    clip_fraction        | 0.305       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.588      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | 0.000174    |\n",
      "|    n_updates            | 3885        |\n",
      "|    policy_gradient_loss | -0.00608    |\n",
      "|    std                  | 0.436       |\n",
      "|    value_loss           | 2.53e-06    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 234         |\n",
      "|    iterations           | 261         |\n",
      "|    time_elapsed         | 9104        |\n",
      "|    total_timesteps      | 2138112     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.032423552 |\n",
      "|    clip_fraction        | 0.316       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.59       |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0226     |\n",
      "|    n_updates            | 3900        |\n",
      "|    policy_gradient_loss | -0.00739    |\n",
      "|    std                  | 0.437       |\n",
      "|    value_loss           | 8.43e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2140000, episode_reward=-163.68 +/- 0.01\n",
      "Episode length: 256.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 256         |\n",
      "|    mean_reward          | -164        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2140000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023169776 |\n",
      "|    clip_fraction        | 0.303       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.588      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | 0.0153      |\n",
      "|    n_updates            | 3915        |\n",
      "|    policy_gradient_loss | -0.0043     |\n",
      "|    std                  | 0.434       |\n",
      "|    value_loss           | 2.5e-06     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best mean reward!\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 234     |\n",
      "|    iterations      | 262     |\n",
      "|    time_elapsed    | 9144    |\n",
      "|    total_timesteps | 2146304 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 234         |\n",
      "|    iterations           | 263         |\n",
      "|    time_elapsed         | 9189        |\n",
      "|    total_timesteps      | 2154496     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027232569 |\n",
      "|    clip_fraction        | 0.311       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.586      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.00262    |\n",
      "|    n_updates            | 3930        |\n",
      "|    policy_gradient_loss | -0.00692    |\n",
      "|    std                  | 0.435       |\n",
      "|    value_loss           | 2.7e-06     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2160000, episode_reward=-163.61 +/- 0.01\n",
      "Episode length: 256.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 256         |\n",
      "|    mean_reward          | -164        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2160000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024577381 |\n",
      "|    clip_fraction        | 0.303       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.586      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | 0.0052      |\n",
      "|    n_updates            | 3945        |\n",
      "|    policy_gradient_loss | -0.00775    |\n",
      "|    std                  | 0.434       |\n",
      "|    value_loss           | 2.19e-06    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 234     |\n",
      "|    iterations      | 264     |\n",
      "|    time_elapsed    | 9238    |\n",
      "|    total_timesteps | 2162688 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 234         |\n",
      "|    iterations           | 265         |\n",
      "|    time_elapsed         | 9274        |\n",
      "|    total_timesteps      | 2170880     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.035721414 |\n",
      "|    clip_fraction        | 0.303       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.585      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.00956    |\n",
      "|    n_updates            | 3960        |\n",
      "|    policy_gradient_loss | -0.0041     |\n",
      "|    std                  | 0.434       |\n",
      "|    value_loss           | 2.88e-06    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 234         |\n",
      "|    iterations           | 266         |\n",
      "|    time_elapsed         | 9308        |\n",
      "|    total_timesteps      | 2179072     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025571767 |\n",
      "|    clip_fraction        | 0.303       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.586      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | 0.00838     |\n",
      "|    n_updates            | 3975        |\n",
      "|    policy_gradient_loss | -0.0047     |\n",
      "|    std                  | 0.435       |\n",
      "|    value_loss           | 3.46e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2180000, episode_reward=-162.89 +/- 0.01\n",
      "Episode length: 256.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 256         |\n",
      "|    mean_reward          | -163        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2180000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022753593 |\n",
      "|    clip_fraction        | 0.299       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.591      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.00168    |\n",
      "|    n_updates            | 3990        |\n",
      "|    policy_gradient_loss | -0.00439    |\n",
      "|    std                  | 0.439       |\n",
      "|    value_loss           | 2.15e-06    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 233     |\n",
      "|    iterations      | 267     |\n",
      "|    time_elapsed    | 9351    |\n",
      "|    total_timesteps | 2187264 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 233         |\n",
      "|    iterations           | 268         |\n",
      "|    time_elapsed         | 9389        |\n",
      "|    total_timesteps      | 2195456     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030050099 |\n",
      "|    clip_fraction        | 0.308       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.593      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | 0.01        |\n",
      "|    n_updates            | 4005        |\n",
      "|    policy_gradient_loss | -0.00588    |\n",
      "|    std                  | 0.437       |\n",
      "|    value_loss           | 3.83e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2200000, episode_reward=-162.40 +/- 0.01\n",
      "Episode length: 256.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 256         |\n",
      "|    mean_reward          | -162        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2200000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.034700908 |\n",
      "|    clip_fraction        | 0.29        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.591      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | 0.0216      |\n",
      "|    n_updates            | 4020        |\n",
      "|    policy_gradient_loss | -0.000674   |\n",
      "|    std                  | 0.437       |\n",
      "|    value_loss           | 2.88e-06    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 233     |\n",
      "|    iterations      | 269     |\n",
      "|    time_elapsed    | 9431    |\n",
      "|    total_timesteps | 2203648 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 233         |\n",
      "|    iterations           | 270         |\n",
      "|    time_elapsed         | 9465        |\n",
      "|    total_timesteps      | 2211840     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021670582 |\n",
      "|    clip_fraction        | 0.285       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.593      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0161     |\n",
      "|    n_updates            | 4035        |\n",
      "|    policy_gradient_loss | -0.00239    |\n",
      "|    std                  | 0.438       |\n",
      "|    value_loss           | 1.94e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2220000, episode_reward=-162.03 +/- 0.01\n",
      "Episode length: 256.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 256         |\n",
      "|    mean_reward          | -162        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2220000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020518236 |\n",
      "|    clip_fraction        | 0.278       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.593      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.00691    |\n",
      "|    n_updates            | 4050        |\n",
      "|    policy_gradient_loss | -0.00107    |\n",
      "|    std                  | 0.438       |\n",
      "|    value_loss           | 2.49e-06    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best mean reward!\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 233     |\n",
      "|    iterations      | 271     |\n",
      "|    time_elapsed    | 9508    |\n",
      "|    total_timesteps | 2220032 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 233         |\n",
      "|    iterations           | 272         |\n",
      "|    time_elapsed         | 9541        |\n",
      "|    total_timesteps      | 2228224     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030402636 |\n",
      "|    clip_fraction        | 0.319       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.591      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0137     |\n",
      "|    n_updates            | 4065        |\n",
      "|    policy_gradient_loss | -0.00747    |\n",
      "|    std                  | 0.436       |\n",
      "|    value_loss           | 4.03e-06    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 233         |\n",
      "|    iterations           | 273         |\n",
      "|    time_elapsed         | 9576        |\n",
      "|    total_timesteps      | 2236416     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030970775 |\n",
      "|    clip_fraction        | 0.307       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.589      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.00443    |\n",
      "|    n_updates            | 4080        |\n",
      "|    policy_gradient_loss | -0.000759   |\n",
      "|    std                  | 0.437       |\n",
      "|    value_loss           | 4.11e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2240000, episode_reward=-161.21 +/- 0.01\n",
      "Episode length: 256.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 256         |\n",
      "|    mean_reward          | -161        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2240000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027967181 |\n",
      "|    clip_fraction        | 0.309       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.591      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0128     |\n",
      "|    n_updates            | 4095        |\n",
      "|    policy_gradient_loss | -0.00417    |\n",
      "|    std                  | 0.437       |\n",
      "|    value_loss           | 4.41e-06    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 233     |\n",
      "|    iterations      | 274     |\n",
      "|    time_elapsed    | 9620    |\n",
      "|    total_timesteps | 2244608 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 233         |\n",
      "|    iterations           | 275         |\n",
      "|    time_elapsed         | 9656        |\n",
      "|    total_timesteps      | 2252800     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.033265933 |\n",
      "|    clip_fraction        | 0.323       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.591      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0118     |\n",
      "|    n_updates            | 4110        |\n",
      "|    policy_gradient_loss | -0.00603    |\n",
      "|    std                  | 0.437       |\n",
      "|    value_loss           | 3.29e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2260000, episode_reward=-160.62 +/- 0.01\n",
      "Episode length: 256.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 256         |\n",
      "|    mean_reward          | -161        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2260000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.032460816 |\n",
      "|    clip_fraction        | 0.307       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.593      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | 0.0244      |\n",
      "|    n_updates            | 4125        |\n",
      "|    policy_gradient_loss | -0.00344    |\n",
      "|    std                  | 0.438       |\n",
      "|    value_loss           | 1.16e-05    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 233     |\n",
      "|    iterations      | 276     |\n",
      "|    time_elapsed    | 9698    |\n",
      "|    total_timesteps | 2260992 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 233         |\n",
      "|    iterations           | 277         |\n",
      "|    time_elapsed         | 9734        |\n",
      "|    total_timesteps      | 2269184     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030465119 |\n",
      "|    clip_fraction        | 0.302       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.596      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0179     |\n",
      "|    n_updates            | 4140        |\n",
      "|    policy_gradient_loss | -0.000744   |\n",
      "|    std                  | 0.44        |\n",
      "|    value_loss           | 3.6e-06     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 233         |\n",
      "|    iterations           | 278         |\n",
      "|    time_elapsed         | 9767        |\n",
      "|    total_timesteps      | 2277376     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023859918 |\n",
      "|    clip_fraction        | 0.294       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.595      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | 0.0043      |\n",
      "|    n_updates            | 4155        |\n",
      "|    policy_gradient_loss | -0.00454    |\n",
      "|    std                  | 0.437       |\n",
      "|    value_loss           | 3.19e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2280000, episode_reward=-159.58 +/- 0.01\n",
      "Episode length: 256.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 256         |\n",
      "|    mean_reward          | -160        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2280000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021600973 |\n",
      "|    clip_fraction        | 0.282       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.591      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.000713   |\n",
      "|    n_updates            | 4170        |\n",
      "|    policy_gradient_loss | -0.00417    |\n",
      "|    std                  | 0.437       |\n",
      "|    value_loss           | 4.26e-06    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 232     |\n",
      "|    iterations      | 279     |\n",
      "|    time_elapsed    | 9809    |\n",
      "|    total_timesteps | 2285568 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 232         |\n",
      "|    iterations           | 280         |\n",
      "|    time_elapsed         | 9848        |\n",
      "|    total_timesteps      | 2293760     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026087267 |\n",
      "|    clip_fraction        | 0.302       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.591      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | 0.00265     |\n",
      "|    n_updates            | 4185        |\n",
      "|    policy_gradient_loss | -0.00431    |\n",
      "|    std                  | 0.437       |\n",
      "|    value_loss           | 1.03e-05    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2300000, episode_reward=-158.77 +/- 0.01\n",
      "Episode length: 256.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 256         |\n",
      "|    mean_reward          | -159        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2300000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027213074 |\n",
      "|    clip_fraction        | 0.293       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.591      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | 0.00282     |\n",
      "|    n_updates            | 4200        |\n",
      "|    policy_gradient_loss | -0.00345    |\n",
      "|    std                  | 0.436       |\n",
      "|    value_loss           | 4.88e-06    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 232     |\n",
      "|    iterations      | 281     |\n",
      "|    time_elapsed    | 9914    |\n",
      "|    total_timesteps | 2301952 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 232         |\n",
      "|    iterations           | 282         |\n",
      "|    time_elapsed         | 9945        |\n",
      "|    total_timesteps      | 2310144     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.036570255 |\n",
      "|    clip_fraction        | 0.307       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.588      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | 0.00212     |\n",
      "|    n_updates            | 4215        |\n",
      "|    policy_gradient_loss | -0.00233    |\n",
      "|    std                  | 0.435       |\n",
      "|    value_loss           | 6.07e-06    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 232         |\n",
      "|    iterations           | 283         |\n",
      "|    time_elapsed         | 9986        |\n",
      "|    total_timesteps      | 2318336     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024395077 |\n",
      "|    clip_fraction        | 0.292       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.588      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0109     |\n",
      "|    n_updates            | 4230        |\n",
      "|    policy_gradient_loss | -0.00371    |\n",
      "|    std                  | 0.436       |\n",
      "|    value_loss           | 2.61e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2320000, episode_reward=-158.15 +/- 0.01\n",
      "Episode length: 256.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 256         |\n",
      "|    mean_reward          | -158        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2320000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027016819 |\n",
      "|    clip_fraction        | 0.31        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.593      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | 0.0103      |\n",
      "|    n_updates            | 4245        |\n",
      "|    policy_gradient_loss | -0.00547    |\n",
      "|    std                  | 0.439       |\n",
      "|    value_loss           | 3.01e-06    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 231     |\n",
      "|    iterations      | 284     |\n",
      "|    time_elapsed    | 10028   |\n",
      "|    total_timesteps | 2326528 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 232         |\n",
      "|    iterations           | 285         |\n",
      "|    time_elapsed         | 10062       |\n",
      "|    total_timesteps      | 2334720     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025072573 |\n",
      "|    clip_fraction        | 0.312       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.598      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.00244    |\n",
      "|    n_updates            | 4260        |\n",
      "|    policy_gradient_loss | -0.00661    |\n",
      "|    std                  | 0.441       |\n",
      "|    value_loss           | 6.57e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2340000, episode_reward=-157.93 +/- 0.01\n",
      "Episode length: 256.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 256         |\n",
      "|    mean_reward          | -158        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2340000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027989551 |\n",
      "|    clip_fraction        | 0.311       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.599      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.00977    |\n",
      "|    n_updates            | 4275        |\n",
      "|    policy_gradient_loss | -0.00463    |\n",
      "|    std                  | 0.44        |\n",
      "|    value_loss           | 2.83e-06    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 231     |\n",
      "|    iterations      | 286     |\n",
      "|    time_elapsed    | 10103   |\n",
      "|    total_timesteps | 2342912 |\n",
      "--------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 231        |\n",
      "|    iterations           | 287        |\n",
      "|    time_elapsed         | 10137      |\n",
      "|    total_timesteps      | 2351104    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02811242 |\n",
      "|    clip_fraction        | 0.33       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -0.6       |\n",
      "|    explained_variance   | 1          |\n",
      "|    learning_rate        | 5e-05      |\n",
      "|    loss                 | -0.0192    |\n",
      "|    n_updates            | 4290       |\n",
      "|    policy_gradient_loss | -0.00397   |\n",
      "|    std                  | 0.442      |\n",
      "|    value_loss           | 4.2e-06    |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 231         |\n",
      "|    iterations           | 288         |\n",
      "|    time_elapsed         | 10172       |\n",
      "|    total_timesteps      | 2359296     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.032129593 |\n",
      "|    clip_fraction        | 0.306       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.602      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0096     |\n",
      "|    n_updates            | 4305        |\n",
      "|    policy_gradient_loss | -0.00522    |\n",
      "|    std                  | 0.442       |\n",
      "|    value_loss           | 4.37e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2360000, episode_reward=-156.67 +/- 0.01\n",
      "Episode length: 256.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 256       |\n",
      "|    mean_reward          | -157      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 2360000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0231476 |\n",
      "|    clip_fraction        | 0.291     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | -0.601    |\n",
      "|    explained_variance   | 1         |\n",
      "|    learning_rate        | 5e-05     |\n",
      "|    loss                 | -0.0137   |\n",
      "|    n_updates            | 4320      |\n",
      "|    policy_gradient_loss | -0.00306  |\n",
      "|    std                  | 0.441     |\n",
      "|    value_loss           | 6.73e-06  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best mean reward!\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 231     |\n",
      "|    iterations      | 289     |\n",
      "|    time_elapsed    | 10219   |\n",
      "|    total_timesteps | 2367488 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 231         |\n",
      "|    iterations           | 290         |\n",
      "|    time_elapsed         | 10250       |\n",
      "|    total_timesteps      | 2375680     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028821606 |\n",
      "|    clip_fraction        | 0.287       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.601      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0105     |\n",
      "|    n_updates            | 4335        |\n",
      "|    policy_gradient_loss | -0.00324    |\n",
      "|    std                  | 0.441       |\n",
      "|    value_loss           | 3.51e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2380000, episode_reward=-156.53 +/- 0.01\n",
      "Episode length: 256.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 256         |\n",
      "|    mean_reward          | -157        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2380000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023164114 |\n",
      "|    clip_fraction        | 0.304       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.6        |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | 0.00138     |\n",
      "|    n_updates            | 4350        |\n",
      "|    policy_gradient_loss | -0.00529    |\n",
      "|    std                  | 0.44        |\n",
      "|    value_loss           | 2.87e-06    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 231     |\n",
      "|    iterations      | 291     |\n",
      "|    time_elapsed    | 10292   |\n",
      "|    total_timesteps | 2383872 |\n",
      "--------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 231        |\n",
      "|    iterations           | 292        |\n",
      "|    time_elapsed         | 10324      |\n",
      "|    total_timesteps      | 2392064    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02324847 |\n",
      "|    clip_fraction        | 0.303      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -0.601     |\n",
      "|    explained_variance   | 1          |\n",
      "|    learning_rate        | 5e-05      |\n",
      "|    loss                 | -0.02      |\n",
      "|    n_updates            | 4365       |\n",
      "|    policy_gradient_loss | -0.00709   |\n",
      "|    std                  | 0.442      |\n",
      "|    value_loss           | 3.06e-06   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=2400000, episode_reward=-156.24 +/- 0.01\n",
      "Episode length: 256.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 256         |\n",
      "|    mean_reward          | -156        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2400000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029574443 |\n",
      "|    clip_fraction        | 0.308       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.602      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0053     |\n",
      "|    n_updates            | 4380        |\n",
      "|    policy_gradient_loss | -0.00772    |\n",
      "|    std                  | 0.442       |\n",
      "|    value_loss           | 1.49e-05    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 231     |\n",
      "|    iterations      | 293     |\n",
      "|    time_elapsed    | 10362   |\n",
      "|    total_timesteps | 2400256 |\n",
      "--------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 231       |\n",
      "|    iterations           | 294       |\n",
      "|    time_elapsed         | 10396     |\n",
      "|    total_timesteps      | 2408448   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0351348 |\n",
      "|    clip_fraction        | 0.308     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | -0.601    |\n",
      "|    explained_variance   | 1         |\n",
      "|    learning_rate        | 5e-05     |\n",
      "|    loss                 | 0.0223    |\n",
      "|    n_updates            | 4395      |\n",
      "|    policy_gradient_loss | -0.00704  |\n",
      "|    std                  | 0.441     |\n",
      "|    value_loss           | 2.89e-06  |\n",
      "---------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 231         |\n",
      "|    iterations           | 295         |\n",
      "|    time_elapsed         | 10434       |\n",
      "|    total_timesteps      | 2416640     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026035637 |\n",
      "|    clip_fraction        | 0.303       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.6        |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.00911    |\n",
      "|    n_updates            | 4410        |\n",
      "|    policy_gradient_loss | -0.00555    |\n",
      "|    std                  | 0.441       |\n",
      "|    value_loss           | 2.68e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2420000, episode_reward=-156.03 +/- 0.01\n",
      "Episode length: 256.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 256        |\n",
      "|    mean_reward          | -156       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 2420000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03184785 |\n",
      "|    clip_fraction        | 0.311      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -0.598     |\n",
      "|    explained_variance   | 1          |\n",
      "|    learning_rate        | 5e-05      |\n",
      "|    loss                 | -0.0119    |\n",
      "|    n_updates            | 4425       |\n",
      "|    policy_gradient_loss | -0.0042    |\n",
      "|    std                  | 0.439      |\n",
      "|    value_loss           | 3.07e-06   |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 231     |\n",
      "|    iterations      | 296     |\n",
      "|    time_elapsed    | 10477   |\n",
      "|    total_timesteps | 2424832 |\n",
      "--------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 231        |\n",
      "|    iterations           | 297        |\n",
      "|    time_elapsed         | 10509      |\n",
      "|    total_timesteps      | 2433024    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03750872 |\n",
      "|    clip_fraction        | 0.311      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -0.597     |\n",
      "|    explained_variance   | 1          |\n",
      "|    learning_rate        | 5e-05      |\n",
      "|    loss                 | -0.0107    |\n",
      "|    n_updates            | 4440       |\n",
      "|    policy_gradient_loss | -0.00608   |\n",
      "|    std                  | 0.44       |\n",
      "|    value_loss           | 2.63e-06   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=2440000, episode_reward=-155.87 +/- 0.01\n",
      "Episode length: 256.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 256         |\n",
      "|    mean_reward          | -156        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2440000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026976142 |\n",
      "|    clip_fraction        | 0.316       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.596      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.00819    |\n",
      "|    n_updates            | 4455        |\n",
      "|    policy_gradient_loss | -0.00841    |\n",
      "|    std                  | 0.439       |\n",
      "|    value_loss           | 2.78e-06    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best mean reward!\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 231     |\n",
      "|    iterations      | 298     |\n",
      "|    time_elapsed    | 10555   |\n",
      "|    total_timesteps | 2441216 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 231         |\n",
      "|    iterations           | 299         |\n",
      "|    time_elapsed         | 10587       |\n",
      "|    total_timesteps      | 2449408     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028087225 |\n",
      "|    clip_fraction        | 0.308       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.595      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0163     |\n",
      "|    n_updates            | 4470        |\n",
      "|    policy_gradient_loss | -0.00504    |\n",
      "|    std                  | 0.439       |\n",
      "|    value_loss           | 3.91e-06    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 231         |\n",
      "|    iterations           | 300         |\n",
      "|    time_elapsed         | 10626       |\n",
      "|    total_timesteps      | 2457600     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025295965 |\n",
      "|    clip_fraction        | 0.311       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.595      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0133     |\n",
      "|    n_updates            | 4485        |\n",
      "|    policy_gradient_loss | -0.00504    |\n",
      "|    std                  | 0.439       |\n",
      "|    value_loss           | 3.4e-06     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2460000, episode_reward=-155.43 +/- 0.01\n",
      "Episode length: 256.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 256         |\n",
      "|    mean_reward          | -155        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2460000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.033392906 |\n",
      "|    clip_fraction        | 0.297       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.593      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.00878    |\n",
      "|    n_updates            | 4500        |\n",
      "|    policy_gradient_loss | -0.00575    |\n",
      "|    std                  | 0.437       |\n",
      "|    value_loss           | 3.22e-06    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 231     |\n",
      "|    iterations      | 301     |\n",
      "|    time_elapsed    | 10668   |\n",
      "|    total_timesteps | 2465792 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 231         |\n",
      "|    iterations           | 302         |\n",
      "|    time_elapsed         | 10700       |\n",
      "|    total_timesteps      | 2473984     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031391185 |\n",
      "|    clip_fraction        | 0.312       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.59       |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.00529    |\n",
      "|    n_updates            | 4515        |\n",
      "|    policy_gradient_loss | -0.00641    |\n",
      "|    std                  | 0.436       |\n",
      "|    value_loss           | 2.81e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2480000, episode_reward=-155.33 +/- 0.01\n",
      "Episode length: 256.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 256         |\n",
      "|    mean_reward          | -155        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2480000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.033248138 |\n",
      "|    clip_fraction        | 0.319       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.59       |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.00845    |\n",
      "|    n_updates            | 4530        |\n",
      "|    policy_gradient_loss | -0.00806    |\n",
      "|    std                  | 0.437       |\n",
      "|    value_loss           | 9.61e-06    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 231     |\n",
      "|    iterations      | 303     |\n",
      "|    time_elapsed    | 10743   |\n",
      "|    total_timesteps | 2482176 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 230         |\n",
      "|    iterations           | 304         |\n",
      "|    time_elapsed         | 10782       |\n",
      "|    total_timesteps      | 2490368     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027413718 |\n",
      "|    clip_fraction        | 0.304       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.589      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | 0.00477     |\n",
      "|    n_updates            | 4545        |\n",
      "|    policy_gradient_loss | -0.00425    |\n",
      "|    std                  | 0.436       |\n",
      "|    value_loss           | 2.97e-06    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 230         |\n",
      "|    iterations           | 305         |\n",
      "|    time_elapsed         | 10856       |\n",
      "|    total_timesteps      | 2498560     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030846177 |\n",
      "|    clip_fraction        | 0.3         |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.588      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.00211    |\n",
      "|    n_updates            | 4560        |\n",
      "|    policy_gradient_loss | -0.004      |\n",
      "|    std                  | 0.435       |\n",
      "|    value_loss           | 3.4e-06     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2500000, episode_reward=-153.92 +/- 0.01\n",
      "Episode length: 256.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 256         |\n",
      "|    mean_reward          | -154        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2500000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027414571 |\n",
      "|    clip_fraction        | 0.296       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.583      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | 0.00329     |\n",
      "|    n_updates            | 4575        |\n",
      "|    policy_gradient_loss | -0.00165    |\n",
      "|    std                  | 0.432       |\n",
      "|    value_loss           | 2.84e-06    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 229     |\n",
      "|    iterations      | 306     |\n",
      "|    time_elapsed    | 10908   |\n",
      "|    total_timesteps | 2506752 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 229         |\n",
      "|    iterations           | 307         |\n",
      "|    time_elapsed         | 10940       |\n",
      "|    total_timesteps      | 2514944     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024816085 |\n",
      "|    clip_fraction        | 0.29        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.578      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0157     |\n",
      "|    n_updates            | 4590        |\n",
      "|    policy_gradient_loss | -0.00101    |\n",
      "|    std                  | 0.431       |\n",
      "|    value_loss           | 2.89e-06    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2520000, episode_reward=-153.76 +/- 0.01\n",
      "Episode length: 256.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 256         |\n",
      "|    mean_reward          | -154        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2520000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028697275 |\n",
      "|    clip_fraction        | 0.311       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.576      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | 0.00782     |\n",
      "|    n_updates            | 4605        |\n",
      "|    policy_gradient_loss | -0.005      |\n",
      "|    std                  | 0.43        |\n",
      "|    value_loss           | 6.64e-06    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 229     |\n",
      "|    iterations      | 308     |\n",
      "|    time_elapsed    | 10981   |\n",
      "|    total_timesteps | 2523136 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 229         |\n",
      "|    iterations           | 309         |\n",
      "|    time_elapsed         | 11012       |\n",
      "|    total_timesteps      | 2531328     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026233587 |\n",
      "|    clip_fraction        | 0.296       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.577      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | 0.00309     |\n",
      "|    n_updates            | 4620        |\n",
      "|    policy_gradient_loss | -0.00614    |\n",
      "|    std                  | 0.431       |\n",
      "|    value_loss           | 5.12e-06    |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 229        |\n",
      "|    iterations           | 310        |\n",
      "|    time_elapsed         | 11045      |\n",
      "|    total_timesteps      | 2539520    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03015744 |\n",
      "|    clip_fraction        | 0.32       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -0.577     |\n",
      "|    explained_variance   | 1          |\n",
      "|    learning_rate        | 5e-05      |\n",
      "|    loss                 | 0.00938    |\n",
      "|    n_updates            | 4635       |\n",
      "|    policy_gradient_loss | -0.00512   |\n",
      "|    std                  | 0.431      |\n",
      "|    value_loss           | 3.09e-06   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=2540000, episode_reward=-153.49 +/- 0.01\n",
      "Episode length: 256.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 256         |\n",
      "|    mean_reward          | -153        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2540000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023636563 |\n",
      "|    clip_fraction        | 0.306       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.578      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.00586    |\n",
      "|    n_updates            | 4650        |\n",
      "|    policy_gradient_loss | -0.00325    |\n",
      "|    std                  | 0.431       |\n",
      "|    value_loss           | 8.78e-06    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 229     |\n",
      "|    iterations      | 311     |\n",
      "|    time_elapsed    | 11084   |\n",
      "|    total_timesteps | 2547712 |\n",
      "--------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 229        |\n",
      "|    iterations           | 312        |\n",
      "|    time_elapsed         | 11130      |\n",
      "|    total_timesteps      | 2555904    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02535741 |\n",
      "|    clip_fraction        | 0.321      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -0.577     |\n",
      "|    explained_variance   | 1          |\n",
      "|    learning_rate        | 5e-05      |\n",
      "|    loss                 | 0.00263    |\n",
      "|    n_updates            | 4665       |\n",
      "|    policy_gradient_loss | -0.00545   |\n",
      "|    std                  | 0.43       |\n",
      "|    value_loss           | 6.5e-05    |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=2560000, episode_reward=-153.51 +/- 0.01\n",
      "Episode length: 256.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 256         |\n",
      "|    mean_reward          | -154        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2560000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029593244 |\n",
      "|    clip_fraction        | 0.317       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.576      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.000142   |\n",
      "|    n_updates            | 4680        |\n",
      "|    policy_gradient_loss | -0.00422    |\n",
      "|    std                  | 0.431       |\n",
      "|    value_loss           | 2.66e-06    |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 229     |\n",
      "|    iterations      | 313     |\n",
      "|    time_elapsed    | 11169   |\n",
      "|    total_timesteps | 2564096 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 229         |\n",
      "|    iterations           | 314         |\n",
      "|    time_elapsed         | 11205       |\n",
      "|    total_timesteps      | 2572288     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025046479 |\n",
      "|    clip_fraction        | 0.301       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.577      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0052     |\n",
      "|    n_updates            | 4695        |\n",
      "|    policy_gradient_loss | -0.00463    |\n",
      "|    std                  | 0.431       |\n",
      "|    value_loss           | 3.19e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2580000, episode_reward=-152.70 +/- 0.01\n",
      "Episode length: 256.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 256         |\n",
      "|    mean_reward          | -153        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2580000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031257175 |\n",
      "|    clip_fraction        | 0.311       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.574      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.00231    |\n",
      "|    n_updates            | 4710        |\n",
      "|    policy_gradient_loss | -0.00488    |\n",
      "|    std                  | 0.428       |\n",
      "|    value_loss           | 4.17e-06    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 229     |\n",
      "|    iterations      | 315     |\n",
      "|    time_elapsed    | 11251   |\n",
      "|    total_timesteps | 2580480 |\n",
      "--------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 229        |\n",
      "|    iterations           | 316        |\n",
      "|    time_elapsed         | 11282      |\n",
      "|    total_timesteps      | 2588672    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04341376 |\n",
      "|    clip_fraction        | 0.316      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -0.57      |\n",
      "|    explained_variance   | 1          |\n",
      "|    learning_rate        | 5e-05      |\n",
      "|    loss                 | -0.015     |\n",
      "|    n_updates            | 4725       |\n",
      "|    policy_gradient_loss | -0.00458   |\n",
      "|    std                  | 0.427      |\n",
      "|    value_loss           | 2.84e-06   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 229        |\n",
      "|    iterations           | 317        |\n",
      "|    time_elapsed         | 11314      |\n",
      "|    total_timesteps      | 2596864    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03299661 |\n",
      "|    clip_fraction        | 0.317      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -0.57      |\n",
      "|    explained_variance   | 1          |\n",
      "|    learning_rate        | 5e-05      |\n",
      "|    loss                 | -0.00409   |\n",
      "|    n_updates            | 4740       |\n",
      "|    policy_gradient_loss | -0.0053    |\n",
      "|    std                  | 0.428      |\n",
      "|    value_loss           | 2.73e-06   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=2600000, episode_reward=-152.49 +/- 0.01\n",
      "Episode length: 256.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 256         |\n",
      "|    mean_reward          | -152        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2600000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026186457 |\n",
      "|    clip_fraction        | 0.324       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.569      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.00208    |\n",
      "|    n_updates            | 4755        |\n",
      "|    policy_gradient_loss | -0.00566    |\n",
      "|    std                  | 0.427       |\n",
      "|    value_loss           | 2.87e-06    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 229     |\n",
      "|    iterations      | 318     |\n",
      "|    time_elapsed    | 11355   |\n",
      "|    total_timesteps | 2605056 |\n",
      "--------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 229        |\n",
      "|    iterations           | 319        |\n",
      "|    time_elapsed         | 11388      |\n",
      "|    total_timesteps      | 2613248    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03188791 |\n",
      "|    clip_fraction        | 0.322      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -0.568     |\n",
      "|    explained_variance   | 1          |\n",
      "|    learning_rate        | 5e-05      |\n",
      "|    loss                 | -0.00675   |\n",
      "|    n_updates            | 4770       |\n",
      "|    policy_gradient_loss | -0.00442   |\n",
      "|    std                  | 0.427      |\n",
      "|    value_loss           | 2.86e-06   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=2620000, episode_reward=-152.20 +/- 0.01\n",
      "Episode length: 256.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 256         |\n",
      "|    mean_reward          | -152        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2620000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.036709923 |\n",
      "|    clip_fraction        | 0.315       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.569      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.00963    |\n",
      "|    n_updates            | 4785        |\n",
      "|    policy_gradient_loss | -0.00646    |\n",
      "|    std                  | 0.428       |\n",
      "|    value_loss           | 5.97e-06    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 229     |\n",
      "|    iterations      | 320     |\n",
      "|    time_elapsed    | 11425   |\n",
      "|    total_timesteps | 2621440 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 229         |\n",
      "|    iterations           | 321         |\n",
      "|    time_elapsed         | 11456       |\n",
      "|    total_timesteps      | 2629632     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026194036 |\n",
      "|    clip_fraction        | 0.306       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.572      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0135     |\n",
      "|    n_updates            | 4800        |\n",
      "|    policy_gradient_loss | -0.00238    |\n",
      "|    std                  | 0.43        |\n",
      "|    value_loss           | 8.38e-06    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 229         |\n",
      "|    iterations           | 322         |\n",
      "|    time_elapsed         | 11489       |\n",
      "|    total_timesteps      | 2637824     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026632145 |\n",
      "|    clip_fraction        | 0.326       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.574      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | 0.00222     |\n",
      "|    n_updates            | 4815        |\n",
      "|    policy_gradient_loss | -0.00456    |\n",
      "|    std                  | 0.429       |\n",
      "|    value_loss           | 3.4e-06     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2640000, episode_reward=-151.82 +/- 0.01\n",
      "Episode length: 256.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 256         |\n",
      "|    mean_reward          | -152        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2640000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028981265 |\n",
      "|    clip_fraction        | 0.329       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.574      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.00273    |\n",
      "|    n_updates            | 4830        |\n",
      "|    policy_gradient_loss | -0.00572    |\n",
      "|    std                  | 0.43        |\n",
      "|    value_loss           | 3.16e-06    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 229     |\n",
      "|    iterations      | 323     |\n",
      "|    time_elapsed    | 11525   |\n",
      "|    total_timesteps | 2646016 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 229         |\n",
      "|    iterations           | 324         |\n",
      "|    time_elapsed         | 11555       |\n",
      "|    total_timesteps      | 2654208     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026858194 |\n",
      "|    clip_fraction        | 0.317       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.575      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.00487    |\n",
      "|    n_updates            | 4845        |\n",
      "|    policy_gradient_loss | -0.0051     |\n",
      "|    std                  | 0.43        |\n",
      "|    value_loss           | 2.64e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2660000, episode_reward=-151.67 +/- 0.01\n",
      "Episode length: 256.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 256        |\n",
      "|    mean_reward          | -152       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 2660000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02626004 |\n",
      "|    clip_fraction        | 0.312      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -0.573     |\n",
      "|    explained_variance   | 1          |\n",
      "|    learning_rate        | 5e-05      |\n",
      "|    loss                 | -0.0139    |\n",
      "|    n_updates            | 4860       |\n",
      "|    policy_gradient_loss | -0.00512   |\n",
      "|    std                  | 0.428      |\n",
      "|    value_loss           | 4.59e-06   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best mean reward!\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 229     |\n",
      "|    iterations      | 325     |\n",
      "|    time_elapsed    | 11594   |\n",
      "|    total_timesteps | 2662400 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 229         |\n",
      "|    iterations           | 326         |\n",
      "|    time_elapsed         | 11626       |\n",
      "|    total_timesteps      | 2670592     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025407238 |\n",
      "|    clip_fraction        | 0.31        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.57       |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0136     |\n",
      "|    n_updates            | 4875        |\n",
      "|    policy_gradient_loss | -0.00217    |\n",
      "|    std                  | 0.428       |\n",
      "|    value_loss           | 3.68e-06    |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 229        |\n",
      "|    iterations           | 327        |\n",
      "|    time_elapsed         | 11657      |\n",
      "|    total_timesteps      | 2678784    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03138219 |\n",
      "|    clip_fraction        | 0.323      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -0.57      |\n",
      "|    explained_variance   | 1          |\n",
      "|    learning_rate        | 5e-05      |\n",
      "|    loss                 | 0.00436    |\n",
      "|    n_updates            | 4890       |\n",
      "|    policy_gradient_loss | -0.000818  |\n",
      "|    std                  | 0.428      |\n",
      "|    value_loss           | 4.28e-06   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=2680000, episode_reward=-151.39 +/- 0.01\n",
      "Episode length: 256.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 256         |\n",
      "|    mean_reward          | -151        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2680000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028103603 |\n",
      "|    clip_fraction        | 0.319       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.57       |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0115     |\n",
      "|    n_updates            | 4905        |\n",
      "|    policy_gradient_loss | -0.00356    |\n",
      "|    std                  | 0.428       |\n",
      "|    value_loss           | 4.5e-06     |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 229     |\n",
      "|    iterations      | 328     |\n",
      "|    time_elapsed    | 11698   |\n",
      "|    total_timesteps | 2686976 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 229         |\n",
      "|    iterations           | 329         |\n",
      "|    time_elapsed         | 11730       |\n",
      "|    total_timesteps      | 2695168     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022515375 |\n",
      "|    clip_fraction        | 0.291       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.57       |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0114     |\n",
      "|    n_updates            | 4920        |\n",
      "|    policy_gradient_loss | -0.00341    |\n",
      "|    std                  | 0.428       |\n",
      "|    value_loss           | 7.59e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2700000, episode_reward=-151.16 +/- 0.01\n",
      "Episode length: 256.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 256        |\n",
      "|    mean_reward          | -151       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 2700000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03208602 |\n",
      "|    clip_fraction        | 0.303      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -0.569     |\n",
      "|    explained_variance   | 1          |\n",
      "|    learning_rate        | 5e-05      |\n",
      "|    loss                 | -0.00933   |\n",
      "|    n_updates            | 4935       |\n",
      "|    policy_gradient_loss | -0.00294   |\n",
      "|    std                  | 0.428      |\n",
      "|    value_loss           | 4.2e-06    |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 229     |\n",
      "|    iterations      | 330     |\n",
      "|    time_elapsed    | 11772   |\n",
      "|    total_timesteps | 2703360 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 229         |\n",
      "|    iterations           | 331         |\n",
      "|    time_elapsed         | 11804       |\n",
      "|    total_timesteps      | 2711552     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024204332 |\n",
      "|    clip_fraction        | 0.303       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.571      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.017      |\n",
      "|    n_updates            | 4950        |\n",
      "|    policy_gradient_loss | -0.00381    |\n",
      "|    std                  | 0.428       |\n",
      "|    value_loss           | 6.32e-06    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 229         |\n",
      "|    iterations           | 332         |\n",
      "|    time_elapsed         | 11834       |\n",
      "|    total_timesteps      | 2719744     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.033506487 |\n",
      "|    clip_fraction        | 0.314       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.57       |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0116     |\n",
      "|    n_updates            | 4965        |\n",
      "|    policy_gradient_loss | -0.00333    |\n",
      "|    std                  | 0.428       |\n",
      "|    value_loss           | 6.59e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2720000, episode_reward=-150.48 +/- 0.01\n",
      "Episode length: 256.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 256        |\n",
      "|    mean_reward          | -150       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 2720000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02672999 |\n",
      "|    clip_fraction        | 0.304      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -0.57      |\n",
      "|    explained_variance   | 1          |\n",
      "|    learning_rate        | 5e-05      |\n",
      "|    loss                 | -0.0164    |\n",
      "|    n_updates            | 4980       |\n",
      "|    policy_gradient_loss | -0.00237   |\n",
      "|    std                  | 0.428      |\n",
      "|    value_loss           | 4.32e-06   |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 229     |\n",
      "|    iterations      | 333     |\n",
      "|    time_elapsed    | 11876   |\n",
      "|    total_timesteps | 2727936 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 229         |\n",
      "|    iterations           | 334         |\n",
      "|    time_elapsed         | 11909       |\n",
      "|    total_timesteps      | 2736128     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031620875 |\n",
      "|    clip_fraction        | 0.324       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.57       |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0103     |\n",
      "|    n_updates            | 4995        |\n",
      "|    policy_gradient_loss | -0.00333    |\n",
      "|    std                  | 0.428       |\n",
      "|    value_loss           | 5.18e-06    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2740000, episode_reward=-149.60 +/- 0.01\n",
      "Episode length: 256.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 256        |\n",
      "|    mean_reward          | -150       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 2740000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03251917 |\n",
      "|    clip_fraction        | 0.31       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -0.567     |\n",
      "|    explained_variance   | 1          |\n",
      "|    learning_rate        | 5e-05      |\n",
      "|    loss                 | -0.00368   |\n",
      "|    n_updates            | 5010       |\n",
      "|    policy_gradient_loss | -0.00503   |\n",
      "|    std                  | 0.426      |\n",
      "|    value_loss           | 0.000177   |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 229     |\n",
      "|    iterations      | 335     |\n",
      "|    time_elapsed    | 11950   |\n",
      "|    total_timesteps | 2744320 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 229         |\n",
      "|    iterations           | 336         |\n",
      "|    time_elapsed         | 11986       |\n",
      "|    total_timesteps      | 2752512     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.038012896 |\n",
      "|    clip_fraction        | 0.3         |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.566      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.00816    |\n",
      "|    n_updates            | 5025        |\n",
      "|    policy_gradient_loss | 0.00157     |\n",
      "|    std                  | 0.427       |\n",
      "|    value_loss           | 7.17e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2760000, episode_reward=-149.66 +/- 0.01\n",
      "Episode length: 256.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 256         |\n",
      "|    mean_reward          | -150        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2760000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028978182 |\n",
      "|    clip_fraction        | 0.319       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.57       |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0145     |\n",
      "|    n_updates            | 5040        |\n",
      "|    policy_gradient_loss | -0.00233    |\n",
      "|    std                  | 0.429       |\n",
      "|    value_loss           | 5.28e-06    |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 229     |\n",
      "|    iterations      | 337     |\n",
      "|    time_elapsed    | 12029   |\n",
      "|    total_timesteps | 2760704 |\n",
      "--------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 229        |\n",
      "|    iterations           | 338        |\n",
      "|    time_elapsed         | 12060      |\n",
      "|    total_timesteps      | 2768896    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02635081 |\n",
      "|    clip_fraction        | 0.298      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -0.571     |\n",
      "|    explained_variance   | 1          |\n",
      "|    learning_rate        | 5e-05      |\n",
      "|    loss                 | -0.0255    |\n",
      "|    n_updates            | 5055       |\n",
      "|    policy_gradient_loss | -0.00277   |\n",
      "|    std                  | 0.428      |\n",
      "|    value_loss           | 5.69e-06   |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 229        |\n",
      "|    iterations           | 339        |\n",
      "|    time_elapsed         | 12090      |\n",
      "|    total_timesteps      | 2777088    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03438117 |\n",
      "|    clip_fraction        | 0.32       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -0.572     |\n",
      "|    explained_variance   | 1          |\n",
      "|    learning_rate        | 5e-05      |\n",
      "|    loss                 | 0.00753    |\n",
      "|    n_updates            | 5070       |\n",
      "|    policy_gradient_loss | -0.000439  |\n",
      "|    std                  | 0.43       |\n",
      "|    value_loss           | 7.17e-06   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=2780000, episode_reward=-149.48 +/- 0.01\n",
      "Episode length: 256.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 256         |\n",
      "|    mean_reward          | -149        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2780000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.034615867 |\n",
      "|    clip_fraction        | 0.324       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.574      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0013     |\n",
      "|    n_updates            | 5085        |\n",
      "|    policy_gradient_loss | -0.00311    |\n",
      "|    std                  | 0.43        |\n",
      "|    value_loss           | 5.26e-06    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 229     |\n",
      "|    iterations      | 340     |\n",
      "|    time_elapsed    | 12127   |\n",
      "|    total_timesteps | 2785280 |\n",
      "--------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 229        |\n",
      "|    iterations           | 341        |\n",
      "|    time_elapsed         | 12157      |\n",
      "|    total_timesteps      | 2793472    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02776017 |\n",
      "|    clip_fraction        | 0.321      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -0.573     |\n",
      "|    explained_variance   | 1          |\n",
      "|    learning_rate        | 5e-05      |\n",
      "|    loss                 | -0.00888   |\n",
      "|    n_updates            | 5100       |\n",
      "|    policy_gradient_loss | -0.00271   |\n",
      "|    std                  | 0.429      |\n",
      "|    value_loss           | 7.53e-06   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=2800000, episode_reward=-148.94 +/- 0.01\n",
      "Episode length: 256.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 256         |\n",
      "|    mean_reward          | -149        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2800000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021803696 |\n",
      "|    clip_fraction        | 0.293       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.574      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.013      |\n",
      "|    n_updates            | 5115        |\n",
      "|    policy_gradient_loss | -0.00192    |\n",
      "|    std                  | 0.43        |\n",
      "|    value_loss           | 6e-06       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 229     |\n",
      "|    iterations      | 342     |\n",
      "|    time_elapsed    | 12195   |\n",
      "|    total_timesteps | 2801664 |\n",
      "--------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 229        |\n",
      "|    iterations           | 343        |\n",
      "|    time_elapsed         | 12227      |\n",
      "|    total_timesteps      | 2809856    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02756492 |\n",
      "|    clip_fraction        | 0.305      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -0.575     |\n",
      "|    explained_variance   | 1          |\n",
      "|    learning_rate        | 5e-05      |\n",
      "|    loss                 | 0.00466    |\n",
      "|    n_updates            | 5130       |\n",
      "|    policy_gradient_loss | 0.000106   |\n",
      "|    std                  | 0.43       |\n",
      "|    value_loss           | 3.32e-06   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 229         |\n",
      "|    iterations           | 344         |\n",
      "|    time_elapsed         | 12256       |\n",
      "|    total_timesteps      | 2818048     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.038568743 |\n",
      "|    clip_fraction        | 0.306       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.576      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.00368    |\n",
      "|    n_updates            | 5145        |\n",
      "|    policy_gradient_loss | -0.00267    |\n",
      "|    std                  | 0.431       |\n",
      "|    value_loss           | 5.66e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2820000, episode_reward=-147.89 +/- 0.01\n",
      "Episode length: 256.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 256         |\n",
      "|    mean_reward          | -148        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2820000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027689435 |\n",
      "|    clip_fraction        | 0.315       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.576      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0157     |\n",
      "|    n_updates            | 5160        |\n",
      "|    policy_gradient_loss | -0.00256    |\n",
      "|    std                  | 0.43        |\n",
      "|    value_loss           | 6.43e-06    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 229     |\n",
      "|    iterations      | 345     |\n",
      "|    time_elapsed    | 12292   |\n",
      "|    total_timesteps | 2826240 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 229         |\n",
      "|    iterations           | 346         |\n",
      "|    time_elapsed         | 12324       |\n",
      "|    total_timesteps      | 2834432     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.032830924 |\n",
      "|    clip_fraction        | 0.31        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.574      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.00813    |\n",
      "|    n_updates            | 5175        |\n",
      "|    policy_gradient_loss | -0.00213    |\n",
      "|    std                  | 0.429       |\n",
      "|    value_loss           | 1.35e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2840000, episode_reward=-147.68 +/- 0.01\n",
      "Episode length: 256.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 256         |\n",
      "|    mean_reward          | -148        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2840000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.036012158 |\n",
      "|    clip_fraction        | 0.31        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.571      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.00296    |\n",
      "|    n_updates            | 5190        |\n",
      "|    policy_gradient_loss | -0.00352    |\n",
      "|    std                  | 0.428       |\n",
      "|    value_loss           | 4.54e-06    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 229     |\n",
      "|    iterations      | 347     |\n",
      "|    time_elapsed    | 12360   |\n",
      "|    total_timesteps | 2842624 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 230         |\n",
      "|    iterations           | 348         |\n",
      "|    time_elapsed         | 12392       |\n",
      "|    total_timesteps      | 2850816     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.034838073 |\n",
      "|    clip_fraction        | 0.324       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.569      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.025      |\n",
      "|    n_updates            | 5205        |\n",
      "|    policy_gradient_loss | -0.00394    |\n",
      "|    std                  | 0.427       |\n",
      "|    value_loss           | 7.62e-06    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 230         |\n",
      "|    iterations           | 349         |\n",
      "|    time_elapsed         | 12422       |\n",
      "|    total_timesteps      | 2859008     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026576655 |\n",
      "|    clip_fraction        | 0.324       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.568      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.00994    |\n",
      "|    n_updates            | 5220        |\n",
      "|    policy_gradient_loss | -0.0023     |\n",
      "|    std                  | 0.427       |\n",
      "|    value_loss           | 3.63e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2860000, episode_reward=-147.14 +/- 0.01\n",
      "Episode length: 256.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 256         |\n",
      "|    mean_reward          | -147        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2860000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.033072144 |\n",
      "|    clip_fraction        | 0.32        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.568      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.00631    |\n",
      "|    n_updates            | 5235        |\n",
      "|    policy_gradient_loss | -0.00186    |\n",
      "|    std                  | 0.427       |\n",
      "|    value_loss           | 4.91e-06    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 230     |\n",
      "|    iterations      | 350     |\n",
      "|    time_elapsed    | 12461   |\n",
      "|    total_timesteps | 2867200 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 230         |\n",
      "|    iterations           | 351         |\n",
      "|    time_elapsed         | 12491       |\n",
      "|    total_timesteps      | 2875392     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024632644 |\n",
      "|    clip_fraction        | 0.306       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.564      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0103     |\n",
      "|    n_updates            | 5250        |\n",
      "|    policy_gradient_loss | -0.00271    |\n",
      "|    std                  | 0.424       |\n",
      "|    value_loss           | 3e-06       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2880000, episode_reward=-146.95 +/- 0.01\n",
      "Episode length: 256.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 256         |\n",
      "|    mean_reward          | -147        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2880000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028512737 |\n",
      "|    clip_fraction        | 0.299       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.56       |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.00332    |\n",
      "|    n_updates            | 5265        |\n",
      "|    policy_gradient_loss | -0.00312    |\n",
      "|    std                  | 0.423       |\n",
      "|    value_loss           | 6.16e-06    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best mean reward!\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 230     |\n",
      "|    iterations      | 352     |\n",
      "|    time_elapsed    | 12528   |\n",
      "|    total_timesteps | 2883584 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 230         |\n",
      "|    iterations           | 353         |\n",
      "|    time_elapsed         | 12559       |\n",
      "|    total_timesteps      | 2891776     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031141782 |\n",
      "|    clip_fraction        | 0.311       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.559      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.016      |\n",
      "|    n_updates            | 5280        |\n",
      "|    policy_gradient_loss | -0.00487    |\n",
      "|    std                  | 0.424       |\n",
      "|    value_loss           | 1.52e-05    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 230         |\n",
      "|    iterations           | 354         |\n",
      "|    time_elapsed         | 12590       |\n",
      "|    total_timesteps      | 2899968     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028253641 |\n",
      "|    clip_fraction        | 0.295       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.56       |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.00064    |\n",
      "|    n_updates            | 5295        |\n",
      "|    policy_gradient_loss | -0.0018     |\n",
      "|    std                  | 0.423       |\n",
      "|    value_loss           | 4.23e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2900000, episode_reward=-146.00 +/- 0.01\n",
      "Episode length: 256.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 256         |\n",
      "|    mean_reward          | -146        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2900000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031835273 |\n",
      "|    clip_fraction        | 0.32        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.558      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0105     |\n",
      "|    n_updates            | 5310        |\n",
      "|    policy_gradient_loss | -0.00343    |\n",
      "|    std                  | 0.422       |\n",
      "|    value_loss           | 5.43e-06    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 230     |\n",
      "|    iterations      | 355     |\n",
      "|    time_elapsed    | 12629   |\n",
      "|    total_timesteps | 2908160 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 230         |\n",
      "|    iterations           | 356         |\n",
      "|    time_elapsed         | 12660       |\n",
      "|    total_timesteps      | 2916352     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.033109955 |\n",
      "|    clip_fraction        | 0.309       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.555      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0125     |\n",
      "|    n_updates            | 5325        |\n",
      "|    policy_gradient_loss | 0.000739    |\n",
      "|    std                  | 0.421       |\n",
      "|    value_loss           | 3.67e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2920000, episode_reward=-145.94 +/- 0.01\n",
      "Episode length: 256.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 256         |\n",
      "|    mean_reward          | -146        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2920000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023055907 |\n",
      "|    clip_fraction        | 0.293       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.551      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.00488    |\n",
      "|    n_updates            | 5340        |\n",
      "|    policy_gradient_loss | -0.00118    |\n",
      "|    std                  | 0.419       |\n",
      "|    value_loss           | 4.03e-06    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 230     |\n",
      "|    iterations      | 357     |\n",
      "|    time_elapsed    | 12700   |\n",
      "|    total_timesteps | 2924544 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 230         |\n",
      "|    iterations           | 358         |\n",
      "|    time_elapsed         | 12731       |\n",
      "|    total_timesteps      | 2932736     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.033490114 |\n",
      "|    clip_fraction        | 0.317       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.551      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | 0.00351     |\n",
      "|    n_updates            | 5355        |\n",
      "|    policy_gradient_loss | -0.003      |\n",
      "|    std                  | 0.42        |\n",
      "|    value_loss           | 6.22e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2940000, episode_reward=-146.03 +/- 0.01\n",
      "Episode length: 256.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 256         |\n",
      "|    mean_reward          | -146        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2940000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.036079936 |\n",
      "|    clip_fraction        | 0.321       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.55       |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.00242    |\n",
      "|    n_updates            | 5370        |\n",
      "|    policy_gradient_loss | -0.00227    |\n",
      "|    std                  | 0.419       |\n",
      "|    value_loss           | 3.49e-06    |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 230     |\n",
      "|    iterations      | 359     |\n",
      "|    time_elapsed    | 12770   |\n",
      "|    total_timesteps | 2940928 |\n",
      "--------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 230        |\n",
      "|    iterations           | 360        |\n",
      "|    time_elapsed         | 12803      |\n",
      "|    total_timesteps      | 2949120    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03141827 |\n",
      "|    clip_fraction        | 0.333      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -0.55      |\n",
      "|    explained_variance   | 1          |\n",
      "|    learning_rate        | 5e-05      |\n",
      "|    loss                 | -0.018     |\n",
      "|    n_updates            | 5385       |\n",
      "|    policy_gradient_loss | -0.000446  |\n",
      "|    std                  | 0.42       |\n",
      "|    value_loss           | 2.28e-05   |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 230         |\n",
      "|    iterations           | 361         |\n",
      "|    time_elapsed         | 12836       |\n",
      "|    total_timesteps      | 2957312     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031259526 |\n",
      "|    clip_fraction        | 0.322       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.55       |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.00282    |\n",
      "|    n_updates            | 5400        |\n",
      "|    policy_gradient_loss | -0.00192    |\n",
      "|    std                  | 0.419       |\n",
      "|    value_loss           | 3.39e-06    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2960000, episode_reward=-145.56 +/- 0.01\n",
      "Episode length: 256.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 256         |\n",
      "|    mean_reward          | -146        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2960000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028225828 |\n",
      "|    clip_fraction        | 0.335       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.552      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.00134    |\n",
      "|    n_updates            | 5415        |\n",
      "|    policy_gradient_loss | -0.00283    |\n",
      "|    std                  | 0.421       |\n",
      "|    value_loss           | 2.72e-06    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 230     |\n",
      "|    iterations      | 362     |\n",
      "|    time_elapsed    | 12877   |\n",
      "|    total_timesteps | 2965504 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 230         |\n",
      "|    iterations           | 363         |\n",
      "|    time_elapsed         | 12909       |\n",
      "|    total_timesteps      | 2973696     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028321922 |\n",
      "|    clip_fraction        | 0.328       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.551      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0148     |\n",
      "|    n_updates            | 5430        |\n",
      "|    policy_gradient_loss | -0.00301    |\n",
      "|    std                  | 0.419       |\n",
      "|    value_loss           | 6.02e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2980000, episode_reward=-144.91 +/- 0.01\n",
      "Episode length: 256.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 256         |\n",
      "|    mean_reward          | -145        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2980000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031000748 |\n",
      "|    clip_fraction        | 0.336       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.548      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0127     |\n",
      "|    n_updates            | 5445        |\n",
      "|    policy_gradient_loss | -0.00568    |\n",
      "|    std                  | 0.419       |\n",
      "|    value_loss           | 3.59e-06    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 230     |\n",
      "|    iterations      | 364     |\n",
      "|    time_elapsed    | 12948   |\n",
      "|    total_timesteps | 2981888 |\n",
      "--------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 230        |\n",
      "|    iterations           | 365        |\n",
      "|    time_elapsed         | 12978      |\n",
      "|    total_timesteps      | 2990080    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03052164 |\n",
      "|    clip_fraction        | 0.332      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -0.547     |\n",
      "|    explained_variance   | 1          |\n",
      "|    learning_rate        | 5e-05      |\n",
      "|    loss                 | -0.0104    |\n",
      "|    n_updates            | 5460       |\n",
      "|    policy_gradient_loss | -0.00468   |\n",
      "|    std                  | 0.418      |\n",
      "|    value_loss           | 2.47e-05   |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 230         |\n",
      "|    iterations           | 366         |\n",
      "|    time_elapsed         | 13015       |\n",
      "|    total_timesteps      | 2998272     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026981033 |\n",
      "|    clip_fraction        | 0.31        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.545      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | 0.00844     |\n",
      "|    n_updates            | 5475        |\n",
      "|    policy_gradient_loss | 0.00412     |\n",
      "|    std                  | 0.417       |\n",
      "|    value_loss           | 4.9e-06     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3000000, episode_reward=-143.81 +/- 0.01\n",
      "Episode length: 256.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 256        |\n",
      "|    mean_reward          | -144       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 3000000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03335952 |\n",
      "|    clip_fraction        | 0.294      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -0.543     |\n",
      "|    explained_variance   | 1          |\n",
      "|    learning_rate        | 5e-05      |\n",
      "|    loss                 | -0.00116   |\n",
      "|    n_updates            | 5490       |\n",
      "|    policy_gradient_loss | -0.00103   |\n",
      "|    std                  | 0.415      |\n",
      "|    value_loss           | 2.8e-06    |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 230     |\n",
      "|    iterations      | 367     |\n",
      "|    time_elapsed    | 13055   |\n",
      "|    total_timesteps | 3006464 |\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "# STEP 5 START: Train PPO model\n",
    "# ---------------------------------------------------------------------------\n",
    "# Comments: We create a checkpoint callback to save models periodically and an EvalCallback\n",
    "# to measure performance on a held-out slice. Training time depends on your hardware.\n",
    "\n",
    "# Setup callbacks\n",
    "checkpoint_callback = CheckpointCallback(save_freq=5000, save_path=\"./checkpoints/\", name_prefix=\"ppo_hvac\")\n",
    "\n",
    "# Create eval env for callback (same wrapping as training)\n",
    "eval_env = DummyVecEnv([make_env(start_index=max(0, n_envs * EPISODE_LENGTH - EPISODE_LENGTH))])\n",
    "eval_env = VecNormalize(eval_env, norm_obs=True, norm_reward=False, clip_obs=10.0)\n",
    "# Dont normalize eval env by default; wrap with same VecNormalize if used during training\n",
    "# Note: if you save VecNormalize stats, you must persist them too.\n",
    "# Sync normalization stats\n",
    "eval_env.obs_rms = vec_env.obs_rms\n",
    "\n",
    "eval_callback = EvalCallback(\n",
    "    eval_env,\n",
    "    best_model_save_path=\"./best_model/\",\n",
    "    n_eval_episodes=10,  # Increased for more robust evaluation\n",
    "    eval_freq=5000,\n",
    "    deterministic=True\n",
    ")\n",
    "\n",
    "# Train - Increased timesteps for better performance\n",
    "TOTAL_TIMESTEPS = 3000000  # Increased from 1M to 3M for better learning\n",
    "model.learn(total_timesteps=TOTAL_TIMESTEPS, callback=[checkpoint_callback, eval_callback])\n",
    "\n",
    "# Save final model and VecNormalize statistics\n",
    "os.makedirs(\"./models\", exist_ok=True)\n",
    "model.save(\"./models/ppo_hvac_final\")\n",
    "# Save VecNormalize\n",
    "vec_env.save(\"./models/vec_normalize.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SUGGESTIONS FOR FURTHER MAE REDUCTION ===\n",
      "\n",
      "1. **ENSEMBLE METHODS:**\n",
      "   - Train multiple models with different random seeds\n",
      "   - Use different hyperparameters for each model\n",
      "   - Average predictions from best 3-5 models\n",
      "   - Expected MAE improvement: 2-5 points\n",
      "\n",
      "2. **BEHAVIORAL CLONING PRE-TRAINING:**\n",
      "   - Pre-train policy using supervised learning on dataset\n",
      "   - Then fine-tune with PPO\n",
      "   - Provides better initialization\n",
      "   - Expected MAE improvement: 3-7 points\n",
      "\n",
      "3. **ADVANCED REWARD ENGINEERING:**\n",
      "   - Add temporal consistency rewards\n",
      "   - Penalize rapid oscillations more heavily\n",
      "   - Reward staying within comfort zones\n",
      "   - Expected MAE improvement: 1-3 points\n",
      "\n",
      "4. **HYPERPARAMETER OPTIMIZATION:**\n",
      "   - Use Optuna or similar for systematic tuning\n",
      "   - Focus on: learning_rate, batch_size, network_architecture\n",
      "   - Expected MAE improvement: 2-4 points\n",
      "\n",
      "5. **DATA AUGMENTATION:**\n",
      "   - Add slight noise to training data\n",
      "   - Create synthetic scenarios\n",
      "   - Bootstrap sampling of episodes\n",
      "   - Expected MAE improvement: 1-3 points\n",
      "\n",
      "6. **CURRICULUM LEARNING ENHANCEMENT:**\n",
      "   - Start with easier patterns\n",
      "   - Gradually increase difficulty\n",
      "   - Multi-stage training\n",
      "   - Expected MAE improvement: 2-4 points\n",
      "\n",
      "CURRENT STATUS: MAE = 34\n",
      "REALISTIC TARGET: MAE = 20-25 with above techniques\n",
      "AGGRESSIVE TARGET: MAE = 15-20 with all techniques + domain expertise\n"
     ]
    }
   ],
   "source": [
    "# STEP 6.5: Next Steps for Further Improvement\n",
    "\n",
    "print(\"=== SUGGESTIONS FOR FURTHER MAE REDUCTION ===\")\n",
    "print()\n",
    "print(\"1. **ENSEMBLE METHODS:**\")\n",
    "print(\"   - Train multiple models with different random seeds\")\n",
    "print(\"   - Use different hyperparameters for each model\")\n",
    "print(\"   - Average predictions from best 3-5 models\")\n",
    "print(\"   - Expected MAE improvement: 2-5 points\")\n",
    "print()\n",
    "print(\"2. **BEHAVIORAL CLONING PRE-TRAINING:**\")\n",
    "print(\"   - Pre-train policy using supervised learning on dataset\")\n",
    "print(\"   - Then fine-tune with PPO\")\n",
    "print(\"   - Provides better initialization\")\n",
    "print(\"   - Expected MAE improvement: 3-7 points\")\n",
    "print()\n",
    "print(\"3. **ADVANCED REWARD ENGINEERING:**\")\n",
    "print(\"   - Add temporal consistency rewards\")\n",
    "print(\"   - Penalize rapid oscillations more heavily\")\n",
    "print(\"   - Reward staying within comfort zones\")\n",
    "print(\"   - Expected MAE improvement: 1-3 points\")\n",
    "print()\n",
    "print(\"4. **HYPERPARAMETER OPTIMIZATION:**\")\n",
    "print(\"   - Use Optuna or similar for systematic tuning\")\n",
    "print(\"   - Focus on: learning_rate, batch_size, network_architecture\")\n",
    "print(\"   - Expected MAE improvement: 2-4 points\")\n",
    "print()\n",
    "print(\"5. **DATA AUGMENTATION:**\")\n",
    "print(\"   - Add slight noise to training data\")\n",
    "print(\"   - Create synthetic scenarios\")\n",
    "print(\"   - Bootstrap sampling of episodes\")\n",
    "print(\"   - Expected MAE improvement: 1-3 points\")\n",
    "print()\n",
    "print(\"6. **CURRICULUM LEARNING ENHANCEMENT:**\")\n",
    "print(\"   - Start with easier patterns\")\n",
    "print(\"   - Gradually increase difficulty\")\n",
    "print(\"   - Multi-stage training\")\n",
    "print(\"   - Ellt: 2-4 points\")\n",
    "print()\n",
    "print(\"CURRENT STATUS: MAE = 34\")\n",
    "print(\"REALISTIC TARGET: MAE = 20-25 with above techniques\")\n",
    "print(\"AGGRESSIVE TARGET: MAE = 15-20 with all techniques + domain expertise\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training + evaluation script completed. Check ./models and ./checkpoints for outputs.\n"
     ]
    }
   ],
   "source": [
    "# STEP 6 START: Evaluate PPO performance\n",
    "# ---------------------------------------------------------------------------\n",
    "# Comments: We'll run the trained policy over a portion of the dataframe and record actions.\n",
    "\n",
    "# Reload model + vec normalize if necessary\n",
    "# model = PPO.load1(\"./models/ppo_hvac_final\")\n",
    "# vec_env = VecNormalize.load(\"./models/vec_normalize.pkl\", DummyVecEnv([make_env(start_index=0)]))\n",
    "# vec_env.reset()\n",
    "\n",
    "# For evaluation, use an un-normalized instance of the environment and manually apply scaler\n",
    "eval_df = feature_df.copy().fillna(method=\"ffill\").fillna(0.0)\n",
    "start_eval = 0\n",
    "end_eval = min(2000, len(eval_df) - 1)\n",
    "eval_episode_length = end_eval - start_eval\n",
    "\n",
    "eval_env_simple = HVACValveEnv(df=eval_df, obs_cols=obs_cols, scaler=scaler, episode_length=eval_episode_length, start_index=start_eval)\n",
    "obs, _ = eval_env_simple.reset()\n",
    "\n",
    "pred_actions = []\n",
    "true_vals = []\n",
    "infos = []\n",
    "\n",
    "done = False\n",
    "while not done:\n",
    "    # SB3 models expect actions in their raw range; since our action space is [0,100], model will output that.\n",
    "    action, _ = model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, _, info = eval_env_simple.step(action)\n",
    "    pred_actions.append(float(action[0]))\n",
    "    true_vals.append(float(info.get(\"gt_valve\", np.nan)))\n",
    "    infos.append(info)\n",
    "\n",
    "\n",
    "\n",
    "# Plot true vs predicted\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(true_vals, label=\"True Valve\")\n",
    "plt.plot(pred_actions, label=\"Predicted Valve (Agent)\")\n",
    "plt.legend()\n",
    "plt.title(\"True vs Predicted Valve over evaluation segment\")\n",
    "plt.show()\n",
    "\n",
    "# Save evaluation results\n",
    "eval_res = pd.DataFrame({\"true_valve\": true_vals, \"pred_valve\": pred_actions})\n",
    "eval_res.to_csv(\"./models/evaluation_results.csv\", index=False)\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# REPORT (brief): How to improve results\n",
    "# ---------------------------------------------------------------------------\n",
    "# 1) Increase TOTAL_TIMESTEPS and tune hyperparameters (learning rate, n_steps, batch_size).\n",
    "# 2) Adjust reward coefficients: smoothness_penalty_coef and energy_penalty_coef.\n",
    "# 3) Add richer state: previous Valve, time-of-day cyclic features, occupancy as categorical.\n",
    "# 4) Use curriculum learning: start with short episodes and increase length.\n",
    "# 5) Use model ensembles or offline behavioral cloning as warm-start from dataset actions.\n",
    "# 6) Consider action scaling: if Valve behaves as ratio, map to [0,1] internally for better learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation MSE: 2952.5168, MAE: 45.9079\n"
     ]
    }
   ],
   "source": [
    "# Compute metrics\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "mse = mean_squared_error(true_vals, pred_actions)\n",
    "mae = mean_absolute_error(true_vals, pred_actions)\n",
    "print(f\"Evaluation MSE: {mse:.4f}, MAE: {mae:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Model Performance Analysis ===\n",
      "MAE: 45.9079\n",
      "MSE: 2952.5168\n",
      "RMSE: 54.3371\n",
      "Error Percentiles: 25%=26.71, 50%=45.45, \n",
      "                  75%=78.53, 90%=82.53, \n",
      "                  95%=83.50, 99%=85.75\n",
      "\n",
      "High error instances (200 cases):\n",
      "True values range: 99.42 - 100.00\n",
      "Predictions range: 12.46 - 17.47\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# STEP 7: Model Diagnostics and Analysis\n",
    "\n",
    "def analyze_model_performance(true_vals, pred_actions, infos):\n",
    "    \"\"\"Analyze model performance in detail\"\"\"\n",
    "    true_vals = np.array(true_vals)\n",
    "    pred_actions = np.array(pred_actions)\n",
    "    \n",
    "    # Basic metrics\n",
    "    mae = np.mean(np.abs(true_vals - pred_actions))\n",
    "    mse = np.mean((true_vals - pred_actions)**2)\n",
    "    rmse = np.sqrt(mse)\n",
    "    \n",
    "    # Error distribution analysis\n",
    "    errors = np.abs(true_vals - pred_actions)\n",
    "    error_percentiles = np.percentile(errors, [25, 50, 75, 90, 95, 99])\n",
    "    \n",
    "    print(f\"=== Model Performance Analysis ===\")\n",
    "    print(f\"MAE: {mae:.4f}\")\n",
    "    print(f\"MSE: {mse:.4f}\")\n",
    "    print(f\"RMSE: {rmse:.4f}\")\n",
    "    print(f\"Error Percentiles: 25%={error_percentiles[0]:.2f}, 50%={error_percentiles[1]:.2f}, \")\n",
    "    print(f\"                  75%={error_percentiles[2]:.2f}, 90%={error_percentiles[3]:.2f}, \")\n",
    "    print(f\"                  95%={error_percentiles[4]:.2f}, 99%={error_percentiles[5]:.2f}\")\n",
    "    \n",
    "    # Identify problematic ranges\n",
    "    high_error_mask = errors > np.percentile(errors, 90)\n",
    "    if np.any(high_error_mask):\n",
    "        print(f\"\\nHigh error instances ({np.sum(high_error_mask)} cases):\")\n",
    "        print(f\"True values range: {true_vals[high_error_mask].min():.2f} - {true_vals[high_error_mask].max():.2f}\")\n",
    "        print(f\"Predictions range: {pred_actions[high_error_mask].min():.2f} - {pred_actions[high_error_mask].max():.2f}\")\n",
    "    \n",
    "    # Plot error distribution\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.hist(errors, bins=50, alpha=0.7)\n",
    "    plt.xlabel('Absolute Error')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Error Distribution')\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.scatter(true_vals, pred_actions, alpha=0.5, s=1)\n",
    "    plt.plot([true_vals.min(), true_vals.max()], [true_vals.min(), true_vals.max()], 'r--')\n",
    "    plt.xlabel('True Values')\n",
    "    plt.ylabel('Predicted Values')\n",
    "    plt.title('True vs Predicted Scatter')\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    residuals = pred_actions - true_vals\n",
    "    plt.scatter(true_vals, residuals, alpha=0.5, s=1)\n",
    "    plt.axhline(y=0, color='r', linestyle='--')\n",
    "    plt.xlabel('True Values')\n",
    "    plt.ylabel('Residuals')\n",
    "    plt.title('Residuals Plot')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return mae, mse, rmse\n",
    "\n",
    "# Run analysis if we have evaluation results\n",
    "if 'true_vals' in locals() and 'pred_actions' in locals():\n",
    "    mae, mse, rmse = analyze_model_performance(true_vals, pred_actions, infos)\n",
    "else:\n",
    "    print(\"Run evaluation first to analyze performance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Advanced techniques defined. To use:\n",
      "1. Run pretrain_with_behavioral_cloning() before PPO training\n",
      "2. Use ensemble prediction for final evaluation\n",
      "3. Enhanced features can improve observation space\n"
     ]
    }
   ],
   "source": [
    "# Additional Techniques for Better Performance\n",
    "\n",
    "# 1. Ensemble Prediction (using multiple checkpoints)\n",
    "def load_and_predict_ensemble(obs, checkpoint_paths):\n",
    "    \"\"\"Load multiple models and average their predictions\"\"\"\n",
    "    predictions = []\n",
    "    for path in checkpoint_paths:\n",
    "        try:\n",
    "            model_temp = PPO.load(path)\n",
    "            pred, _ = model_temp.predict(obs, deterministic=True)\n",
    "            predictions.append(pred[0])\n",
    "        except:\n",
    "            continue\n",
    "    return np.mean(predictions) if predictions else 50.0  # fallback\n",
    "\n",
    "# 2. Behavioral Cloning Pretraining (optional)\n",
    "def pretrain_with_behavioral_cloning():\n",
    "    \"\"\"Pre-train the policy using supervised learning on the dataset\"\"\"\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    from sklearn.neural_network import MLPRegressor\n",
    "    \n",
    "    # Prepare training data\n",
    "    X = feature_df[obs_cols].fillna(method=\"ffill\").fillna(0.0).values\n",
    "    X_scaled = scaler.transform(X)\n",
    "    y = feature_df['Valve'].fillna(method=\"ffill\").fillna(50.0).values\n",
    "    \n",
    "    # Train a supervised model\n",
    "    bc_model = MLPRegressor(\n",
    "        hidden_layer_sizes=(512, 256, 128),\n",
    "        learning_rate_init=0.001,\n",
    "        max_iter=500,\n",
    "        early_stopping=True,\n",
    "        validation_fraction=0.2,\n",
    "        n_iter_no_change=20\n",
    "    )\n",
    "    \n",
    "    # Split data\n",
    "    split_idx = int(0.8 * len(X_scaled))\n",
    "    bc_model.fit(X_scaled[:split_idx], y[:split_idx])\n",
    "    \n",
    "    print(f\"Behavioral cloning model trained. Score: {bc_model.score(X_scaled[split_idx:], y[split_idx:]):.4f}\")\n",
    "    return bc_model\n",
    "\n",
    "# 3. Advanced Feature Engineering\n",
    "def create_advanced_features(df):\n",
    "    \"\"\"Create additional features that might help\"\"\"\n",
    "    df_enhanced = df.copy()\n",
    "    \n",
    "    # Rolling statistics\n",
    "    window_size = 10\n",
    "    for col in ['RaTemp', 'SaTemp', 'RaHumidity', 'Valve_lag_1']:\n",
    "        if col in df_enhanced.columns:\n",
    "            df_enhanced[f'{col}_rolling_mean'] = df_enhanced[col].rolling(window=window_size).mean()\n",
    "            df_enhanced[f'{col}_rolling_std'] = df_enhanced[col].rolling(window=window_size).std()\n",
    "    \n",
    "    # Temperature differences and ratios\n",
    "    if 'RaTemp' in df_enhanced.columns and 'SaTemp' in df_enhanced.columns:\n",
    "        df_enhanced['temp_ratio'] = df_enhanced['RaTemp'] / (df_enhanced['SaTemp'] + 1e-8)\n",
    "        df_enhanced['temp_change_rate'] = df_enhanced['RaTemp'].diff()\n",
    "    \n",
    "    # Interaction features\n",
    "    if 'Occp' in df_enhanced.columns and 'main_temp' in df_enhanced.columns:\n",
    "        df_enhanced['occupancy_temp_interaction'] = df_enhanced['Occp'] * df_enhanced['main_temp']\n",
    "    \n",
    "    return df_enhanced.fillna(method='ffill').fillna(0.0)\n",
    "\n",
    "print(\"Advanced techniques defined. To use:\")\n",
    "print(\"1. Run pretrain_with_behavioral_cloning() before PPO training\")\n",
    "print(\"2. Use ensemble prediction for final evaluation\")\n",
    "print(\"3. Enhanced features can improve observation space\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (proj)",
   "language": "python",
   "name": "proj"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
